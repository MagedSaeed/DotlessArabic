####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
ا ص ب ح ا ل م ل ك ل ل ذ ي ف ط ر ا ل خ ل
ق ب ت ق د ي ر ل ل ع ز ي ز ا ل ع ل ي م
غ ا ف ر ا ل ذ ن ب ل ل م س ي ء ب ع ف و
ق ا ب ل ا ل ت و ب ذ ي ا ل ع ط ا ء ا ل ع م ي م
م ر س ل ا ل م ص ط ف ى ا ل ب ش ي ر ا ل ي ن ا
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-27 04:29:56.226290 for dataset poems_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 7,714,858
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 145,574,240
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.3021
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset poems_dataset tokenized by CharacterTokenizer at 2023-03-27 04:30:15.541077
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-27 04:30:15.541107 for dataset poems_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ا ص ٮ ح ا ل م ل ك ل ل د ى ڡ ط ر ا ل ح ل
ٯ ٮ ٮ ٯ د ى ر ل ل ع ر ى ر ا ل ع ل ى م
ع ا ڡ ر ا ل د ں ٮ ل ل م س ى ء ٮ ع ڡ و
ٯ ا ٮ ل ا ل ٮ و ٮ د ى ا ل ع ط ا ء ا ل ع م ى م
م ر س ل ا ل م ص ط ڡ ى ا ل ٮ س ى ر ا ل ى ں ا
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 145,574,240
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8983
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset poems_dataset tokenized by CharacterTokenizer at 2023-03-27 04:38:48.376861
####################################################################################################
