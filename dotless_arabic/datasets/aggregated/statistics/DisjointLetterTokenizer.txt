####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
بسم ا لله ا لر حمن ا لر حيم
ا لحمد لله ر ب ا لعا لمين
ا لر حمن ا لر حيم
ما لك يو م ا لد ين
ا يا ك نعبد و ا يا ك نستعين
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-27 03:18:04.457589 for dataset aggregated_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 15,779,363
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 402,019
####################################################################################################
####################################################################################################
All Tokens Count: 867,375,567
####################################################################################################
####################################################################################################
vocab/tokens: 0.0005
####################################################################################################
####################################################################################################
Tokens Entropy: 8.2102
####################################################################################################
####################################################################################################
Average tokens length: 5.5860
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.7031
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset aggregated_dataset tokenized by DisjointLetterTokenizer at 2023-03-27 03:20:19.525357
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-27 03:20:19.525393 for dataset aggregated_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ٮسم ا لله ا لر حمں ا لر حٮم
ا لحمد لله ر ٮ ا لعا لمٮں
ا لر حمں ا لر حٮم
ما لك ٮو م ا لد ٮں
ا ٮا ك ٮعٮد و ا ٮا ك ٮسٮعٮں
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 162,389
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 867,375,567
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0002
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 7.5231
####################################################################################################
####################################################################################################
Average tokens length: 6.1099
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.8872
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 239,630
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset aggregated_dataset tokenized by DisjointLetterTokenizer at 2023-03-27 04:14:37.730478
####################################################################################################
