####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
ب س م ا ل ل ه ا ل ر ح م ن ا ل ر ح ي م
ا ل ح م د ل ل ه ر ب ا ل ع ا ل م ي ن
ا ل ر ح م ن ا ل ر ح ي م
م ا ل ك ي و م ا ل د ي ن
ا ي ا ك ن ع ب د و ا ي ا ك ن س ت ع ي ن
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 05:24:21.025139 for dataset aggregated_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 15,779,363
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 1,774,280,089
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2727
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset aggregated_dataset tokenized by CharacterTokenizer at 2023-03-06 05:27:20.899522
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 05:27:20.899549 for dataset aggregated_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ٮ س م ا ل ل ه ا ل ر ح م ں ا ل ر ح ى م
ا ل ح م د ل ل ه ر ٮ ا ل ع ا ل م ى ں
ا ل ر ح م ں ا ل ر ح ى م
م ا ل ك ى و م ا ل د ى ں
ا ى ا ك ں ع ٮ د و ا ى ا ك ں س ٮ ع ى ں
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 1,774,280,089
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8663
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset aggregated_dataset tokenized by CharacterTokenizer at 2023-03-06 07:05:23.039603
####################################################################################################
