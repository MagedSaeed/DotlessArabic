####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
بسم ا لله ا لر حمن ا لر حيم
ا لحمد لله ر ب ا لعا لمين
ا لر حمن ا لر حيم
ما لك يو م ا لد ين
ا يا ك نعبد و ا يا ك نستعين
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 03:00:48.248660 for dataset quran_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 6,236
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 6,484
####################################################################################################
####################################################################################################
All Tokens Count: 164,609
####################################################################################################
####################################################################################################
vocab/tokens: 0.0394
####################################################################################################
####################################################################################################
Tokens Entropy: 7.4512
####################################################################################################
####################################################################################################
Average tokens length: 4.0560
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 3.2870
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset quran_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 03:00:48.280533
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 03:00:48.280561 for dataset quran_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ٮسم ا لله ا لر حمں ا لر حىم
ا لحمد لله ر ٮ ا لعا لمىں
ا لر حمں ا لر حىم
ما لك ىو م ا لد ىں
ا ىا ك ٮعٮد و ا ىا ك ٮسٮعىں
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 4,956
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 164,609
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0301
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 7.0824
####################################################################################################
####################################################################################################
Average tokens length: 4.2837
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 3.4303
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 1,528
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset quran_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 03:00:48.866896
####################################################################################################
