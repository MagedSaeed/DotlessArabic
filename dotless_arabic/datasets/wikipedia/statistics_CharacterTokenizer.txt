####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

ت و ف ي ف ي ز ي و ر خ ع ن ع م ر ي ن ا ه ز ع ا م ا
ا ل ج و ب ل ي ة م ح ل ة ت ا ب ع ة ل ق ر ي ة م د ح ج ا ل س ا ف ل ا ل ت ا ب ع ة ل ع ز ل ة م د ح ج ي ن ب م د ي ر ي ة ا ل ق ف ر ا ح د ى م د ي ر ي ا ت م ح ا ف ظ ة ا ب ف ي ا ل ج م ه و ر ي ة ا ل ي م ن ي ة ب ل غ ت ع د ا د س ك ا ن ه ا ح س ب ت ع د ا د ا ل ي م ن ل ع ا م
ي ش ك ل ا ل ت د ر ي ب ع ل ى ا ل ر ا ى ح ة ا ل ى ج ا ن ب ا ل ز ر ع ا ت ا ل ش م ي ة ا ح د ا ل خ ي ا ر ا ت ا ل ع ل ا ج ي ة ا ل و ا ع د ة ر غ م ك و ن ه ا ت ج ر ي ب ي ة
ر غ م م ع ا ر ض ة ا ب ي ه ا ل ل ز و ا ج ب س ب ب ا ن ت م ا ء ا ل م خ ل و ف ل ل ح ز ب ا ل س و ر ي ا ل ق و م ي ا ل ا ج ت م ا ع ي ب ي ن م ا ي ن ت م ي ا ل ا س د ل ح ز ب ا ل ب ع ث ا ل ع ر ب ي ا ل ا ش ت ر ا ك ي ت ز و ج ت م ن ح ا ف ظ ا ل ا س د ع ن د م ا ك ا ن ض ا ب ط ا ف ي ا ل ق و ا ت ا ل ج و ي ة ا ل ع ر ب ي ة ا ل س و ر ي ة ف ي ع ا م ا ر ت ف ع ت م ك ا ن ة ع ا ى ل ة م خ ل و ف و ث ر و ت ه ا ب س ب ب ز و ا ج ه ا م ن ح ا ف ظ ا ل ا س د ح ص ل ا ق ا ر ب ا ن ي س ة م خ ل و ف ع ل ى ع ق و د م ر ب ح ة د ا خ ل ق ط ا ع ا ت ا ل ب ن و ك و ا ل ن ف ط و ا ل ا ت ص ا ل ا ت ف ي ا ل ب ل ا د ي ع ت ق د ا ن ا ح د ا ب ن ا ء ش ق ي ق ه ا ر ا م ي م خ ل و ف ه و ا غ ن ى ر ج ل ف ي س و ر ي ا ح ي ث ب ل غ ت ق ي م ه ث ر و ت ه ا ل ص ا ف ي ة م ل ي ا ر ا ت د و ل ا ر ا م ر ي ك ي ا ع ت ب ا ر ا م ن ع ا م
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-02-27 18:38:45.068138 for dataset wikipedia_dataset
####################################################################################################
####################################################################################################
Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 851,615,598
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2744
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset wikipedia_dataset at 2023-02-27 18:40:09.040284
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-02-27 18:40:09.040354 for dataset wikipedia_dataset
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

ٮ و ڡ ى ڡ ى ر ى و ر ح ع ں ع م ر ى ں ا ه ر ع ا م ا
ا ل ح و ٮ ل ى ه م ح ل ه ٮ ا ٮ ع ه ل ٯ ر ى ه م د ح ح ا ل س ا ڡ ل ا ل ٮ ا ٮ ع ه ل ع ر ل ه م د ح ح ى ں ٮ م د ى ر ى ه ا ل ٯ ڡ ر ا ح د ى م د ى ر ى ا ٮ م ح ا ڡ ط ه ا ٮ ڡ ى ا ل ح م ه و ر ى ه ا ل ى م ں ى ه ٮ ل ع ٮ ع د ا د س ك ا ں ه ا ح س ٮ ٮ ع د ا د ا ل ى م ں ل ع ا م
ى س ك ل ا ل ٮ د ر ى ٮ ع ل ى ا ل ر ا ى ح ه ا ل ى ح ا ں ٮ ا ل ر ر ع ا ٮ ا ل س م ى ه ا ح د ا ل ح ى ا ر ا ٮ ا ل ع ل ا ح ى ه ا ل و ا ع د ه ر ع م ك و ں ه ا ٮ ح ر ى ٮ ى ه
ر ع م م ع ا ر ص ه ا ٮ ى ه ا ل ل ر و ا ح ٮ س ٮ ٮ ا ں ٮ م ا ء ا ل م ح ل و ڡ ل ل ح ر ٮ ا ل س و ر ى ا ل ٯ و م ى ا ل ا ح ٮ م ا ع ى ٮ ى ں م ا ى ں ٮ م ى ا ل ا س د ل ح ر ٮ ا ل ٮ ع ٮ ا ل ع ر ٮ ى ا ل ا س ٮ ر ا ك ى ٮ ر و ح ٮ م ں ح ا ڡ ط ا ل ا س د ع ں د م ا ك ا ں ص ا ٮ ط ا ڡ ى ا ل ٯ و ا ٮ ا ل ح و ى ه ا ل ع ر ٮ ى ه ا ل س و ر ى ه ڡ ى ع ا م ا ر ٮ ڡ ع ٮ م ك ا ں ه ع ا ى ل ه م ح ل و ڡ و ٮ ر و ٮ ه ا ٮ س ٮ ٮ ر و ا ح ه ا م ں ح ا ڡ ط ا ل ا س د ح ص ل ا ٯ ا ر ٮ ا ں ى س ه م ح ل و ڡ ع ل ى ع ٯ و د م ر ٮ ح ه د ا ح ل ٯ ط ا ع ا ٮ ا ل ٮ ں و ك و ا ل ں ڡ ط و ا ل ا ٮ ص ا ل ا ٮ ڡ ى ا ل ٮ ل ا د ى ع ٮ ٯ د ا ں ا ح د ا ٮ ں ا ء س ٯ ى ٯ ه ا ر ا م ى م ح ل و ڡ ه و ا ع ں ى ر ح ل ڡ ى س و ر ى ا ح ى ٮ ٮ ل ع ٮ ٯ ى م ه ٮ ر و ٮ ه ا ل ص ا ڡ ى ه م ل ى ا ر ا ٮ د و ل ا ر ا م ر ى ك ى ا ع ٮ ٮ ا ر ا م ں ع ا م
####################################################################################################
####################################################################################################
Undotted Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 851,615,598
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8724
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset at 2023-02-27 19:27:08.460863
####################################################################################################
