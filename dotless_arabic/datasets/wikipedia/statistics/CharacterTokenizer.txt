####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

ك م ن ت م ج م و ع ة م ن ك ت ا ى ب ا ل ق س ا م ل و ح د ة م ت س ل ل ة م ن ك و م ا ن د و ز ب ح ر ي ص ه ي و ن ي ة ف ي م ن ط ق ة ا ل س و د ا ن ي ة غ ر ب و ف ت ح و ا ن ي ر ا ن ا س ل ح ت ه م ب ا ت ج ا ه ه ا ف و ر و ق و ع ه ا ف ي ا ل ك م ي ن م و ق ع ي ن ف ي ص ف و ف ا ل و ح د ة ا ل ا س ر ا ى ي ل ي ة ا ص ا ب ا ت م ح ق ق ة ا ل ا ا ن ا ع ل ا م ا ل ا س ر ا ى ي ل ي ا ك ت ف ى ب ا ل ا ع ت ر ا ف ب ا ص ا ب ة م ن ج ن و د ه ف ي ا ل ا ش ت ب ا ك
ا د ا ن ك ل ا ا ل ح ز ب ي ن ا ل س ي ا س ي ي ن ا ل ب ر ي ط ا ن ي ي ن ق ر ا ر ا ل م ح ك م ة و ق ا ل و ز ي ر ا ل د ا خ ل ي ة ف ي ح ك و م ة ا ل ظ ل د ي ف ي د د ي ف ي س ا ر ت ك ب ه و ل ا ء ا ل خ ا ط ف و ن ج ر ا ى م خ ط ي ر ة ي ف ت ر ض ا ن ت ج ع ل ه م ي ت ع ا ر ض و ن م ع ص ف ة ا ل ل ا ج ى و ج ا د ل ا ن ا ل م ش ك ل ة ت ك م ن ف ي ا ب د ا ع ح ك و م ة ا ل ع م ل ب س ب ب ا س ت ح د ا ث ه م ل ق ا ن و ن ح ق و ق ا ل ا ن س ا ن ل ع ا م
ي ت و ا ج د ح ا م ض ا ل م ا ل ي ك ا ل س ت ر ي ك ف ي ل ب ا ل ج و ا ف ة
ك ا ن ل د ى ا ل ح ر ك ة ع د ة ق ا د ة ق ا د ا ل ص ح ف ي و ا ل ك ا ت ب ا و ت و غ ل ا غ ا و م ج ل ة م ك ا ف ح ا ل ث ق ا ف ة ا ل ت ي ن ش ر ت ه ذ ه ا ل ا ف ك ا ر ل ق د ق ا د ا ل ل ا ه و ت ي و ا ل س ي ا س ي ا ل ل و ث ر ي ا د و ل ف س ت و ي ك ر ا ل ح ز ب ا ل ا ج ت م ا ع ي ا ل م س ي ح ي ك م ا ك ا ن ا ل م م ث ل ا ل و ح ي د ا ل م ن ت خ ب ل ل ح ز ب ف ي ا ل ر ا ي خ س ت ا غ
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-04 21:59:41.034128 for dataset wikipedia_dataset
####################################################################################################
####################################################################################################
Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 851,615,598
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2744
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset wikipedia_dataset at 2023-03-04 22:01:03.557162
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-04 22:01:03.557194 for dataset wikipedia_dataset
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

ك م ں ٮ م ح م و ع ه م ں ك ٮ ا ى ٮ ا ل ٯ س ا م ل و ح د ه م ٮ س ل ل ه م ں ك و م ا ں د و ر ٮ ح ر ى ص ه ى و ں ى ه ڡ ى م ں ط ٯ ه ا ل س و د ا ں ى ه ع ر ٮ و ڡ ٮ ح و ا ں ى ر ا ں ا س ل ح ٮ ه م ٮ ا ٮ ح ا ه ه ا ڡ و ر و ٯ و ع ه ا ڡ ى ا ل ك م ى ں م و ٯ ع ى ں ڡ ى ص ڡ و ڡ ا ل و ح د ه ا ل ا س ر ا ى ى ل ى ه ا ص ا ٮ ا ٮ م ح ٯ ٯ ه ا ل ا ا ں ا ع ل ا م ا ل ا س ر ا ى ى ل ى ا ك ٮ ڡ ى ٮ ا ل ا ع ٮ ر ا ڡ ٮ ا ص ا ٮ ه م ں ح ں و د ه ڡ ى ا ل ا س ٮ ٮ ا ك
ا د ا ں ك ل ا ا ل ح ر ٮ ى ں ا ل س ى ا س ى ى ں ا ل ٮ ر ى ط ا ں ى ى ں ٯ ر ا ر ا ل م ح ك م ه و ٯ ا ل و ر ى ر ا ل د ا ح ل ى ه ڡ ى ح ك و م ه ا ل ط ل د ى ڡ ى د د ى ڡ ى س ا ر ٮ ك ٮ ه و ل ا ء ا ل ح ا ط ڡ و ں ح ر ا ى م ح ط ى ر ه ى ڡ ٮ ر ص ا ں ٮ ح ع ل ه م ى ٮ ع ا ر ص و ں م ع ص ڡ ه ا ل ل ا ح ى و ح ا د ل ا ں ا ل م س ك ل ه ٮ ك م ں ڡ ى ا ٮ د ا ع ح ك و م ه ا ل ع م ل ٮ س ٮ ٮ ا س ٮ ح د ا ٮ ه م ل ٯ ا ں و ں ح ٯ و ٯ ا ل ا ں س ا ں ل ع ا م
ى ٮ و ا ح د ح ا م ص ا ل م ا ل ى ك ا ل س ٮ ر ى ك ڡ ى ل ٮ ا ل ح و ا ڡ ه
ك ا ں ل د ى ا ل ح ر ك ه ع د ه ٯ ا د ه ٯ ا د ا ل ص ح ڡ ى و ا ل ك ا ٮ ٮ ا و ٮ و ع ل ا ع ا و م ح ل ه م ك ا ڡ ح ا ل ٮ ٯ ا ڡ ه ا ل ٮ ى ں س ر ٮ ه د ه ا ل ا ڡ ك ا ر ل ٯ د ٯ ا د ا ل ل ا ه و ٮ ى و ا ل س ى ا س ى ا ل ل و ٮ ر ى ا د و ل ڡ س ٮ و ى ك ر ا ل ح ر ٮ ا ل ا ح ٮ م ا ع ى ا ل م س ى ح ى ك م ا ك ا ں ا ل م م ٮ ل ا ل و ح ى د ا ل م ں ٮ ح ٮ ل ل ح ر ٮ ڡ ى ا ل ر ا ى ح س ٮ ا ع
####################################################################################################
####################################################################################################
Undotted Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 851,615,598
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8724
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset at 2023-03-04 22:47:58.563844
####################################################################################################
