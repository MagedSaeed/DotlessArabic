####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
ا ل م ا ء م ا د ة ش ف ا ف ة ع د ي م ة ا ل ل و ن و ا ل ر ا ى ح ة و ه و ا ل م ك و ن ا ل ا س ا س ي ل ل ج د ا و ل و ا ل ب ح ي ر ا ت و ا ل ب ح ا ر و ا ل م ح ي ط ا ت و ك ذ ل ك ل ل س و ا ى ل ف ي ج م ي ع ا ل ك ا ى ن ا ت ا ل ح ي ة و ه و ا ك ث ر ا ل م ر ك ب ا ت ا ل ك ي م ي ا ى ي ة ا ن ت ش ا ر ا ع ل ى س ط ح ا ل ا ر ض ي ت ا ل ف ج ز ي ء ا ل م ا ء م ن ذ ر ة ا ك س ج ي ن م ر ك ز ي ة ت ر ت ب ط ب ه ا ذ ر ت ا ه ي د ر و ج ي ن ع ل ى ط ر ف ي ه ا ب ر ا ب ط ة ت س ا ه م ي ة ب ح ي ث ت ك و ن ص ي غ ت ه ا ل ك ي م ي ا ى ي ة ع ن د ا ل ظ ر و ف ا ل ق ي ا س ي ة م ن ا ل ض غ ط و د ر ج ة ا ل ح ر ا ر ة ي ك و ن ا ل م ا ء س ا ى ل ا ا م ا ا ل ح ا ل ة ا ل ص ل ب ة ف ت ت ش ك ل ع ن د ن ق ط ة ا ل ت ج م د و ت د ع ى ب ا ل ج ل ي د ا م ا ا ل ح ا ل ة ا ل غ ا ز ي ة ف ت ت ش ك ل ع ن د ن ق ط ة ا ل غ ل ي ا ن و ت س م ى ب خ ا ر ا ل م ا ء
ا ن ا ل م ا ء ه و ا س ا س و ج و د ا ل ح ي ا ة ع ل ى ك و ك ب ا ل ا ر ض و ه و ي غ ط ي م ن س ط ح ه ا و ت م ث ل م ي ا ه ا ل ب ح ا ر و ا ل م ح ي ط ا ت ا ك ب ر ن س ب ة ل ل م ا ء ع ل ى ا ل ا ر ض ح ي ث ت ب ل غ ح و ا ل ي و ت ت و ز ع ا ل ن س ب ا ل ب ا ق ي ة ب ي ن ا ل م ي ا ه ا ل ج و ف ي ة و ب ي ن ج ل ي د ا ل م ن ا ط ق ا ل ق ط ب ي ة ل ك ل ي ه م ا م ع و ج و د ن س ب ة ص غ ي ر ة ع ل ى ش ك ل ب خ ا ر م ا ء م ع ل ق ف ي ا ل ه و ا ء ع ل ى ه ي ى ة س ح ا ب غ ي و م و ا ح ي ا ن ا ا خ ر ى ع ل ى ه ي ى ة ض ب ا ب ا و ن د ى ب ا ل ا ض ا ف ة ا ل ى ا ل ز خ ا ت ا ل م ط ر ي ة ا و ا ل ث ل ج ي ة ت ب ل غ ن س ب ة ا ل م ا ء ا ل ع ذ ب ح و ا ل ي ف ق ط م ن ا ل م ا ء ا ل م و ج و د ع ل ى ا ل ا ر ض و ا غ ل ب ه ذ ه ا ل ك م ي ة ح و ا ل ي م و ج و د ة ف ي ا ل ك ت ل ا ل ج ل ي د ي ة ف ي ا ل م ن ا ط ق ا ل ق ط ب ي ة ف ي ح ي ن ت ت و ا ج د م ن ا ل م ا ء ا ل ع ذ ب ف ي ا ل ا ن ه ا ر و ا ل ب ح ي ر ا ت و ف ي ا ل غ ل ا ف ا ل ج و ي
ا م ا ف ي ا ل ط ب ي ع ة ف ت ت غ ي ر ح ا ل ة ا ل م ا ء ب ي ن ا ل ح ا ل ا ت ا ل ث ل ا ث ة ل ل م ا د ة ع ل ى س ط ح ا ل ا ر ض ب ا س ت م ر ا ر م ن خ ل ا ل م ا ي ع ر ف ب ا س م ا ل د و ر ة ا ل م ا ى ي ة ا و د و ر ة ا ل م ا ء و ا ل ت ي ت ت ض م ن ح د و ث ت ب خ ر و ن ت ح ن ت ح ت ب خ ر ي ث م ت ك ث ي ف ف ه ط و ل ث م ج ر ي ا ن ل ت ص ل ا ل ى ا ل م ص ب ف ي ا ل م س ط ح ا ت ا ل م ا ى ي ة
ش ك ل ا ل ح ص و ل ع ل ى م ص د ر ن ق ي م ن م ي ا ه ا ل ش ر ب ا م ر ا م ه م ا ل ن ش و ء ا ل ح ض ا ر ا ت ع ب ر ا ل ت ا ر ي خ و ف ي ا ل ع ق و د ا ل ا خ ي ر ة س ج ل ت ح ا ل ا ت ش ح ف ي ا ل م ي ا ه ا ل ع ذ ب ة ف ي م ن ا ط ق ع د ي د ة م ن ا ل ع ا ل م و ل ق د ق د ر ت ا ح ص ا ء ا ت ا ل ا م م ا ل م ت ح د ة ا ن ح و ا ل ي م ل ي ا ر ش خ ص ع ل ى س ط ح ا ل ا ر ض ل ا ي ز ا ل و ن ي ف ت ق ر و ن ا ل و س ا ى ل ا ل م ت ا ح ة ل ل و ص و ل ا ل ى م ص د ر ا م ن ل م ي ا ه ا ل ش ر ب و ا ن ح و ا ل ي م ل ي ا ر ي ف ت ق ر و ن ا ل ى و س ي ل ة م ل ا ى م ة م ن ا ج ل ت ط ه ي ر ا ل م ي ا ه
ا ل خ و ا ص ا ل ف ي ز ي ا ى ي ة و ا ل ك ي م ي ا ى ي ة
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2024-01-01 07:03:19.125684 for dataset wikipedia_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 1,000,000
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 198,267,175
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2754
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by CharacterTokenizer at 2024-01-01 07:03:58.608040
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2024-01-01 07:03:58.608237 for dataset wikipedia_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ا ل م ا ء م ا د ه س ڡ ا ڡ ه ع د ى م ه ا ل ل و ں و ا ل ر ا ى ح ه و ه و ا ل م ك و ں ا ل ا س ا س ى ل ل ح د ا و ل و ا ل ٮ ح ى ر ا ٮ و ا ل ٮ ح ا ر و ا ل م ح ى ط ا ٮ و ك د ل ك ل ل س و ا ى ل ڡ ى ح م ى ع ا ل ك ا ى ں ا ٮ ا ل ح ى ه و ه و ا ك ٮ ر ا ل م ر ك ٮ ا ٮ ا ل ك ى م ى ا ى ى ه ا ں ٮ س ا ر ا ع ل ى س ط ح ا ل ا ر ص ى ٮ ا ل ڡ ح ر ى ء ا ل م ا ء م ں د ر ه ا ك س ح ى ں م ر ك ر ى ه ٮ ر ٮ ٮ ط ٮ ه ا د ر ٮ ا ه ى د ر و ح ى ں ع ل ى ط ر ڡ ى ه ا ٮ ر ا ٮ ط ه ٮ س ا ه م ى ه ٮ ح ى ٮ ٮ ك و ں ص ى ع ٮ ه ا ل ك ى م ى ا ى ى ه ع ں د ا ل ط ر و ڡ ا ل ٯ ى ا س ى ه م ں ا ل ص ع ط و د ر ح ه ا ل ح ر ا ر ه ى ك و ں ا ل م ا ء س ا ى ل ا ا م ا ا ل ح ا ل ه ا ل ص ل ٮ ه ڡ ٮ ٮ س ك ل ع ں د ں ٯ ط ه ا ل ٮ ح م د و ٮ د ع ى ٮ ا ل ح ل ى د ا م ا ا ل ح ا ل ه ا ل ع ا ر ى ه ڡ ٮ ٮ س ك ل ع ں د ں ٯ ط ه ا ل ع ل ى ا ں و ٮ س م ى ٮ ح ا ر ا ل م ا ء
ا ں ا ل م ا ء ه و ا س ا س و ح و د ا ل ح ى ا ه ع ل ى ك و ك ٮ ا ل ا ر ص و ه و ى ع ط ى م ں س ط ح ه ا و ٮ م ٮ ل م ى ا ه ا ل ٮ ح ا ر و ا ل م ح ى ط ا ٮ ا ك ٮ ر ں س ٮ ه ل ل م ا ء ع ل ى ا ل ا ر ص ح ى ٮ ٮ ٮ ل ع ح و ا ل ى و ٮ ٮ و ر ع ا ل ں س ٮ ا ل ٮ ا ٯ ى ه ٮ ى ں ا ل م ى ا ه ا ل ح و ڡ ى ه و ٮ ى ں ح ل ى د ا ل م ں ا ط ٯ ا ل ٯ ط ٮ ى ه ل ك ل ى ه م ا م ع و ح و د ں س ٮ ه ص ع ى ر ه ع ل ى س ك ل ٮ ح ا ر م ا ء م ع ل ٯ ڡ ى ا ل ه و ا ء ع ل ى ه ى ى ه س ح ا ٮ ع ى و م و ا ح ى ا ں ا ا ح ر ى ع ل ى ه ى ى ه ص ٮ ا ٮ ا و ں د ى ٮ ا ل ا ص ا ڡ ه ا ل ى ا ل ر ح ا ٮ ا ل م ط ر ى ه ا و ا ل ٮ ل ح ى ه ٮ ٮ ل ع ں س ٮ ه ا ل م ا ء ا ل ع د ٮ ح و ا ل ى ڡ ٯ ط م ں ا ل م ا ء ا ل م و ح و د ع ل ى ا ل ا ر ص و ا ع ل ٮ ه د ه ا ل ك م ى ه ح و ا ل ى م و ح و د ه ڡ ى ا ل ك ٮ ل ا ل ح ل ى د ى ه ڡ ى ا ل م ں ا ط ٯ ا ل ٯ ط ٮ ى ه ڡ ى ح ى ں ٮ ٮ و ا ح د م ں ا ل م ا ء ا ل ع د ٮ ڡ ى ا ل ا ں ه ا ر و ا ل ٮ ح ى ر ا ٮ و ڡ ى ا ل ع ل ا ڡ ا ل ح و ى
ا م ا ڡ ى ا ل ط ٮ ى ع ه ڡ ٮ ٮ ع ى ر ح ا ل ه ا ل م ا ء ٮ ى ں ا ل ح ا ل ا ٮ ا ل ٮ ل ا ٮ ه ل ل م ا د ه ع ل ى س ط ح ا ل ا ر ص ٮ ا س ٮ م ر ا ر م ں ح ل ا ل م ا ى ع ر ڡ ٮ ا س م ا ل د و ر ه ا ل م ا ى ى ه ا و د و ر ه ا ل م ا ء و ا ل ٮ ى ٮ ٮ ص م ں ح د و ٮ ٮ ٮ ح ر و ں ٮ ح ں ٮ ح ٮ ٮ ح ر ى ٮ م ٮ ك ٮ ى ڡ ڡ ه ط و ل ٮ م ح ر ى ا ں ل ٮ ص ل ا ل ى ا ل م ص ٮ ڡ ى ا ل م س ط ح ا ٮ ا ل م ا ى ى ه
س ك ل ا ل ح ص و ل ع ل ى م ص د ر ں ٯ ى م ں م ى ا ه ا ل س ر ٮ ا م ر ا م ه م ا ل ں س و ء ا ل ح ص ا ر ا ٮ ع ٮ ر ا ل ٮ ا ر ى ح و ڡ ى ا ل ع ٯ و د ا ل ا ح ى ر ه س ح ل ٮ ح ا ل ا ٮ س ح ڡ ى ا ل م ى ا ه ا ل ع د ٮ ه ڡ ى م ں ا ط ٯ ع د ى د ه م ں ا ل ع ا ل م و ل ٯ د ٯ د ر ٮ ا ح ص ا ء ا ٮ ا ل ا م م ا ل م ٮ ح د ه ا ں ح و ا ل ى م ل ى ا ر س ح ص ع ل ى س ط ح ا ل ا ر ص ل ا ى ر ا ل و ں ى ڡ ٮ ٯ ر و ں ا ل و س ا ى ل ا ل م ٮ ا ح ه ل ل و ص و ل ا ل ى م ص د ر ا م ں ل م ى ا ه ا ل س ر ٮ و ا ں ح و ا ل ى م ل ى ا ر ى ڡ ٮ ٯ ر و ں ا ل ى و س ى ل ه م ل ا ى م ه م ں ا ح ل ٮ ط ه ى ر ا ل م ى ا ه
ا ل ح و ا ص ا ل ڡ ى ر ى ا ى ى ه و ا ل ك ى م ى ا ى ى ه
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 198,267,175
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8673
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by CharacterTokenizer at 2024-01-01 07:23:54.548851
####################################################################################################
####################################################################################################
Undotted with random letters remapping Statistics Analysis Started at 2024-01-01 07:23:54.548977 for dataset wikipedia_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Undotting the first sample showing the random letters remapping
None
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ط ٮ ص ط و ص ط ه ه ك ل ط ل ه ى ه ر ص ه ط ٮ ٮ ٮ ء ٮ ط ٮ د ط ح ٮ ه ٮ م ٮ ط ٮ ص ع ٮ ء ط ٮ ط ى ط ى ر ٮ ٮ ع ه ط ٮ ٮ ٮ ط ٮ د ٮ ر د ط ڡ ٮ ط ٮ د ٮ ط د ٮ ط ٮ ص ٮ ر س ط ڡ ٮ ع ٮ ٮ ع ٮ ٮ ى ٮ ط ح ٮ ل ر ع ص ر ى ط ٮ ع ط ح ء ط ڡ ط ٮ ٮ ر ه ٮ م ٮ ط ع ح د ط ٮ ص د ع د ط ڡ ط ٮ ع ر ص ر ط ح ر ه ط ء ڡ ك ط د ط ى ٮ ح ى س ٮ ط ٮ ط د ا ر ڡ ط ٮ ل ع س ر و ط ٮ ص ط و ص ء ٮ د ه ط ع ى ع ر ء ص د ع س ر ه ڡ د ڡ د س د م ط ٮ د ڡ ط م ر ه د ٮ ع ر ء ى ٮ ح س د ل ر م ط د د ط د س ه ڡ ى ط م ص ر ه د ٮ ر ح ڡ ع ٮ ء ر ر ح ڡ م ط ٮ ع ر ص ر ط ح ر ه ى ء ه ط ٮ ص د ٮ ل ط ٮ ا ر ط ى ر ه ص ء ط ٮ ا ح س ٮ ه د ع ه ط ٮ ٮ د ط د ه ر ع ٮ ء ط ٮ ص ط و ى ط ح ٮ ط ط ص ط ط ٮ ٮ ط ٮ ه ط ٮ ر ٮ د ه ل ڡ ڡ ك ع ٮ ى ء ه ء ا س ه ط ٮ ڡ ع ص ه ٮ ڡ ه ى ح د ط ٮ ع ٮ ر ه ط ص ط ط ٮ ٮ ط ٮ ه ط ٮ ح ط س ر ه ل ڡ ڡ ك ع ٮ ى ء ه ء ا س ه ط ٮ ح ٮ ر ط ء ٮ ڡ ى ص ح د ى ط د ط ٮ ص ط و
ط ء ط ٮ ص ط و م ٮ ط ى ط ى ٮ ع ٮ ه ط ٮ ٮ ر ط ه ى ٮ ح ع ٮ ع د ط ٮ ط د ا ٮ م ٮ ر ح س ر ص ء ى س ٮ م ط ٮ ڡ ص ح ٮ ص ر ط م ط ٮ د ٮ ط د ٮ ط ٮ ص ٮ ر س ط ڡ ط ع د د ء ى د ه ٮ ٮ ص ط و ى ٮ ح ط ٮ ط د ا ٮ ر ح ڡ د ٮ ح ٮ ٮ ط ٮ ر ٮ ڡ ڡ ٮ س ى ط ٮ ء ى د ط ٮ د ط ا ر ه د ر ء ط ٮ ص ر ط م ط ٮ ع ٮ ل ر ه ٮ د ر ء ع ٮ ر ه ط ٮ ص ء ط س ا ط ٮ ا س د ر ه ٮ ع ٮ ر م ص ط ص ى ٮ ع ٮ ه ء ى د ه ر ح ر د ه ى ٮ ح ك ع ٮ د ى ط د ص ط و ص ى ٮ ا ل ر ط ٮ م ٮ ط و ى ٮ ح م ر ح ه ى ٮ ط د ح ر ٮ ص ٮ ط ٮ ر ط ء ط ط ى د ح ى ٮ ح م ر ح ه ا د ط د ط ٮ ء ه ح د ط ٮ ط ا ط ل ه ط ٮ ح ط ٮ س ى ط ڡ ط ٮ ص س د ر ه ط ٮ ط ٮ ح ٮ ع ر ه ڡ د ٮ ح ء ى د ه ط ٮ ص ط و ط ٮ ى ٮ د ٮ ٮ ط ٮ ر ل ا س ص ء ط ٮ ص ط و ط ٮ ص ٮ ع ٮ ه ى ٮ ح ط ٮ ط د ا ٮ ط ح ٮ د م ٮ م ط ٮ ع ص ر ه ٮ ٮ ط ٮ ر ص ٮ ع ٮ ه ه ل ر ط ٮ ع ڡ ٮ ط ٮ ع ٮ ر ه ر ه ل ر ط ٮ ص ء ط س ا ط ٮ ا س د ر ه ل ر ٮ ر ء ڡ ڡ ٮ ط ع ه ص ء ط ٮ ص ط و ط ٮ ى ٮ د ل ر ط ٮ ط ء م ط د ٮ ط ٮ د ٮ ر د ط ڡ ٮ ل ر ط ٮ ح ٮ ط ل ط ٮ ع ٮ ر
ط ص ط ل ر ط ٮ س د ر ى ه ل ڡ ڡ ح ر د ٮ ط ٮ ه ط ٮ ص ط و د ر ء ط ٮ ٮ ط ٮ ط ڡ ط ٮ ح ٮ ط ح ه ٮ ٮ ص ط ه ه ى ٮ ح ى س ٮ ط ٮ ط د ا د ط ى ڡ ص د ط د ص ء ى ٮ ط ٮ ص ط ر ى د ل د ط ى ص ط ٮ ه ٮ د ه ط ٮ ص ط ح ر ه ط ٮ ه ٮ د ه ط ٮ ص ط و ٮ ط ٮ ڡ ر ڡ ڡ ا ص ء ٮ ه ٮ ح ڡ د ى د ٮ ء ڡ ٮ ء ڡ ٮ ڡ د ى د ر ح ص ڡ ع ح ر ل ل م س ٮ ٮ ح ص ع د ر ط ء ٮ ڡ ر ٮ ط ٮ ح ط ٮ ص ر د ل ر ط ٮ ص ى س ٮ ط ڡ ط ٮ ص ط ح ر ه
ك ع ٮ ط ٮ ٮ ر ٮ ٮ ى ٮ ح ص ر ه د ء ا ر ص ء ص ر ط م ط ٮ ك د د ط ص د ط ص م ص ط ٮ ء ك ٮ و ط ٮ ٮ ا ط د ط ڡ ى د د ط ٮ ڡ ط د ر ى ٮ ل ر ط ٮ ى ا ٮ ه ط ٮ ط ى ر د ه ى ع ٮ ڡ ٮ ط ٮ ط ڡ ك ٮ ل ر ط ٮ ص ر ط م ط ٮ ى ٮ د ه ل ر ص ء ط س ا ى ه ر ه ه ص ء ط ٮ ى ط ٮ ص ٮ ٮ ا ه ا ه د ڡ ط ٮ ر ط و ط ڡ ط ٮ ط ص ص ط ٮ ص ڡ ٮ ه ه ط ء ٮ ٮ ط ٮ ر ص ٮ ر ط د ك ى ر ى ٮ ح ى س ٮ ط ٮ ط د ا ٮ ط ر س ط ٮ ٮ ء ر ل ڡ ا د ٮ ء ط ٮ ٮ ى ط ح ٮ ط ٮ ص ڡ ط ٮ ه ٮ ٮ ٮ ر ٮ ٮ ط ٮ ح ص ر ه د ط ص ء ٮ ص ر ط م ط ٮ ك د د ٮ ط ء ٮ ٮ ط ٮ ر ص ٮ ر ط د ر ل ڡ ا د ٮ ء ط ٮ ح ٮ ى ر ٮ ه ص ٮ ط ح ص ه ص ء ط ع ٮ ڡ س م ر د ط ٮ ص ر ط م
ط ٮ ى ٮ ط ر ط ٮ ل ر س ر ط ح ر ه ٮ ط ٮ ع ر ص ر ط ح ر ه
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 17
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 198,267,175
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.5952
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 14
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by CharacterTokenizer at 2024-01-01 07:25:22.246771
####################################################################################################
