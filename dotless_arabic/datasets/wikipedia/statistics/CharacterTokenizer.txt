####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

ن ص ف ا ل ق ط ر ا ل ا ق ص ي ل ع م ل ي ا ت ا ل ط ا ى ر ة ا ب ا ب ي ل ي ص ل ا ل ى ك م م ي ل و ح د ه ا ا ل ا ق ص ى ل ل ا ر ت ف ا ع ه و ق د م م ا ي ض ا ا ل ط ا ى ر ة ل د ي ه ا ق د ر ة ع ل ى ا ل س ف ر ب س ر ع ة ق ص و ى ت ب ل غ ك ل م س ا ع ة م ي ل ا ف ي ا ل س ا ع ة و ق ا د ر ة ع ل ى ا ل ط ي ر ا ن ب ح م و ل ة ت ب ل غ ك ج م ا و ر ط ل و ت ح ت و ي ا ل ط ا ى ر ة ع ل ى م ك و ن ا ت ل ل م ر ا ق ب ة ا ل ج و ي ة ف ه ي ت ح م ل ك ا م ي ر ا و م ع د ا ت ل ل ا ت ص ا ل ا ت ا ل ر ق م ي ة و ل ك ن ه ا ا ي ض ا ي م ك ن ه ا ا ن ت ص ن ع ب م ك و ن ا ت ه ج و م ي ة ف ت ح م ل ر و و س ا ش د ي د ة ا ل ا ن ف ج ا ر ي ت م ت س د ي د ه ا ب و ا س ط ة ا ص ط د ا م ا ل ط ا ى ر ة ب ا ل ه د ف ا ل م ر ا د ت ف ج ي ر ه
ي و ك د ع ب د ا ل ل ط ي ف ا ل ب ن ا ي ف ي ا ح د ح و ا ر ت ه ا ل ص ح ا ف ي ة ا ن ا ل ف ن ا ن ا ل ق د ي ر ع ب د ا ل ك ر ي م ع ب د ا ل ق ا د ر و ق ف
و ج د ف ر ي ق ا ن س و ق ا ك ث ر م ن ع ش ر ة م ح ت ر ف ي ن م ن ا ك ث ر م ن ا ر ب ع ة ا ل ا ف ف ي ت ر ك ي ا م ر خ ص و ن ا ي ض ا ل ل ر ي ا ض ي ي ن ا ل ا ل ك ت ر و ن ي ي ن ك م ا ت د خ ل ت ب ش ي ك ت ا ش و ا ل ن و ا د ي ا ل ر ي ا ض ي ة ا ل ت ق ل ي د ي ة و ا ن ض م ت ن و ا د ي ر ي ا ض ي ة ا خ ر ى و خ ا ص ة غ ل ط ة س ر ا ي و ف ن ر ب خ ش ة ا ل ى س و ق ا ل ر ي ا ض ة ا ل ا ل ك ت ر و ن ي ة
س و ا ن ج ل ا ن د ه ي م م ث ل ة ا م ر ي ك ي ة و ل د ت ف ي ي و ل ي و ب ت ل س ا ف ي ا ل و ل ا ي ا ت ا ل م ت ح د ة و ت و ف ي ت ف ي م ا ر س
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 03:30:28.136272 for dataset wikipedia_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 851,699,872
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2744
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by CharacterTokenizer at 2023-03-06 03:31:54.993301
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 03:31:54.993335 for dataset wikipedia_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

ں ص ڡ ا ل ٯ ط ر ا ل ا ٯ ص ى ل ع م ل ى ا ٮ ا ل ط ا ى ر ه ا ٮ ا ٮ ى ل ى ص ل ا ل ى ك م م ى ل و ح د ه ا ا ل ا ٯ ص ى ل ل ا ر ٮ ڡ ا ع ه و ٯ د م م ا ى ص ا ا ل ط ا ى ر ه ل د ى ه ا ٯ د ر ه ع ل ى ا ل س ڡ ر ٮ س ر ع ه ٯ ص و ى ٮ ٮ ل ع ك ل م س ا ع ه م ى ل ا ڡ ى ا ل س ا ع ه و ٯ ا د ر ه ع ل ى ا ل ط ى ر ا ں ٮ ح م و ل ه ٮ ٮ ل ع ك ح م ا و ر ط ل و ٮ ح ٮ و ى ا ل ط ا ى ر ه ع ل ى م ك و ں ا ٮ ل ل م ر ا ٯ ٮ ه ا ل ح و ى ه ڡ ه ى ٮ ح م ل ك ا م ى ر ا و م ع د ا ٮ ل ل ا ٮ ص ا ل ا ٮ ا ل ر ٯ م ى ه و ل ك ں ه ا ا ى ص ا ى م ك ں ه ا ا ں ٮ ص ں ع ٮ م ك و ں ا ٮ ه ح و م ى ه ڡ ٮ ح م ل ر و و س ا س د ى د ه ا ل ا ں ڡ ح ا ر ى ٮ م ٮ س د ى د ه ا ٮ و ا س ط ه ا ص ط د ا م ا ل ط ا ى ر ه ٮ ا ل ه د ڡ ا ل م ر ا د ٮ ڡ ح ى ر ه
ى و ك د ع ٮ د ا ل ل ط ى ڡ ا ل ٮ ں ا ى ڡ ى ا ح د ح و ا ر ٮ ه ا ل ص ح ا ڡ ى ه ا ں ا ل ڡ ں ا ں ا ل ٯ د ى ر ع ٮ د ا ل ك ر ى م ع ٮ د ا ل ٯ ا د ر و ٯ ڡ
و ح د ڡ ر ى ٯ ا ں س و ٯ ا ك ٮ ر م ں ع س ر ه م ح ٮ ر ڡ ى ں م ں ا ك ٮ ر م ں ا ر ٮ ع ه ا ل ا ڡ ڡ ى ٮ ر ك ى ا م ر ح ص و ں ا ى ص ا ل ل ر ى ا ص ى ى ں ا ل ا ل ك ٮ ر و ں ى ى ں ك م ا ٮ د ح ل ٮ ٮ س ى ك ٮ ا س و ا ل ں و ا د ى ا ل ر ى ا ص ى ه ا ل ٮ ٯ ل ى د ى ه و ا ں ص م ٮ ں و ا د ى ر ى ا ص ى ه ا ح ر ى و ح ا ص ه ع ل ط ه س ر ا ى و ڡ ں ر ٮ ح س ه ا ل ى س و ٯ ا ل ر ى ا ص ه ا ل ا ل ك ٮ ر و ں ى ه
س و ا ں ح ل ا ں د ه ى م م ٮ ل ه ا م ر ى ك ى ه و ل د ٮ ڡ ى ى و ل ى و ٮ ٮ ل س ا ڡ ى ا ل و ل ا ى ا ٮ ا ل م ٮ ح د ه و ٮ و ڡ ى ٮ ڡ ى م ا ر س
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 851,699,872
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8724
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by CharacterTokenizer at 2023-03-06 04:19:08.747679
####################################################################################################
