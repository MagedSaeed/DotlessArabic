####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

يسمح لتعر يف ا لو ظيفة ا لد ا خلية با ستخد ا م عبا ر ة و ا حد ة للتعر يف
و قد تم تطو ير ا لطر ق من قبل ا لر و ما ن في ا سبا نيا في ميلا د ي لا ستغلا ل ا لر و ا سب ا لذ هبية ا لكبير ة ا لغر ينيه و ا كبر مو قع في لا س مد و لا س حيث ا ستغلت سبعة قنو ا ت طو يلة ا لا نها ر ا لمحلية و سد د ت ا لو د ا ىع كا نت ا سبا نيا و ا حد ة من ا هم منا طق ا لتعد ين و لكن تم ا ستغلا ل جميع منا طق ا لا مبر ا طو ر ية ا لر و ما نية في بر يطا نيا ا لعظمى كا ن ا لسكا ن ا لا صليو ن قد ا ستخر جو ا ا لمعا د ن منذ ا لا ف ا لسنين و لكن بعد ا لغز و ا لر و ما ني ا ز د ا د حجم ا لعمليا ت بشكل كبير حيث كا ن ا لر و ما ن يحتا جو ن ا لى مو ا ر د بر يتا نيا و خا صة ا لذ هب و ا لفضة و ا لقصد ير و ا لر صا ص
قا م بعض ا لمتر بصين بر مضا ن با لو شا ية ا لى ا لا يطا ليين با ن ر مضا ن لا ز ا ل يعا د ي ا لقضية ا لا يطا لية و لم يلتز م با لا بتعا د عن محا ر بتهم ا و تا ييد ا لجها د ضد هم فقا م ا لمتر بصو ن به بتجهيز مكيد ة له و هو ا ن يقو م ا لا يطا ليو ن با ختبا ر ه في معر كة ا لقر ضا بية و ا لتي كا ن يجهز لها ا لمستعمر لسيطر ة ا لكا ملة على ا لمنطقة ا لو سطى و كا ن ر مضا ن يبعث با لا شا ر ا ت لجميع قو ا ت ا لمجا هد ين و عن كيفية ا لو قو ع با لمستعمر في هذ ه ا لمعر كة و لكن كا ن ا نعد ا م ا لو فا ق بين ا لمجا هد ين هي سمة تلك ا لمر حلة و حسب مصا د ر ا لتا ر يخ ا لمو ثقة و حسب ر و ا ية ا لشيخ ا لجليل ا لز ا و ي كتا ب ا لجها د في طر ا بلس ا لغر ب كا ن ا لمجا هد و ن في مو قف لا يحسد عليه مع فر ق ا لعتا د و ا لعد ة و لكن بفضل خطة ر مضا ن ا لعسكر ية ا لفذ ة قا م مجا هد و مصر ا تة و بعد ا ستطا عت ر مضا ن ا لا فلا ت من ا لقيا د ة ا لا يطا لية ا لا تفا ف حو ل ا لقو ا ت ا لا يطا لية و قلب ا لمو ا ز ين
ا قليم تيغر ا ي ا و تكر ينيا تكر ا ي كلل با لتجر ينية يقع في شما ل ا ثيو بيا و يحد ه من ا لشما ل ا ر يتر يا و من ا لغر ب ا لسو د ا ن و من ا لشر ق عفر و من جنو بها ا قليم ا مهر ة
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 03:25:27.538936 for dataset wikipedia_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 4,636,663
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 266,822
####################################################################################################
####################################################################################################
All Tokens Count: 412,843,347
####################################################################################################
####################################################################################################
vocab/tokens: 0.0006
####################################################################################################
####################################################################################################
Tokens Entropy: 8.1534
####################################################################################################
####################################################################################################
Average tokens length: 5.5446
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.6293
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 03:26:28.122613
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 03:26:28.122653 for dataset wikipedia_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

ىسمح لٮعر ىڡ ا لو طىڡه ا لد ا حلىه ٮا سٮحد ا م عٮا ر ه و ا حد ه للٮعر ىڡ
و ڡد ٮم ٮطو ىر ا لطر ٯ مں ڡٮل ا لر و ما ں ڡى ا سٮا ٮىا ڡى مىلا د ى لا سٮعلا ل ا لر و ا سٮ ا لد هٮىه ا لكٮىر ه ا لعر ىٮىه و ا كٮر مو ڡع ڡى لا س مد و لا س حىٮ ا سٮعلٮ سٮعه ڡٮو ا ٮ طو ىله ا لا ٮها ر ا لمحلىه و سد د ٮ ا لو د ا ىع كا ٮٮ ا سٮا ٮىا و ا حد ه مں ا هم مٮا طٯ ا لٮعد ىں و لكں ٮم ا سٮعلا ل حمىع مٮا طٯ ا لا مٮر ا طو ر ىه ا لر و ما ٮىه ڡى ٮر ىطا ٮىا ا لعطمى كا ں ا لسكا ں ا لا صلىو ں ڡد ا سٮحر حو ا ا لمعا د ں مٮد ا لا ڡ ا لسٮىں و لكں ٮعد ا لعر و ا لر و ما ٮى ا ر د ا د ححم ا لعملىا ٮ ٮسكل كٮىر حىٮ كا ں ا لر و ما ں ىحٮا حو ں ا لى مو ا ر د ٮر ىٮا ٮىا و حا صه ا لد هٮ و ا لڡصه و ا لڡصد ىر و ا لر صا ص
ڡا م ٮعص ا لمٮر ٮصىں ٮر مصا ں ٮا لو سا ىه ا لى ا لا ىطا لىىں ٮا ں ر مصا ں لا ر ا ل ىعا د ى ا لڡصىه ا لا ىطا لىه و لم ىلٮر م ٮا لا ٮٮعا د عں محا ر ٮٮهم ا و ٮا ىىد ا لحها د صد هم ڡڡا م ا لمٮر ٮصو ں ٮه ٮٮحهىر مكىد ه له و هو ا ں ىڡو م ا لا ىطا لىو ں ٮا حٮٮا ر ه ڡى معر كه ا لڡر صا ٮىه و ا لٮى كا ں ىحهر لها ا لمسٮعمر لسىطر ه ا لكا مله على ا لمٮطڡه ا لو سطى و كا ں ر مصا ں ىٮعٮ ٮا لا سا ر ا ٮ لحمىع ڡو ا ٮ ا لمحا هد ىں و عں كىڡىه ا لو ڡو ع ٮا لمسٮعمر ڡى هد ه ا لمعر كه و لكں كا ں ا ٮعد ا م ا لو ڡا ٯ ٮىں ا لمحا هد ىں هى سمه ٮلك ا لمر حله و حسٮ مصا د ر ا لٮا ر ىح ا لمو ٮڡه و حسٮ ر و ا ىه ا لسىح ا لحلىل ا لر ا و ى كٮا ٮ ا لحها د ڡى طر ا ٮلس ا لعر ٮ كا ں ا لمحا هد و ں ڡى مو ڡڡ لا ىحسد علىه مع ڡر ٯ ا لعٮا د و ا لعد ه و لكں ٮڡصل حطه ر مصا ں ا لعسكر ىه ا لڡد ه ڡا م محا هد و مصر ا ٮه و ٮعد ا سٮطا عٮ ر مصا ں ا لا ڡلا ٮ مں ا لڡىا د ه ا لا ىطا لىه ا لا ٮڡا ڡ حو ل ا لڡو ا ٮ ا لا ىطا لىه و ڡلٮ ا لمو ا ر ىں
ا ڡلىم ٮىعر ا ى ا و ٮكر ىٮىا ٮكر ا ى كلل ٮا لٮحر ىٮىه ىڡع ڡى سما ل ا ٮىو ٮىا و ىحد ه مں ا لسما ل ا ر ىٮر ىا و مں ا لعر ٮ ا لسو د ا ں و مں ا لسر ٯ عڡر و مں حٮو ٮها ا ڡلىم ا مهر ه
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 155,512
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 412,843,347
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0004
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 7.6491
####################################################################################################
####################################################################################################
Average tokens length: 5.9922
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.8544
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 111,310
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset wikipedia_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 03:50:21.467579
####################################################################################################
