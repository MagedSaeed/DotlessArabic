####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

ا لكعبي ا لنسخة ا لعا شر ة غير و صا نع
و ر غم ا ضا فة نقا ط بقر ا ر من لجنة ا لا ستىنا ف با تحا د ا لكر ة لر صيد ا لخليج بعد قبو ل ا لطعن حو ل سيا ر ة ا لا سعا ف ا لا ا ن ر فع ر صيد ا بنا ء خو ر فكا ن من نقطة ا لى نقطة لم يمنح ا لفر يق سو ى بصيص ا لا مل
ا لمبا ر ا ة في مجملها كا نت جيد ة ا لمستو ى ا عتمد فيها ا لو حد ة على طر يقة و ظهر متما سكا و متحكما في كل مجر يا ت ا للعب و بر غم ا نه كا ن محظو ظا ا حيا نا من خلا ل ضيا ع بعض ا لفر ص ا لسهلة من ا لا هلي ا لا ا نه ا هد ر هو ا لا خر عد ة فر ص حقيقية علي مد ا ر ا لشو طين كا نت كفيلة بمضا عفة ا لنتيجة
و ا عتبر سنا تو ر ا يلينو ي با ر ا ك ا و با ما خلا ل حملته ا لا نتخا بية ا مس ا لا و ل في نبر ا سكا و سط ا نه ا لا فضل لمو ا جهة ما كين و قا ل ا و با ما ا نا متشو ق لمنا قشة جو ن ما كين لا ني ا عر ف كيف ا تو جه ا لى ا لمستقلين و لا ني ا حظى بد عم جمهو ر ي و هذ ا ما نحتا جه للفو ز
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 03:58:23.254022 for dataset news_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 2,784,041
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 151,557
####################################################################################################
####################################################################################################
All Tokens Count: 326,024,821
####################################################################################################
####################################################################################################
vocab/tokens: 0.0005
####################################################################################################
####################################################################################################
Tokens Entropy: 7.9806
####################################################################################################
####################################################################################################
Average tokens length: 5.3099
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.4088
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset news_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 03:59:08.613361
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 03:59:08.613412 for dataset news_dataset tokenized by DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

ا لكعٮى ا لٮسحه ا لعا سر ه عىر و صا ٮع
و ر عم ا صا ڡه ٮڡا ط ٮڡر ا ر مں لحٮه ا لا سٮىٮا ڡ ٮا ٮحا د ا لكر ه لر صىد ا لحلىح ٮعد ڡٮو ل ا لطعں حو ل سىا ر ه ا لا سعا ڡ ا لا ا ں ر ڡع ر صىد ا ٮٮا ء حو ر ڡكا ں مں ٮڡطه ا لى ٮڡطه لم ىمٮح ا لڡر ىٯ سو ى ٮصىص ا لا مل
ا لمٮا ر ا ه ڡى محملها كا ٮٮ حىد ه ا لمسٮو ى ا عٮمد ڡىها ا لو حد ه على طر ىڡه و طهر مٮما سكا و مٮحكما ڡى كل محر ىا ٮ ا للعٮ و ٮر عم ا ٮه كا ں محطو طا ا حىا ٮا مں حلا ل صىا ع ٮعص ا لڡر ص ا لسهله مں ا لا هلى ا لا ا ٮه ا هد ر هو ا لا حر عد ه ڡر ص حڡىڡىه على مد ا ر ا لسو طىں كا ٮٮ كڡىله ٮمصا عڡه ا لٮٮىحه
و ا عٮٮر سٮا ٮو ر ا ىلىٮو ى ٮا ر ا ك ا و ٮا ما حلا ل حملٮه ا لا ٮٮحا ٮىه ا مس ا لا و ل ڡى ٮٮر ا سكا و سط ا ٮه ا لا ڡصل لمو ا حهه ما كىں و ڡا ل ا و ٮا ما ا ٮا مٮسو ٯ لمٮا ڡسه حو ں ما كىں لا ٮى ا عر ڡ كىڡ ا ٮو حه ا لى ا لمسٮڡلىں و لا ٮى ا حطى ٮد عم حمهو ر ى و هد ا ما ٮحٮا حه للڡو ر
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 92,472
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 326,024,821
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0003
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 7.4952
####################################################################################################
####################################################################################################
Average tokens length: 5.6799
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 4.6229
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 59,085
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset news_dataset tokenized by DisjointLetterTokenizer at 2023-03-06 04:18:05.872088
####################################################################################################
