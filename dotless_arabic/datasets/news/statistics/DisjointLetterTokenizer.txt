####################################################################################################
Tokenize the Dataset with DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

و قا لت مصا د ر ا منية و طبية ا ن ا لحا فلة كا نت في طر يقها من ا لمنيا ا لى محا فظة بني سو يف ا لمجا و ر ة و ا ن سا ىقها حا و ل تفا د ي ا لا صطد ا م بشا حنة قا د مة في ا لا تجا ه ا لمعا كس فا نقلبت ا لحا فلة و سقطت في تر عة ا لا بر ا هيمية ا لمجا و ر ة للطر يق و ا ضا ف ا ن جميع ا لقتلى تقر يبا ما تو ا غر قا في تر عة ا لا بر ا هيمية و هي من ا طو ل ا لتر ع في ا لعا لم و تا بع ا ن غطا سين ا نتشلو ا جثث ا لغر قى و قا ل مصد ر طبي ا ن ا لا صا با ت تتر ا و ح بين خد و ش و كسو ر و صد ما ت عصبية
ا لنصر يتقد م
با مكا نك ا ذ ا ما و ضعتها على ا ذ نك ا ن تسمع هد ير ا لبحر سيتسنى لك ا ن تسمع همسا ت ا د ق ا لمخلو قا ت في ا بعد ا لا عما ق غو ر ا لكن لا تفعل ذ لك ا لا ن ينبغي ا ن تكو ن لو حد ك ا نت و ا لقو قعة و لا ثا لث بينكما
و ا و صى ا لمو تمر بد عم جهو د ا كا د يمية ا لا ما ر ا ت للتطو ع و ا لتي تعمل تحت مظلة ا لا تحا د ا لعر بي للتطو ع و تنظيم تد ر يبا ت د و ر ية ميد ا نية بمشا ر كة مختلف ا لمو سسا ت ا لحكو مية و ا لخا صة ا لمعنية محليا و عا لميا لتبا د ل ا لخبر ا ت و ا لتجا ر ب في مجا ل ا لعمل ا لتطو عي ا لميد ا ني و ضر و ر ة ا نشا ء سجل للكو ا د ر ا لو طنية ا لتطو عية ا لا د ا ر ية و ا لطبية و و ضع ا لية لا ستد عا ىها و قت ا لحا جة للمشا ر كة في ا لمها م ا لا نسا نية محليا و عا لميا
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-04 19:53:17.723175 for dataset news_dataset
####################################################################################################
####################################################################################################
Samples Count: 2,784,041
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 151,557
####################################################################################################
####################################################################################################
All Tokens Count: 326,024,667
####################################################################################################
####################################################################################################
vocab/tokens: 0.0005
####################################################################################################
####################################################################################################
Tokens Entropy: 7.9806
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset news_dataset at 2023-03-04 19:54:03.232971
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-04 19:54:03.233006 for dataset news_dataset
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

و ڡا لٮ مصا د ر ا مٮىه و طٮىه ا ں ا لحا ڡله كا ٮٮ ڡى طر ىڡها مں ا لمٮىا ا لى محا ڡطه ٮٮى سو ىڡ ا لمحا و ر ه و ا ں سا ىڡها حا و ل ٮڡا د ى ا لا صطد ا م ٮسا حٮه ڡا د مه ڡى ا لا ٮحا ه ا لمعا كس ڡا ٮڡلٮٮ ا لحا ڡله و سڡطٮ ڡى ٮر عه ا لا ٮر ا هىمىه ا لمحا و ر ه للطر ىٯ و ا صا ڡ ا ں حمىع ا لڡٮلى ٮڡر ىٮا ما ٮو ا عر ڡا ڡى ٮر عه ا لا ٮر ا هىمىه و هى مں ا طو ل ا لٮر ع ڡى ا لعا لم و ٮا ٮع ا ں عطا سىں ا ٮٮسلو ا حٮٮ ا لعر ڡى و ڡا ل مصد ر طٮى ا ں ا لا صا ٮا ٮ ٮٮر ا و ح ٮىں حد و س و كسو ر و صد ما ٮ عصٮىه
ا لٮصر ىٮڡد م
ٮا مكا ٮك ا د ا ما و صعٮها على ا د ٮك ا ں ٮسمع هد ىر ا لٮحر سىٮسٮى لك ا ں ٮسمع همسا ٮ ا د ٯ ا لمحلو ڡا ٮ ڡى ا ٮعد ا لا عما ٯ عو ر ا لكں لا ٮڡعل د لك ا لا ں ىٮٮعى ا ں ٮكو ں لو حد ك ا ٮٮ و ا لڡو ڡعه و لا ٮا لٮ ٮىٮكما
و ا و صى ا لمو ٮمر ٮد عم حهو د ا كا د ىمىه ا لا ما ر ا ٮ للٮطو ع و ا لٮى ٮعمل ٮحٮ مطله ا لا ٮحا د ا لعر ٮى للٮطو ع و ٮٮطىم ٮد ر ىٮا ٮ د و ر ىه مىد ا ٮىه ٮمسا ر كه محٮلڡ ا لمو سسا ٮ ا لحكو مىه و ا لحا صه ا لمعٮىه محلىا و عا لمىا لٮٮا د ل ا لحٮر ا ٮ و ا لٮحا ر ٮ ڡى محا ل ا لعمل ا لٮطو عى ا لمىد ا ٮى و صر و ر ه ا ٮسا ء سحل للكو ا د ر ا لو طٮىه ا لٮطو عىه ا لا د ا ر ىه و ا لطٮىه و و صع ا لىه لا سٮد عا ىها و ڡٮ ا لحا حه للمسا ر كه ڡى ا لمها م ا لا ٮسا ٮىه محلىا و عا لمىا
####################################################################################################
####################################################################################################
Undotted Samples Count: 2,784,041
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 92,472
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 326,024,667
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0003
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 7.4952
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 59,085
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset news_dataset at 2023-03-04 20:12:49.657769
####################################################################################################
