####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:

و ك ش ف ت ص ح ي ف ة و ا ش ن ط ن ب و س ت ا ن ش ر ك ة م ا ي ك ر و س و ف ت ت ع ت ز م ا ت ا ح ة ع ر ض ا ل ح م ل ا ت ا ل ا ع ل ا ن ي ة ل ل ا ح ز ا ب ا ل س ي ا س ي ة ا ل ا م ي ر ك ي ة ع ب ر ا ل خ د م ة ا ل خ ا ص ة ب ا ل ت ر ف ي ه ا ل م ن ز ل ي و ب د ا ت ف ي ا ل ت و ا ص ل م ع م ج م و ع ة م ن ا ل س ي ا س ي ي ن ل ل ب د ء ف ي ا س ت خ د ا م ا ل خ د م ة ا ل ا ع ل ا ن ي ة ا ل ج د ي د ة ح ي ث س ت ظ ه ر ت ل ك ا ل ا ع ل ا ن ا ت ض م ن و ا ج ه ة ا ل م س ت خ د م ا ل ر ى ي س ي ة ب ا ل خ د م ة
و ف ي ا ل ا و ن ة ا ل ا خ ي ر ة ش غ ل ت ش ا ن غ م ن ص ب ا ل م ن ت ج ا ل ت ن ف ي ذ ي ف ي ل و ل ا ب ا ي ف و ر ب ي ل ا و ل ع م ل م ن ا خ ر ا ج ب ي ن و ا ف ي ل ي ب و ن و ب ط و ل ة ر و ب ر ت ف ر ي ن د و ك ل ي م ا ن س ب و ز ي و ف و ر س ت و ا ي ت ي ك ر
و ا ج ر ي ت ا ل ق ر ع ة ع ل ى ا ل م ن ت خ ب ا ت ا ل س ت ة ا ل م ت ب ق ي ة ب ع د ت ق س ي م ه ا ا ل ى م س ت و ي ا ت ا ي ض ا ط ب ق ا ل ل ت ص ن ي ف ا ل د و ل ي و ا ف ر ز ت ع ن و ق و ع م ص ر و ا ل ج ز ا ى ر و ا س ت ر ا ل ي ا ض م ن ا ل م ج م و ع ة ا ل ا و ل ى و ا ل م ا ن ي ا و ا ذ ر ب ي ج ا ن و ف ر ن س ا ض م ن ا ل م ج م و ع ة ا ل ث ا ن ي ة و ت م ا ل ا ع ل ا ن ع ن ا ن ط ل ا ق م ب ا ر ي ا ت ا ل د و ر ا ل ت م ه ي د ي ا ل ج ا ر ي ب ص ا ل ة ن ا د ي ا ل ج ز ي ر ة و ت س ت م ر ل ق ا ء ا ت ه ح ت ى م ا ي و و ت ق ا م م ب ا ر ا ت ا ا ل د و ر ن ص ف ا ل ن ه ا ى ي ي و م و م ب ا ر ا ة ا ل د و ر ا ل ن ه ا ى ي ف ي م ا ي و
ا ل م ش ر و ع ا ل و ط ن ي
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-06 04:25:43.536141 for dataset news_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 2,784,041
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 663,796,116
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2503
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset news_dataset tokenized by CharacterTokenizer at 2023-03-06 04:26:54.955196
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-06 04:26:54.955226 for dataset news_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:

و ك س ڡ ٮ ص ح ى ڡ ه و ا س ں ط ں ٮ و س ٮ ا ں س ر ك ه م ا ى ك ر و س و ڡ ٮ ٮ ع ٮ ر م ا ٮ ا ح ه ع ر ص ا ل ح م ل ا ٮ ا ل ا ع ل ا ں ى ه ل ل ا ح ر ا ٮ ا ل س ى ا س ى ه ا ل ا م ى ر ك ى ه ع ٮ ر ا ل ح د م ه ا ل ح ا ص ه ٮ ا ل ٮ ر ڡ ى ه ا ل م ں ر ل ى و ٮ د ا ٮ ڡ ى ا ل ٮ و ا ص ل م ع م ح م و ع ه م ں ا ل س ى ا س ى ى ں ل ل ٮ د ء ڡ ى ا س ٮ ح د ا م ا ل ح د م ه ا ل ا ع ل ا ں ى ه ا ل ح د ى د ه ح ى ٮ س ٮ ط ه ر ٮ ل ك ا ل ا ع ل ا ں ا ٮ ص م ں و ا ح ه ه ا ل م س ٮ ح د م ا ل ر ى ى س ى ه ٮ ا ل ح د م ه
و ڡ ى ا ل ا و ں ه ا ل ا ح ى ر ه س ع ل ٮ س ا ں ع م ں ص ٮ ا ل م ں ٮ ح ا ل ٮ ں ڡ ى د ى ڡ ى ل و ل ا ٮ ا ى ڡ و ر ٮ ى ل ا و ل ع م ل م ں ا ح ر ا ح ٮ ى ں و ا ڡ ى ل ى ٮ و ں و ٮ ط و ل ه ر و ٮ ر ٮ ڡ ر ى ں د و ك ل ى م ا ں س ٮ و ر ى و ڡ و ر س ٮ و ا ى ٮ ى ك ر
و ا ح ر ى ٮ ا ل ٯ ر ع ه ع ل ى ا ل م ں ٮ ح ٮ ا ٮ ا ل س ٮ ه ا ل م ٮ ٮ ٯ ى ه ٮ ع د ٮ ٯ س ى م ه ا ا ل ى م س ٮ و ى ا ٮ ا ى ص ا ط ٮ ٯ ا ل ل ٮ ص ں ى ڡ ا ل د و ل ى و ا ڡ ر ر ٮ ع ں و ٯ و ع م ص ر و ا ل ح ر ا ى ر و ا س ٮ ر ا ل ى ا ص م ں ا ل م ح م و ع ه ا ل ا و ل ى و ا ل م ا ں ى ا و ا د ر ٮ ى ح ا ں و ڡ ر ں س ا ص م ں ا ل م ح م و ع ه ا ل ٮ ا ں ى ه و ٮ م ا ل ا ع ل ا ں ع ں ا ں ط ل ا ٯ م ٮ ا ر ى ا ٮ ا ل د و ر ا ل ٮ م ه ى د ى ا ل ح ا ر ى ٮ ص ا ل ه ں ا د ى ا ل ح ر ى ر ه و ٮ س ٮ م ر ل ٯ ا ء ا ٮ ه ح ٮ ى م ا ى و و ٮ ٯ ا م م ٮ ا ر ا ٮ ا ا ل د و ر ں ص ڡ ا ل ں ه ا ى ى ى و م و م ٮ ا ر ا ه ا ل د و ر ا ل ں ه ا ى ى ڡ ى م ا ى و
ا ل م س ر و ع ا ل و ط ں ى
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 663,796,116
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8467
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset news_dataset tokenized by CharacterTokenizer at 2023-03-06 05:03:46.160507
####################################################################################################
