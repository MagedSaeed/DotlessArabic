####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
ا ك د ا ل م ه ن د س م ر و ا ن ب ن غ ل ي ط ة ا ل م د ي ر ا ل ت ن ف ي ذ ي ل م و س س ة ا ل ت ن ظ ي م ا ل ع ق ا ر ي ب د ب ي ا ن ا س ت خ د ا م ا ل ع ق و ب ا ت و ا ل غ ر ا م ا ت ا ل م ن ص و ص ع ل ي ه ا ف ي ا ل ق ا ن و ن ض د ا ل ش ر ك ا ت ا ل ع ق ا ر ي ة غ ي ر ا ل م س ج ل ة ب ح س ا ب ا ت ا ل ض م ا ن ا و ت ل ك ا ل ت ي ل م ت ح ص ل ع ل ى ت ر خ ي ص ل م م ا ر س ة ا ل ن ش ا ط ا ل ع ق ا ر ي ا خ ر م ا ت ف ك ر ف ي ه ا ل م و س س ة و ش د د ع ل ى ا ن ا ل ت ن ظ ي م ا ل ع ق ا ر ي ل ا ت س ت خ د م ا ل ق ا ن و ن س ي ف ا م س ل ط ا ض د ا ل ش ر ك ا ت ب ل ه د ف ه ا ا ل ع م ل ع ل ى خ ل ق ش ر ا ك ا ت ع م ل و ا ض ح ة و م ر ن ة م ع ك ل ا ل ش ر ك ا ت و ا ل ف ا ع ل ي ن ف ي ق ط ا ع ا ل ت ط و ي ر ا ل ع ق ا ر ي م ن ا ج ل ت ح ق ي ق ا ل ه د ف ا ل ا س ت ر ا ت ي ج ي م ن ا ن ش ا ء ا ل م و س س ة و ا ل ر ا م ي ا ل ى ت ن ظ ي م ا ل ق ط ا ع و ا ي ج ا د ق ا ع د ة ب ي ا ن ا ت ت س ا ه م ف ي ت ط و ي ر و ت ع ز ي ز ه ذ ا ا ل ق ط ا ع ا ل ح ي و ي
و ا ض ا ف م ر و ا ن ب ن غ ل ي ط ة ف ي ت ص ر ي ح ا ت ل ا ل ا ت ح ا د ا ن ا ع ل ا ن ا ل م و س س ة ع ن ق ا ى م ة ا ل ش ر ك ا ت ا ل م ر خ ص ة ل ه ا ل ا ي ع ن ي ب ا ل ض ر و ر ة ا ل ب د ء ف ي ت و ق ي ع ا ل غ ر ا م ا ت ا ل م ن ص و ص ع ل ي ه ا ف ي ا ل ق ا ن و ن ض د ا ل ش ر ك ا ت ا ل ت ي ل م ت ل ت ز م ب ا ل م ه ل ة ا ل م م ن و ح ة ل ت و ف ي ق ا و ض ا ع ا ل م ط و ر ي ن ا ل ع ق ا ر ي ي ن ف ا ل ب ر غ م م ن ح ق ا ل ه ي ى ة ف ي ف ر ض غ ر ا م ا ت ف ا ن ه ا ت ف ت ح ا ب و ا ب ه ا ا م ا م ا ل ش ر ك ا ت ا ل ت ي ل م ت ح ص ل ع ل ى ت ر ا خ ي ص و ف ق ا ل ل ق ا ن و ن ر ق م ل س ن ة ا ل خ ا ص ب ح س ا ب ا ت ض م ا ن ا ل ت ط و ي ر ا ل ع ق ا ر ي ل ت ص ح ي ح ا و ض ا ع ه ا و ن ع م ل م ن ج ا ن ب ن ا ع ل ى ت و ع ي ة و ت ث ق ي ف ا ل ش ر ك ا ت ب ا ه م ي ة ا ل ا ل ت ز ا م ب ا ل ق ا ن و ن ل ي ع م ل ا ل ج م ي ع ف ي ظ ل م ن ظ و م ة م ت ك ا م ل ة ل ا ف ت ا ا ل ى ا ن ا ل ق ا ن و ن م ن ح ا ل م و س س ة ح ق ف ر ض غ ر ا م ا ت ت ص ل ا ل ى ا ل ف د ر ه م ع ل ى ا ل ش ر ك ا ت ا ل ت ي ل م ت ق م ب ا ل ت س ج ي ل و ت ص ح ي ح ا و ض ا ع ه ا ا ل ا ا ن ه ذ ا ا ل ن ص ل ن ت س ت خ د م ه ا ل م و س س ة ح ت ى ت ت ي ح ا ل م ج ا ل ا م ا م ا ل ش ر ك ا ت ا ل ا خ ر ى ل ل د خ و ل ل ل م ن ظ و م ة ا ل ج د ي د ة
و ا ف ا د ا ل م د ي ر ا ل ت ن ف ي ذ ي ل م و س س ة ا ل ت ن ظ ي م ا ل ع ق ا ر ي ب د ب ي ا ن ا ل ق ا ى م ة ا ل ت ي ا ص د ر ت ه ا ا ل م و س س ة ا م س ا ل ا و ل و ا ل ت ي ت ض م ش ر ك ة ع ق ا ر ي ة م ع ت م د ة ل ل ع م ل و م م ا ر س ة ا ل ن ش ا ط ا ل ع ق ا ر ي ل ي س ن ه ا ي ة ا ل م ط ا ق ف ه ذ ا ا ل ر ق م ق ا ب ل ل ل ز ي ا د ة ف ي ا ل ا ي ا م ا ل م ق ب ل ة و ن ح ن ع ل ى ث ق ة ب ا ن ت ق و م ا ل ش ر ك ا ت ب ت ص ح ي ح ا و ض ا ع ه ا و ق ا ل ب ن غ ل ي ط ة ا ن ع ا م س ي ك و ن ا ل ب د ا ي ة ل ل و ص و ل ا ل ى م ر ج ع ي ة و ا ض ح ة ل ل ن ش ا ط ا ل ع ق ا ر ي ف ي ا م ا ر ة د ب ي خ ا ص ة ا ن ا ل س و ق ي ع ا ن ي ف ع ل ا م ن غ ي ا ب ب ي ا ن ا ت ح ق ي ق ي ة و م ع ل و م ا ت ح و ل ا ل ق ط ا ع ا ل ع ق ا ر ي و ا ل ش ر ك ا ت ا ل ع ا م ل ة ف ي ه و ه و م ا ن س ع ى ل ا ي ج ا د ه م ن خ ل ا ل ا ل م و س س ة ك م ا س ي ك و ن ع ا م خ ا ل ي ا م ن ا ل م ط و ر ي ن ا ل ع ق ا ر ي ي ن غ ي ر ا ل م ر خ ص ي ن ل ت ص ب ح ه ن ا ك ر ك ي ز ة ا س ا س ي ة ش ا م ل ة ل ل س و ق ا ل ع ق ا ر ي ة ف ي د ب ي م ن و ه ا ا ل ى ا ن ا ل ع ا م ا ل ج ا ر ي س ي ش ه د ص د و ر ا ل ل و ا ى ح ا ل ت ن ف ي ذ ي ة و ا ل ت ن ظ ي م ي ة ل ل ع د ي د م ن ا ل ق و ا ن ي ن ذ ا ت ا ل ص ل ة ب ا ل ن ش ا ط ا ل ع ق ا ر ي و ب ن ا ء م ر ك ز م ع ل و م ا ت ح و ل ا ل ق ط ا ع
و ا ف ا د ل ا ت و ج د ا ر ق ا م ن ه ا ى ي ة ح و ل ا ل ش ر ك ا ت ا ل ت ي ت م ا ر س ن ش ا ط ا ع ق ا ر ي ا و ب ا ل ت ا ل ي ف ت س ج ي ل ش ر ك ة ل د ى ا ل م و س س ة ب ن ه ا ي ة ا ل م ه ل ة ا ل ت ي ا ن ت ه ت ف ي د ي س م ب ر ا ل م ا ض ي ن ع ت ب ر ه ا ا ل ب د ا ي ة و ا ل ل ب ن ة ا ل ا و ل ى ل ق ا ع د ة ب ي ا ن ا ت ح و ل ا ل س و ق ا ل ع ق ا ر ي ة ف ي د ب ي و ه ذ ه ا ل خ ط و ة س ت ل ي ه ا خ ط و ا ت ا خ ر ى ل ت ش ك ل ف ي م ج م ل ه ا م ر ج ع م ع ل و م ا ت ي ح و ل ا ل ن ش ا ط ا ل ع ق ا ر ي س و ا ء م ن ح ي ث ع د د ا ل ش ر ك ا ت و ا ل م ش ر و ع ا ت ا ل ت ط و ي ر ي ة و ح ج م ا ل ق ط ا ع و ك ل م ا ي ت ع ل ق ب ه و ح و ل ا ل م و ق ف م ن ت ع ا م ل ا ل م س ت ث م ر ي ن و م ش ت ر ي ا ل ع ق ا ر ا ت م ع ا ل ش ر ك ا ت غ ي ر ا ل م س ج ل ة ف ي ا ل م و س س ة و خ ا ر ج ق ا ى م ة ا ل ش ر ك ة ا و ض ح م ر و ا ن ب ن غ ل ي ط ة ا ن ا ل م س و و ل ي ة ف ي ا ل ت ع ا م ل م ع م ث ل ه ذ ه ا ل ش ر ك ا ت ت ق ع ع ل ى ع ا ت ق ا ل م س ت ث م ر و ا ل م ش ت ر ي ن ف س ه و ع ل ي ه ا ن ي ت خ ذ ا ل ق ر ا ر و ي ت ح م ل م س و و ل ي ت ه ك ا م ل ة و ا ل ك ر ة ا ل ا ن ف ي م ل ع ب ا ل م س ت ث م ر ف ا ي ت ع ا م ل م ع ش ر ك ة غ ي ر م ر خ ص ة م ن م و س س ة ا ل ت ن ظ ي م ا ل ع ق ا ر ي ل ن ت ك و ن ل ن ا م س و و ل ي ة ت ج ا ه ه ا ا ذ ا م ا ح د ث ا ي خ ل ا ف ا و ا خ ط ا ء ب ي ن ا ل م س ت ث م ر و ت ل ك ا ل ش ر ك ة و م ن ه ن ا ف ي ج ب ع ل ى ا ي م س ت ث م ر ا ن ي ر ا ج ع ا ل م و س س ة ل ل ت ع ر ف ع ل ى س ج ل ا ع م ا ل ا ي ش ر ك ة ق ب ل ا ل ت ع ا م ل م ع ه ا ل ل ت ا ك د م ن ك و ن ه ا م س ج ل ة ل د ى ا ل م و س س ة ا و ل ا ح ت ى ي ت خ ذ ق ر ا ر ه ا س س ت ن ا د ا ل م ع ل و م ا ت و ب ي ا ن ا ت ص ح ي ح ة و ح ت ى ل ا ي ق ع ض ح ي ة ل ا ي ة ا خ ط ا ء ا و ش ر ك ا ت م ج ه و ل ة م و ك د ا ا ن م ه م ت ن ا ا ل م ح ا ف ظ ة ع ل ى ح ق و ق ج م ي ع م ك و ن ا ت ا ل م ن ت ج ا ل ع ق ا ر ي و ا ل ا ف ر ا د س و ا ء ك ا ن م ط و ر ا ا و م س ت ث م ر ا
و ق ع ت ا ل ا م ا ر ا ت م ذ ك ر ة ت ف ا ه م ل ل ن ق ل ا ل ج و ي م ع ج م ه و ر ي ة ن ي ب ا ل ف ي ك ا ت م ا ن د و م و خ ر ا ت ق ض ي ب ز ي ا د ة ع د د ا ل ر ح ل ا ت ا ل ج و ي ة ل ل ن ا ق ل ا ت ا ل و ط ن ي ة ل ك ل ا ا ل ب ل د ي ن
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-27 05:44:55.702424 for dataset news_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Samples Count: 2,784,041
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 663,796,116
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2503
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset news_dataset tokenized by CharacterTokenizer at 2023-03-27 05:46:13.321569
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-27 05:46:13.321597 for dataset news_dataset tokenized by CharacterTokenizer
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Create an undotted tokens frequency mapping and save it to a json file
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ا ك د ا ل م ه ں د س م ر و ا ں ٮ ں ع ل ى ط ه ا ل م د ى ر ا ل ٮ ں ڡ ى د ى ل م و س س ه ا ل ٮ ں ط ى م ا ل ع ٯ ا ر ى ٮ د ٮ ى ا ں ا س ٮ ح د ا م ا ل ع ٯ و ٮ ا ٮ و ا ل ع ر ا م ا ٮ ا ل م ں ص و ص ع ل ى ه ا ڡ ى ا ل ٯ ا ں و ں ص د ا ل س ر ك ا ٮ ا ل ع ٯ ا ر ى ه ع ى ر ا ل م س ح ل ه ٮ ح س ا ٮ ا ٮ ا ل ص م ا ں ا و ٮ ل ك ا ل ٮ ى ل م ٮ ح ص ل ع ل ى ٮ ر ح ى ص ل م م ا ر س ه ا ل ں س ا ط ا ل ع ٯ ا ر ى ا ح ر م ا ٮ ڡ ك ر ڡ ى ه ا ل م و س س ه و س د د ع ل ى ا ں ا ل ٮ ں ط ى م ا ل ع ٯ ا ر ى ل ا ٮ س ٮ ح د م ا ل ٯ ا ں و ں س ى ڡ ا م س ل ط ا ص د ا ل س ر ك ا ٮ ٮ ل ه د ڡ ه ا ا ل ع م ل ع ل ى ح ل ٯ س ر ا ك ا ٮ ع م ل و ا ص ح ه و م ر ں ه م ع ك ل ا ل س ر ك ا ٮ و ا ل ڡ ا ع ل ى ں ڡ ى ٯ ط ا ع ا ل ٮ ط و ى ر ا ل ع ٯ ا ر ى م ں ا ح ل ٮ ح ٯ ى ٯ ا ل ه د ڡ ا ل ا س ٮ ر ا ٮ ى ح ى م ں ا ں س ا ء ا ل م و س س ه و ا ل ر ا م ى ا ل ى ٮ ں ط ى م ا ل ٯ ط ا ع و ا ى ح ا د ٯ ا ع د ه ٮ ى ا ں ا ٮ ٮ س ا ه م ڡ ى ٮ ط و ى ر و ٮ ع ر ى ر ه د ا ا ل ٯ ط ا ع ا ل ح ى و ى
و ا ص ا ڡ م ر و ا ں ٮ ں ع ل ى ط ه ڡ ى ٮ ص ر ى ح ا ٮ ل ا ل ا ٮ ح ا د ا ں ا ع ل ا ں ا ل م و س س ه ع ں ٯ ا ى م ه ا ل س ر ك ا ٮ ا ل م ر ح ص ه ل ه ا ل ا ى ع ں ى ٮ ا ل ص ر و ر ه ا ل ٮ د ء ڡ ى ٮ و ٯ ى ع ا ل ع ر ا م ا ٮ ا ل م ں ص و ص ع ل ى ه ا ڡ ى ا ل ٯ ا ں و ں ص د ا ل س ر ك ا ٮ ا ل ٮ ى ل م ٮ ل ٮ ر م ٮ ا ل م ه ل ه ا ل م م ں و ح ه ل ٮ و ڡ ى ٯ ا و ص ا ع ا ل م ط و ر ى ں ا ل ع ٯ ا ر ى ى ں ڡ ا ل ٮ ر ع م م ں ح ٯ ا ل ه ى ى ه ڡ ى ڡ ر ص ع ر ا م ا ٮ ڡ ا ں ه ا ٮ ڡ ٮ ح ا ٮ و ا ٮ ه ا ا م ا م ا ل س ر ك ا ٮ ا ل ٮ ى ل م ٮ ح ص ل ع ل ى ٮ ر ا ح ى ص و ڡ ٯ ا ل ل ٯ ا ں و ں ر ٯ م ل س ں ه ا ل ح ا ص ٮ ح س ا ٮ ا ٮ ص م ا ں ا ل ٮ ط و ى ر ا ل ع ٯ ا ر ى ل ٮ ص ح ى ح ا و ص ا ع ه ا و ں ع م ل م ں ح ا ں ٮ ں ا ع ل ى ٮ و ع ى ه و ٮ ٮ ٯ ى ڡ ا ل س ر ك ا ٮ ٮ ا ه م ى ه ا ل ا ل ٮ ر ا م ٮ ا ل ٯ ا ں و ں ل ى ع م ل ا ل ح م ى ع ڡ ى ط ل م ں ط و م ه م ٮ ك ا م ل ه ل ا ڡ ٮ ا ا ل ى ا ں ا ل ٯ ا ں و ں م ں ح ا ل م و س س ه ح ٯ ڡ ر ص ع ر ا م ا ٮ ٮ ص ل ا ل ى ا ل ڡ د ر ه م ع ل ى ا ل س ر ك ا ٮ ا ل ٮ ى ل م ٮ ٯ م ٮ ا ل ٮ س ح ى ل و ٮ ص ح ى ح ا و ص ا ع ه ا ا ل ا ا ں ه د ا ا ل ں ص ل ں ٮ س ٮ ح د م ه ا ل م و س س ه ح ٮ ى ٮ ٮ ى ح ا ل م ح ا ل ا م ا م ا ل س ر ك ا ٮ ا ل ا ح ر ى ل ل د ح و ل ل ل م ں ط و م ه ا ل ح د ى د ه
و ا ڡ ا د ا ل م د ى ر ا ل ٮ ں ڡ ى د ى ل م و س س ه ا ل ٮ ں ط ى م ا ل ع ٯ ا ر ى ٮ د ٮ ى ا ں ا ل ٯ ا ى م ه ا ل ٮ ى ا ص د ر ٮ ه ا ا ل م و س س ه ا م س ا ل ا و ل و ا ل ٮ ى ٮ ص م س ر ك ه ع ٯ ا ر ى ه م ع ٮ م د ه ل ل ع م ل و م م ا ر س ه ا ل ں س ا ط ا ل ع ٯ ا ر ى ل ى س ں ه ا ى ه ا ل م ط ا ٯ ڡ ه د ا ا ل ر ٯ م ٯ ا ٮ ل ل ل ر ى ا د ه ڡ ى ا ل ا ى ا م ا ل م ٯ ٮ ل ه و ں ح ں ع ل ى ٮ ٯ ه ٮ ا ں ٮ ٯ و م ا ل س ر ك ا ٮ ٮ ٮ ص ح ى ح ا و ص ا ع ه ا و ٯ ا ل ٮ ں ع ل ى ط ه ا ں ع ا م س ى ك و ں ا ل ٮ د ا ى ه ل ل و ص و ل ا ل ى م ر ح ع ى ه و ا ص ح ه ل ل ں س ا ط ا ل ع ٯ ا ر ى ڡ ى ا م ا ر ه د ٮ ى ح ا ص ه ا ں ا ل س و ٯ ى ع ا ں ى ڡ ع ل ا م ں ع ى ا ٮ ٮ ى ا ں ا ٮ ح ٯ ى ٯ ى ه و م ع ل و م ا ٮ ح و ل ا ل ٯ ط ا ع ا ل ع ٯ ا ر ى و ا ل س ر ك ا ٮ ا ل ع ا م ل ه ڡ ى ه و ه و م ا ں س ع ى ل ا ى ح ا د ه م ں ح ل ا ل ا ل م و س س ه ك م ا س ى ك و ں ع ا م ح ا ل ى ا م ں ا ل م ط و ر ى ں ا ل ع ٯ ا ر ى ى ں ع ى ر ا ل م ر ح ص ى ں ل ٮ ص ٮ ح ه ں ا ك ر ك ى ر ه ا س ا س ى ه س ا م ل ه ل ل س و ٯ ا ل ع ٯ ا ر ى ه ڡ ى د ٮ ى م ں و ه ا ا ل ى ا ں ا ل ع ا م ا ل ح ا ر ى س ى س ه د ص د و ر ا ل ل و ا ى ح ا ل ٮ ں ڡ ى د ى ه و ا ل ٮ ں ط ى م ى ه ل ل ع د ى د م ں ا ل ٯ و ا ں ى ں د ا ٮ ا ل ص ل ه ٮ ا ل ں س ا ط ا ل ع ٯ ا ر ى و ٮ ں ا ء م ر ك ر م ع ل و م ا ٮ ح و ل ا ل ٯ ط ا ع
و ا ڡ ا د ل ا ٮ و ح د ا ر ٯ ا م ں ه ا ى ى ه ح و ل ا ل س ر ك ا ٮ ا ل ٮ ى ٮ م ا ر س ں س ا ط ا ع ٯ ا ر ى ا و ٮ ا ل ٮ ا ل ى ڡ ٮ س ح ى ل س ر ك ه ل د ى ا ل م و س س ه ٮ ں ه ا ى ه ا ل م ه ل ه ا ل ٮ ى ا ں ٮ ه ٮ ڡ ى د ى س م ٮ ر ا ل م ا ص ى ں ع ٮ ٮ ر ه ا ا ل ٮ د ا ى ه و ا ل ل ٮ ں ه ا ل ا و ل ى ل ٯ ا ع د ه ٮ ى ا ں ا ٮ ح و ل ا ل س و ٯ ا ل ع ٯ ا ر ى ه ڡ ى د ٮ ى و ه د ه ا ل ح ط و ه س ٮ ل ى ه ا ح ط و ا ٮ ا ح ر ى ل ٮ س ك ل ڡ ى م ح م ل ه ا م ر ح ع م ع ل و م ا ٮ ى ح و ل ا ل ں س ا ط ا ل ع ٯ ا ر ى س و ا ء م ں ح ى ٮ ع د د ا ل س ر ك ا ٮ و ا ل م س ر و ع ا ٮ ا ل ٮ ط و ى ر ى ه و ح ح م ا ل ٯ ط ا ع و ك ل م ا ى ٮ ع ل ٯ ٮ ه و ح و ل ا ل م و ٯ ڡ م ں ٮ ع ا م ل ا ل م س ٮ ٮ م ر ى ں و م س ٮ ر ى ا ل ع ٯ ا ر ا ٮ م ع ا ل س ر ك ا ٮ ع ى ر ا ل م س ح ل ه ڡ ى ا ل م و س س ه و ح ا ر ح ٯ ا ى م ه ا ل س ر ك ه ا و ص ح م ر و ا ں ٮ ں ع ل ى ط ه ا ں ا ل م س و و ل ى ه ڡ ى ا ل ٮ ع ا م ل م ع م ٮ ل ه د ه ا ل س ر ك ا ٮ ٮ ٯ ع ع ل ى ع ا ٮ ٯ ا ل م س ٮ ٮ م ر و ا ل م س ٮ ر ى ں ڡ س ه و ع ل ى ه ا ں ى ٮ ح د ا ل ٯ ر ا ر و ى ٮ ح م ل م س و و ل ى ٮ ه ك ا م ل ه و ا ل ك ر ه ا ل ا ں ڡ ى م ل ع ٮ ا ل م س ٮ ٮ م ر ڡ ا ى ٮ ع ا م ل م ع س ر ك ه ع ى ر م ر ح ص ه م ں م و س س ه ا ل ٮ ں ط ى م ا ل ع ٯ ا ر ى ل ں ٮ ك و ں ل ں ا م س و و ل ى ه ٮ ح ا ه ه ا ا د ا م ا ح د ٮ ا ى ح ل ا ڡ ا و ا ح ط ا ء ٮ ى ں ا ل م س ٮ ٮ م ر و ٮ ل ك ا ل س ر ك ه و م ں ه ں ا ڡ ى ح ٮ ع ل ى ا ى م س ٮ ٮ م ر ا ں ى ر ا ح ع ا ل م و س س ه ل ل ٮ ع ر ڡ ع ل ى س ح ل ا ع م ا ل ا ى س ر ك ه ٯ ٮ ل ا ل ٮ ع ا م ل م ع ه ا ل ل ٮ ا ك د م ں ك و ں ه ا م س ح ل ه ل د ى ا ل م و س س ه ا و ل ا ح ٮ ى ى ٮ ح د ٯ ر ا ر ه ا س س ٮ ں ا د ا ل م ع ل و م ا ٮ و ٮ ى ا ں ا ٮ ص ح ى ح ه و ح ٮ ى ل ا ى ٯ ع ص ح ى ه ل ا ى ه ا ح ط ا ء ا و س ر ك ا ٮ م ح ه و ل ه م و ك د ا ا ں م ه م ٮ ں ا ا ل م ح ا ڡ ط ه ع ل ى ح ٯ و ٯ ح م ى ع م ك و ں ا ٮ ا ل م ں ٮ ح ا ل ع ٯ ا ر ى و ا ل ا ڡ ر ا د س و ا ء ك ا ں م ط و ر ا ا و م س ٮ ٮ م ر ا
و ٯ ع ٮ ا ل ا م ا ر ا ٮ م د ك ر ه ٮ ڡ ا ه م ل ل ں ٯ ل ا ل ح و ى م ع ح م ه و ر ى ه ں ى ٮ ا ل ڡ ى ك ا ٮ م ا ں د و م و ح ر ا ٮ ٯ ص ى ٮ ر ى ا د ه ع د د ا ل ر ح ل ا ٮ ا ل ح و ى ه ل ل ں ا ٯ ل ا ٮ ا ل و ط ں ى ه ل ك ل ا ا ل ٮ ل د ى ں
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 663,796,116
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8467
####################################################################################################
####################################################################################################
Average tokens length: 1.0000
####################################################################################################
####################################################################################################
Top 0.1% average tokens length: 1.0000
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset news_dataset tokenized by CharacterTokenizer at 2023-03-27 06:24:12.568420
####################################################################################################
