####################################################################################################
Original Number of Samples:
650,986
####################################################################################################
####################################################################################################
Number of Samples after dropping duplicates:
637,565
####################################################################################################
####################################################################################################
Tokenize the Dataset with CharacterTokenizer
####################################################################################################
####################################################################################################
Process the Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before collecting statistics:
ي ت و ض ا ث ل ا ث ا ي ر ف ع ه ا ل ى ا ل ن ب ي ص ل ى ا ل ل ه ع ل ي ه و س ل م
ي غ ت س ل م ن ا ل ج ن ا ب ة ث م ي ج ي ء و ل ه ق ف ق ف ة ف ي س ت د ف ى ب ي و ل م ا غ ت س ل
ل ا ت ب ل ق ا ى م ا ف م ا ب ل ت ب ع د ق ا ى م ا
ا ن ت ن ه ي ت ا ل ن ا س ا ن ي ص ل و ا ف ي ن ع ا ل ه م ف ق ا ل ل ا ل ع م ر ا ل ل ه م ا ن ه ي ت ا ل ن ا س ا ن ي ص ل و ا ف ي ن ع ا ل ه م غ ي ر ا ن ي و ر ب ه ذ ه ا ل ح ر م ة ح ت ى ق ا ل ه ا ث ل ا ث ا ل ق د ر ا ي ت ا ل ن ب ي ص ل ى ا ل ل ه ع ل ي ه و س ل م ه ه ن ا ع ن د ا ل م ق ا م ي ص ل ي و ع ل ي ه ن ع ل ا ه ث م ا ن ص ر ف و ه م ا ع ل ي ه
ا ن ا ن ا ص د ق ت ف ص د ق ن ي و ا ن ا ن ا ك ذ ب ت ف ك ذ ب ن ي ق ا ل ف ا ف ع ل ق ا ل ف ا ن ش د ك ب ا ل ل ه ه ل س م ع ت ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ي ه و س ل م ي ن ه ى ع ن ل ب س ا ل ذ ه ب ق ا ل ن ع م ق ا ل ف ا ن ش د ك ب ا ل ل ه ه ل ت ع ل م ا ن ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ي ه و س ل م ن ه ى ع ن ل ب س ا ل ح ر ي ر ق ا ل ن ع م ق ا ل ف ا ن ش د ك ب ا ل ل ه ه ل ت ع ل م ا ن ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ي ه و س ل م ن ه ى ع ن ل ب س ج ل و د ا ل س ب ا ع و ا ل ر ك و ب ع ل ي ه ا ق ا ل ن ع م
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Started at 2023-03-04 21:31:37.917006 for dataset sanadset_hadeeth_dataset
####################################################################################################
####################################################################################################
Samples Count: 637,565
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 31
####################################################################################################
####################################################################################################
All Tokens Count: 112,879,152
####################################################################################################
####################################################################################################
vocab/tokens: 0.0000
####################################################################################################
####################################################################################################
Tokens Entropy: 4.2364
####################################################################################################
####################################################################################################
Dotted Statistics Analysis Finished for dataset sanadset_hadeeth_dataset at 2023-03-04 21:31:49.276164
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Started at 2023-03-04 21:31:49.276193 for dataset sanadset_hadeeth_dataset
####################################################################################################
####################################################################################################
Undotting Dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ى ٮ و ص ا ٮ ل ا ٮ ا ى ر ڡ ع ه ا ل ى ا ل ں ٮ ى ص ل ى ا ل ل ه ع ل ى ه و س ل م
ى ع ٮ س ل م ں ا ل ح ں ا ٮ ه ٮ م ى ح ى ء و ل ه ٯ ڡ ٯ ڡ ه ڡ ى س ٮ د ڡ ى ٮ ى و ل م ا ع ٮ س ل
ل ا ٮ ٮ ل ٯ ا ى م ا ڡ م ا ٮ ل ٮ ٮ ع د ٯ ا ى م ا
ا ں ٮ ں ه ى ٮ ا ل ں ا س ا ں ى ص ل و ا ڡ ى ں ع ا ل ه م ڡ ٯ ا ل ل ا ل ع م ر ا ل ل ه م ا ں ه ى ٮ ا ل ں ا س ا ں ى ص ل و ا ڡ ى ں ع ا ل ه م ع ى ر ا ں ى و ر ٮ ه د ه ا ل ح ر م ه ح ٮ ى ٯ ا ل ه ا ٮ ل ا ٮ ا ل ٯ د ر ا ى ٮ ا ل ں ٮ ى ص ل ى ا ل ل ه ع ل ى ه و س ل م ه ه ں ا ع ں د ا ل م ٯ ا م ى ص ل ى و ع ل ى ه ں ع ل ا ه ٮ م ا ں ص ر ڡ و ه م ا ع ل ى ه
ا ں ا ں ا ص د ٯ ٮ ڡ ص د ٯ ں ى و ا ں ا ں ا ك د ٮ ٮ ڡ ك د ٮ ں ى ٯ ا ل ڡ ا ڡ ع ل ٯ ا ل ڡ ا ں س د ك ٮ ا ل ل ه ه ل س م ع ٮ ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ى ه و س ل م ى ں ه ى ع ں ل ٮ س ا ل د ه ٮ ٯ ا ل ں ع م ٯ ا ل ڡ ا ں س د ك ٮ ا ل ل ه ه ل ٮ ع ل م ا ں ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ى ه و س ل م ں ه ى ع ں ل ٮ س ا ل ح ر ى ر ٯ ا ل ں ع م ٯ ا ل ڡ ا ں س د ك ٮ ا ل ل ه ه ل ٮ ع ل م ا ں ر س و ل ا ل ل ه ص ل ى ا ل ل ه ع ل ى ه و س ل م ں ه ى ع ں ل ٮ س ح ل و د ا ل س ٮ ا ع و ا ل ر ك و ٮ ع ل ى ه ا ٯ ا ل ں ع م
####################################################################################################
####################################################################################################
Undotted Samples Count: 637,565
####################################################################################################
####################################################################################################
Unique Vocabulary Count: 19
####################################################################################################
####################################################################################################
All Undotted Tokens Count: 112,879,152
####################################################################################################
####################################################################################################
undotted vocab/undotted tokens: 0.0000
####################################################################################################
####################################################################################################
Undotted Tokens Entropy: 3.8604
####################################################################################################
####################################################################################################
dotted voacb - undotted vocab: 12
####################################################################################################
####################################################################################################
Undotted Statistics Analysis Finished for dataset sanadset_hadeeth_dataset at 2023-03-04 21:38:05.584887
####################################################################################################
