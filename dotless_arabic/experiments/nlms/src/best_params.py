best_params = {
    "quran": {
        "WordTokenization": dict(
            num_layers=2,
            learning_rate=0.001,
            hidden_size=256,
            embedding_size=512,
            dropout_prop=0.2,
        ),
        # "FarasaMorphologicalTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "DisjointLetterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "CharacterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
    },
    "sanadset_hadeeth": {
        "WordTokenization": dict(
            num_layers=2,
            learning_rate=0.001,
            hidden_size=256,
            embedding_size=512,
            dropout_prop=0.2,
        ),
        # "FarasaMorphologicalTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "DisjointLetterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "CharacterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
    },
    "poems": {
        "WordTokenization": dict(
            num_layers=2,
            learning_rate=0.001,
            hidden_size=256,
            embedding_size=512,
            dropout_prop=0.2,
        ),
        # "FarasaMorphologicalTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "DisjointLetterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "CharacterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
    },
    "wikipedia": {
        "WordTokenization": dict(
            num_layers=2,
            learning_rate=0.001,
            hidden_size=512,
            embedding_size=512,
            dropout_prop=0.2,
        ),
        # "FarasaMorphologicalTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "DisjointLetterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
        # "CharacterTokenizer": dict(
        #     num_layers=2,
        #     learning_rate=0.001,
        #     hidden_size=256,
        #     embedding_size=512,
        #     dropout_prop=0.2,
        # ),
    },
    # "news": {
    #     "WordTokenization": dict(
    #         num_layers=2,
    #         learning_rate=0.001,
    #         hidden_size=512,
    #         embedding_size=512,
    #         dropout_prop=0.2,
    #     ),
    # "FarasaMorphologicalTokenizer": dict(
    #     num_layers=2,
    #     learning_rate=0.001,
    #     hidden_size=256,
    #     embedding_size=512,
    #     dropout_prop=0.2,
    # ),
    # "DisjointLetterTokenizer": dict(
    #     num_layers=2,
    #     learning_rate=0.001,
    #     hidden_size=256,
    #     embedding_size=512,
    #     dropout_prop=0.2,
    # ),
    # "CharacterTokenizer": dict(
    #     num_layers=2,
    #     learning_rate=0.001,
    #     hidden_size=256,
    #     embedding_size=512,
    #     dropout_prop=0.2,
    # ),
    # },
}
