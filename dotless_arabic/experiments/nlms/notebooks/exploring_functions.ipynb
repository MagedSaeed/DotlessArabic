{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Explorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to check and validate the functions in the NLMs training pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, to properly run this noteobook, set the root folder to be the root of this repository at the same level as dotless_arabic.\n",
    "To set this in jupyter vscode:\n",
    "\n",
    "go to setting > search for \"Notebook File Root\" > change its value from `${fileDirname}` to `${workspaceFolder}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkseem as tk\n",
    "from dotless_arabic.processing import process\n",
    "from dotless_arabic.experiments.nlms.src.utils import get_tokenizer\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_language_modeling as collect_quran_dataset\n",
    "from dotless_arabic.tokenizers import FarasaMorphologicalTokenizer, DisjointLetterTokenizer, CharacterTokenizer,WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Number of samples:\n",
      "6236\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6236"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = collect_quran_dataset()\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to compare `tokenize` and `tokenize_from_splits` methods of the tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-17 13:14:40,118 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FarasaMorphologicalTokenizer...\n"
     ]
    }
   ],
   "source": [
    "farasa_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=FarasaMorphologicalTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ويعود الفضل في توحيد اللغة العربية الى نزول القران الكريم حيث لم تكن موحدة قبل هذا العهد رغم انها كانت ذات غنى ومرونة الى ان نزل القران الكريم وتحدى الجموع ببيانه واعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب وقد وحد القران الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض كما حدث مع العديد من اللغات السامية الاخرى التي اضحت لغات باىدة واندثرت مع الزمن او لغات طالها الضعف والانحطاط وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%82%D8%B1%D8%A2%D9%86\n",
    "sample_text = process(\"\"\"\n",
    "ويعود الفضل في توحيد اللغة العربية إلى نزول القرآن الكريم، حيث لم تكن موحدة قبل هذا العهد رغم أنها كانت ذات غنى ومرونة، إلى أن نزل القرآن الكريم وتحدى الجموع ببيانه، وأعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب. وقد وحد القرآن الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض، كما حدث مع العديد من اللغات السامية الأخرى، التي أضحت لغات بائدة واندثرت مع الزمن، أو لغات طالها الضعف والانحطاط، وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث\n",
    "\"\"\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', '\"غير_معروف\"', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'وحد', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', 'حدث', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعود', '<##>', 'ال', 'فضل', '<##>', 'في', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'الى', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', '\"غير_معروف\"', 'ة', '<##>', 'قبل', '<##>', 'هذا', '<##>', 'ال', 'عهد', '<##>', '\"غير_معروف\"', '<##>', 'ان', 'ها', '<##>', 'كان', 'ت', '<##>', 'ذات', '<##>', '\"غير_معروف\"', '<##>', 'و', '\"غير_معروف\"', 'ة', '<##>', 'الى', '<##>', 'ان', '<##>', 'نزل', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'و', '\"غير_معروف\"', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'ب', 'بيان', 'ه', '<##>', 'و', 'أعطى', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'سيل', 'ا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', '\"غير_معروف\"', 'ة', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', 'من', '<##>', 'ال', 'بلاغ', 'ة', '<##>', 'و', 'ال', 'بيان', '<##>', 'ما', '<##>', '\"غير_معروف\"', '<##>', 'عن', 'ه', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'عرب', '<##>', 'و', 'قد', '<##>', 'وحد', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', '\"غير_معروف\"', 'ا', '<##>', 'كامل', 'ا', '<##>', 'و', 'حفظ', 'ها', '<##>', 'من', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', 'ال', '\"غير_معروف\"', '<##>', 'كما', '<##>', 'حدث', '<##>', 'مع', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'من', '<##>', 'ال', 'لغ', 'ات', '<##>', 'ال', '\"غير_معروف\"', 'ة', '<##>', 'ال', 'أخرى', '<##>', 'التي', '<##>', '\"غير_معروف\"', 'ت', '<##>', 'لغ', 'ات', '<##>', 'ب', '\"غير_معروف\"', 'ة', '<##>', 'و', '\"غير_معروف\"', 'ت', '<##>', 'مع', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'أو', '<##>', 'لغ', 'ات', '<##>', 'طال', 'ها', '<##>', 'ال', 'ضعف', '<##>', 'و', 'ال', '\"غير_معروف\"', '<##>', 'و', 'ب', 'ال', 'تالي', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'قدر', 'ة', '<##>', 'على', '<##>', '\"غير_معروف\"', 'ة', '<##>', 'ال', '\"غير_معروف\"', 'ات', '<##>', 'و', 'ال', '\"غير_معروف\"', 'ات', '<##>', 'التي', '<##>', 'تعرف', 'ها', '<##>', 'ال', '\"غير_معروف\"', 'ة', '<##>', 'و', 'شعوب', '<##>', 'ال', 'عالم', '<##>', 'ال', 'قديم', '<##>', 'و', 'ال', 'حديث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعود', '<##>', 'ال', 'فضل', '<##>', 'في', '<##>', 'توحيد', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'الى', '<##>', 'نزول', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'موحد', 'ة', '<##>', 'قبل', '<##>', 'هذا', '<##>', 'ال', 'عهد', '<##>', 'رغم', '<##>', 'ان', 'ها', '<##>', 'كان', 'ت', '<##>', 'ذات', '<##>', 'غنى', '<##>', 'و', 'مرون', 'ة', '<##>', 'الى', '<##>', 'ان', '<##>', 'نزل', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'و', 'تحدى', '<##>', 'ال', 'جموع', '<##>', 'ب', 'بيان', 'ه', '<##>', 'و', 'أعطى', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'سيل', 'ا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ال', 'سبك', '<##>', 'و', 'عذوب', 'ة', '<##>', 'ال', 'سجع', '<##>', 'و', 'من', '<##>', 'ال', 'بلاغ', 'ة', '<##>', 'و', 'ال', 'بيان', '<##>', 'ما', '<##>', 'عجز', '<##>', 'عن', 'ه', '<##>', 'بلغاء', '<##>', 'ال', 'عرب', '<##>', 'و', 'قد', '<##>', 'وحد', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'توحيد', 'ا', '<##>', 'كامل', 'ا', '<##>', 'و', 'حفظ', 'ها', '<##>', 'من', '<##>', 'ال', 'تلاشي', '<##>', 'و', 'ال', 'انقراض', '<##>', 'كما', '<##>', 'حدث', '<##>', 'مع', '<##>', 'ال', 'عديد', '<##>', 'من', '<##>', 'ال', 'لغ', 'ات', '<##>', 'ال', 'سامي', 'ة', '<##>', 'ال', 'أخرى', '<##>', 'التي', '<##>', 'اضح', 'ت', '<##>', 'لغ', 'ات', '<##>', 'ب', 'اىد', 'ة', '<##>', 'و', 'اندثر', 'ت', '<##>', 'مع', '<##>', 'ال', 'زمن', '<##>', 'أو', '<##>', 'لغ', 'ات', '<##>', 'طال', 'ها', '<##>', 'ال', 'ضعف', '<##>', 'و', 'ال', 'انحطاط', '<##>', 'و', 'ب', 'ال', 'تالي', '<##>', 'عدم', '<##>', 'ال', 'قدر', 'ة', '<##>', 'على', '<##>', 'مساير', 'ة', '<##>', 'ال', 'تغيير', 'ات', '<##>', 'و', 'ال', 'تجاذب', 'ات', '<##>', 'التي', '<##>', 'تعرف', 'ها', '<##>', 'ال', 'حضار', 'ة', '<##>', 'و', 'شعوب', '<##>', 'ال', 'عالم', '<##>', 'ال', 'قديم', '<##>', 'و', 'ال', 'حديث']\n"
     ]
    }
   ],
   "source": [
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.split_text(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "disjoint_letters_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=DisjointLetterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', '\"غير_معروف\"', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'غنى', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', 'عجز', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعو', 'د', '<##>', 'ا', 'لفضل', '<##>', 'في', '<##>', 'تو', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', 'لعر', '\"غير_معروف\"', '<##>', 'ا', 'لى', '<##>', 'نز', 'و', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'مو', 'حد', 'ة', '<##>', 'قبل', '<##>', 'هذ', 'ا', '<##>', 'ا', 'لعهد', '<##>', 'ر', '\"غير_معروف\"', '<##>', 'ا', 'نها', '<##>', 'كا', 'نت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غنى', '<##>', 'و', 'مر', 'و', 'نة', '<##>', 'ا', 'لى', '<##>', 'ا', 'ن', '<##>', 'نز', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'و', 'تحد', 'ى', '<##>', 'ا', '\"غير_معروف\"', 'ع', '<##>', '\"غير_معروف\"', 'نه', '<##>', 'و', 'ا', 'عطى', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', 'لعر', '\"غير_معروف\"', '<##>', '\"غير_معروف\"', '<##>', 'من', '<##>', 'حسن', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'عذ', 'و', 'بة', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'من', '<##>', 'ا', 'لبلا', 'غة', '<##>', 'و', 'ا', '\"غير_معروف\"', 'ن', '<##>', 'ما', '<##>', 'عجز', '<##>', 'عنه', '<##>', '\"غير_معروف\"', 'ء', '<##>', 'ا', 'لعر', 'ب', '<##>', 'و', 'قد', '<##>', 'و', 'حد', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', 'لعر', '\"غير_معروف\"', '<##>', 'تو', '\"غير_معروف\"', 'ا', '<##>', 'كا', 'ملا', '<##>', 'و', '\"غير_معروف\"', '<##>', 'من', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'ا', 'لا', 'نقر', 'ا', 'ض', '<##>', 'كما', '<##>', 'حد', 'ث', '<##>', 'مع', '<##>', 'ا', 'لعد', 'يد', '<##>', 'من', '<##>', 'ا', '\"غير_معروف\"', 'ت', '<##>', 'ا', 'لسا', 'مية', '<##>', 'ا', 'لا', 'خر', 'ى', '<##>', 'ا', 'لتي', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'لغا', 'ت', '<##>', 'با', 'ىد', 'ة', '<##>', 'و', 'ا', 'ند', 'ثر', 'ت', '<##>', 'مع', '<##>', 'ا', 'لز', 'من', '<##>', 'ا', 'و', '<##>', 'لغا', 'ت', '<##>', 'طا', 'لها', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'ا', 'لا', '\"غير_معروف\"', 'ط', '<##>', 'و', 'با', 'لتا', 'لي', '<##>', 'عد', 'م', '<##>', 'ا', 'لقد', 'ر', 'ة', '<##>', 'على', '<##>', 'مسا', 'ير', 'ة', '<##>', 'ا', '\"غير_معروف\"', 'ا', 'ت', '<##>', 'و', 'ا', '\"غير_معروف\"', 'ذ', 'با', 'ت', '<##>', 'ا', 'لتي', '<##>', 'تعر', 'فها', '<##>', 'ا', '\"غير_معروف\"', 'ر', 'ة', '<##>', 'و', '\"غير_معروف\"', 'ب', '<##>', 'ا', 'لعا', 'لم', '<##>', 'ا', 'لقد', 'يم', '<##>', 'و', 'ا', 'لحد', 'يث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعو', 'د', '<##>', 'ا', 'لفضل', '<##>', 'في', '<##>', 'تو', 'حيد', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'ا', 'لى', '<##>', 'نز', 'و', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'مو', 'حد', 'ة', '<##>', 'قبل', '<##>', 'هذ', 'ا', '<##>', 'ا', 'لعهد', '<##>', 'ر', 'غم', '<##>', 'ا', 'نها', '<##>', 'كا', 'نت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غنى', '<##>', 'و', 'مر', 'و', 'نة', '<##>', 'ا', 'لى', '<##>', 'ا', 'ن', '<##>', 'نز', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'و', 'تحد', 'ى', '<##>', 'ا', 'لجمو', 'ع', '<##>', 'ببيا', 'نه', '<##>', 'و', 'ا', 'عطى', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'سيلا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ا', 'لسبك', '<##>', 'و', 'عذ', 'و', 'بة', '<##>', 'ا', 'لسجع', '<##>', 'و', 'من', '<##>', 'ا', 'لبلا', 'غة', '<##>', 'و', 'ا', 'لبيا', 'ن', '<##>', 'ما', '<##>', 'عجز', '<##>', 'عنه', '<##>', 'بلغا', 'ء', '<##>', 'ا', 'لعر', 'ب', '<##>', 'و', 'قد', '<##>', 'و', 'حد', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'تو', 'حيد', 'ا', '<##>', 'كا', 'ملا', '<##>', 'و', 'حفظها', '<##>', 'من', '<##>', 'ا', 'لتلا', 'شي', '<##>', 'و', 'ا', 'لا', 'نقر', 'ا', 'ض', '<##>', 'كما', '<##>', 'حد', 'ث', '<##>', 'مع', '<##>', 'ا', 'لعد', 'يد', '<##>', 'من', '<##>', 'ا', 'للغا', 'ت', '<##>', 'ا', 'لسا', 'مية', '<##>', 'ا', 'لا', 'خر', 'ى', '<##>', 'ا', 'لتي', '<##>', 'ا', 'ضحت', '<##>', 'لغا', 'ت', '<##>', 'با', 'ىد', 'ة', '<##>', 'و', 'ا', 'ند', 'ثر', 'ت', '<##>', 'مع', '<##>', 'ا', 'لز', 'من', '<##>', 'ا', 'و', '<##>', 'لغا', 'ت', '<##>', 'طا', 'لها', '<##>', 'ا', 'لضعف', '<##>', 'و', 'ا', 'لا', 'نحطا', 'ط', '<##>', 'و', 'با', 'لتا', 'لي', '<##>', 'عد', 'م', '<##>', 'ا', 'لقد', 'ر', 'ة', '<##>', 'على', '<##>', 'مسا', 'ير', 'ة', '<##>', 'ا', 'لتغيير', 'ا', 'ت', '<##>', 'و', 'ا', 'لتجا', 'ذ', 'با', 'ت', '<##>', 'ا', 'لتي', '<##>', 'تعر', 'فها', '<##>', 'ا', 'لحضا', 'ر', 'ة', '<##>', 'و', 'شعو', 'ب', '<##>', 'ا', 'لعا', 'لم', '<##>', 'ا', 'لقد', 'يم', '<##>', 'و', 'ا', 'لحد', 'يث']\n"
     ]
    }
   ],
   "source": [
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.split_text(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعو', 'د', '<##>', 'ا', 'لفضل', '<##>', 'في', '<##>', 'تو', 'حيد', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'ا', 'لى', '<##>', 'نز', 'و', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'مو', 'حد', 'ة', '<##>', 'قبل', '<##>', 'هذ', 'ا', '<##>', 'ا', 'لعهد', '<##>', 'ر', 'غم', '<##>', 'ا', 'نها', '<##>', 'كا', 'نت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غنى', '<##>', 'و', 'مر', 'و', 'نة', '<##>', 'ا', 'لى', '<##>', 'ا', 'ن', '<##>', 'نز', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'و', 'تحد', 'ى', '<##>', 'ا', 'لجمو', 'ع', '<##>', 'ببيا', 'نه', '<##>', 'و', 'ا', 'عطى', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'سيلا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ا', 'لسبك', '<##>', 'و', 'عذ', 'و', 'بة', '<##>', 'ا', 'لسجع', '<##>', 'و', 'من', '<##>', 'ا', 'لبلا', 'غة', '<##>', 'و', 'ا', 'لبيا', 'ن', '<##>', 'ما', '<##>', 'عجز', '<##>', 'عنه', '<##>', 'بلغا', 'ء', '<##>', 'ا', 'لعر', 'ب', '<##>', 'و', 'قد', '<##>', 'و', 'حد', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'تو', 'حيد', 'ا', '<##>', 'كا', 'ملا', '<##>', 'و', 'حفظها', '<##>', 'من', '<##>', 'ا', 'لتلا', 'شي', '<##>', 'و', 'ا', 'لا', 'نقر', 'ا', 'ض', '<##>', 'كما', '<##>', 'حد', 'ث', '<##>', 'مع', '<##>', 'ا', 'لعد', 'يد', '<##>', 'من', '<##>', 'ا', 'للغا', 'ت', '<##>', 'ا', 'لسا', 'مية', '<##>', 'ا', 'لا', 'خر', 'ى', '<##>', 'ا', 'لتي', '<##>', 'ا', 'ضحت', '<##>', 'لغا', 'ت', '<##>', 'با', 'ىد', 'ة', '<##>', 'و', 'ا', 'ند', 'ثر', 'ت', '<##>', 'مع', '<##>', 'ا', 'لز', 'من', '<##>', 'ا', 'و', '<##>', 'لغا', 'ت', '<##>', 'طا', 'لها', '<##>', 'ا', 'لضعف', '<##>', 'و', 'ا', 'لا', 'نحطا', 'ط', '<##>', 'و', 'با', 'لتا', 'لي', '<##>', 'عد', 'م', '<##>', 'ا', 'لقد', 'ر', 'ة', '<##>', 'على', '<##>', 'مسا', 'ير', 'ة', '<##>', 'ا', 'لتغيير', 'ا', 'ت', '<##>', 'و', 'ا', 'لتجا', 'ذ', 'با', 'ت', '<##>', 'ا', 'لتي', '<##>', 'تعر', 'فها', '<##>', 'ا', 'لحضا', 'ر', 'ة', '<##>', 'و', 'شعو', 'ب', '<##>', 'ا', 'لعا', 'لم', '<##>', 'ا', 'لقد', 'يم', '<##>', 'و', 'ا', 'لحد', 'يث']\n"
     ]
    }
   ],
   "source": [
    "# A fixed bug discovered in the disjoint letters tokenizer. It was putting ## as a token, hence it adds a very high frequency to it.\n",
    "import re\n",
    "# rx = re.compile(r\"([اأإآءؤﻵﻹﻷدذرزو])\")\n",
    "# text = rx.sub(r\"\\1## \", sample_text)\n",
    "# text = text.replace(\"## \", \" ##\")\n",
    "# text\n",
    "print(disjoint_letters_tokenizer.split_text(sample_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "words_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ويعود', 'الفضل', 'في', 'توحيد', 'اللغة', 'العربية', 'الى', 'نزول', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', 'موحدة', 'قبل', 'هذا', 'العهد', 'رغم', 'انها', 'كانت', 'ذات', 'غنى', 'ومرونة', 'الى', 'ان', 'نزل', 'القران', 'الكريم', 'وتحدى', 'الجموع', 'ببيانه', 'واعطى', 'اللغة', 'العربية', 'سيلا', 'من', 'حسن', 'السبك', 'وعذوبة', 'السجع', 'ومن', 'البلاغة', 'والبيان', 'ما', 'عجز', 'عنه', 'بلغاء', 'العرب', 'وقد', 'وحد', 'القران', 'الكريم', 'اللغة', 'العربية', 'توحيدا', 'كاملا', 'وحفظها', 'من', 'التلاشي', 'والانقراض', 'كما', 'حدث', 'مع', 'العديد', 'من', 'اللغات', 'السامية', 'الاخرى', 'التي', 'اضحت', 'لغات', 'باىدة', 'واندثرت', 'مع', 'الزمن', 'او', 'لغات', 'طالها', 'الضعف', 'والانحطاط', 'وبالتالي', 'عدم', 'القدرة', 'على', 'مسايرة', 'التغييرات', 'والتجاذبات', 'التي', 'تعرفها', 'الحضارة', 'وشعوب', 'العالم', 'القديم', 'والحديث']\n"
     ]
    }
   ],
   "source": [
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.split_text(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "chars_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=CharacterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'ي', 'ع', 'و', 'د', '<##>', 'ا', 'ل', 'ف', 'ض', 'ل', '<##>', 'ف', 'ي', '<##>', 'ت', 'و', 'ح', 'ي', 'د', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ن', 'ز', 'و', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ح', 'ي', 'ث', '<##>', 'ل', 'م', '<##>', 'ت', 'ك', 'ن', '<##>', 'م', 'و', 'ح', 'د', 'ة', '<##>', 'ق', 'ب', 'ل', '<##>', 'ه', 'ذ', 'ا', '<##>', 'ا', 'ل', 'ع', 'ه', 'د', '<##>', 'ر', 'غ', 'م', '<##>', 'ا', 'ن', 'ه', 'ا', '<##>', 'ك', 'ا', 'ن', 'ت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غ', 'ن', 'ى', '<##>', 'و', 'م', 'ر', 'و', 'ن', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ا', 'ن', '<##>', 'ن', 'ز', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'و', 'ت', 'ح', 'د', 'ى', '<##>', 'ا', 'ل', 'ج', 'م', 'و', 'ع', '<##>', 'ب', 'ب', 'ي', 'ا', 'ن', 'ه', '<##>', 'و', 'ا', 'ع', 'ط', 'ى', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'س', 'ي', 'ل', 'ا', '<##>', 'م', 'ن', '<##>', 'ح', 'س', 'ن', '<##>', 'ا', 'ل', 'س', 'ب', 'ك', '<##>', 'و', 'ع', 'ذ', 'و', 'ب', 'ة', '<##>', 'ا', 'ل', 'س', 'ج', 'ع', '<##>', 'و', 'م', 'ن', '<##>', 'ا', 'ل', 'ب', 'ل', 'ا', 'غ', 'ة', '<##>', 'و', 'ا', 'ل', 'ب', 'ي', 'ا', 'ن', '<##>', 'م', 'ا', '<##>', 'ع', 'ج', 'ز', '<##>', 'ع', 'ن', 'ه', '<##>', 'ب', 'ل', 'غ', 'ا', 'ء', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', '<##>', 'و', 'ق', 'د', '<##>', 'و', 'ح', 'د', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ت', 'و', 'ح', 'ي', 'د', 'ا', '<##>', 'ك', 'ا', 'م', 'ل', 'ا', '<##>', 'و', 'ح', 'ف', 'ظ', 'ه', 'ا', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ت', 'ل', 'ا', 'ش', 'ي', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ق', 'ر', 'ا', 'ض', '<##>', 'ك', 'م', 'ا', '<##>', 'ح', 'د', 'ث', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ع', 'د', 'ي', 'د', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ل', 'غ', 'ا', 'ت', '<##>', 'ا', 'ل', 'س', 'ا', 'م', 'ي', 'ة', '<##>', 'ا', 'ل', 'ا', 'خ', 'ر', 'ى', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ا', 'ض', 'ح', 'ت', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ب', 'ا', 'ى', 'د', 'ة', '<##>', 'و', 'ا', 'ن', 'د', 'ث', 'ر', 'ت', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ز', 'م', 'ن', '<##>', 'ا', 'و', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ط', 'ا', 'ل', 'ه', 'ا', '<##>', 'ا', 'ل', 'ض', 'ع', 'ف', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ح', 'ط', 'ا', 'ط', '<##>', 'و', 'ب', 'ا', 'ل', 'ت', 'ا', 'ل', 'ي', '<##>', 'ع', 'د', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ر', 'ة', '<##>', 'ع', 'ل', 'ى', '<##>', 'م', 'س', 'ا', 'ي', 'ر', 'ة', '<##>', 'ا', 'ل', 'ت', 'غ', 'ي', 'ي', 'ر', 'ا', 'ت', '<##>', 'و', 'ا', 'ل', 'ت', 'ج', 'ا', 'ذ', 'ب', 'ا', 'ت', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ت', 'ع', 'ر', 'ف', 'ه', 'ا', '<##>', 'ا', 'ل', 'ح', 'ض', 'ا', 'ر', 'ة', '<##>', 'و', 'ش', 'ع', 'و', 'ب', '<##>', 'ا', 'ل', 'ع', 'ا', 'ل', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ي', 'م', '<##>', 'و', 'ا', 'ل', 'ح', 'د', 'ي', 'ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'ي', 'ع', 'و', 'د', '<##>', 'ا', 'ل', 'ف', 'ض', 'ل', '<##>', 'ف', 'ي', '<##>', 'ت', 'و', 'ح', 'ي', 'د', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ن', 'ز', 'و', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ح', 'ي', 'ث', '<##>', 'ل', 'م', '<##>', 'ت', 'ك', 'ن', '<##>', 'م', 'و', 'ح', 'د', 'ة', '<##>', 'ق', 'ب', 'ل', '<##>', 'ه', 'ذ', 'ا', '<##>', 'ا', 'ل', 'ع', 'ه', 'د', '<##>', 'ر', 'غ', 'م', '<##>', 'ا', 'ن', 'ه', 'ا', '<##>', 'ك', 'ا', 'ن', 'ت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غ', 'ن', 'ى', '<##>', 'و', 'م', 'ر', 'و', 'ن', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ا', 'ن', '<##>', 'ن', 'ز', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'و', 'ت', 'ح', 'د', 'ى', '<##>', 'ا', 'ل', 'ج', 'م', 'و', 'ع', '<##>', 'ب', 'ب', 'ي', 'ا', 'ن', 'ه', '<##>', 'و', 'ا', 'ع', 'ط', 'ى', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'س', 'ي', 'ل', 'ا', '<##>', 'م', 'ن', '<##>', 'ح', 'س', 'ن', '<##>', 'ا', 'ل', 'س', 'ب', 'ك', '<##>', 'و', 'ع', 'ذ', 'و', 'ب', 'ة', '<##>', 'ا', 'ل', 'س', 'ج', 'ع', '<##>', 'و', 'م', 'ن', '<##>', 'ا', 'ل', 'ب', 'ل', 'ا', 'غ', 'ة', '<##>', 'و', 'ا', 'ل', 'ب', 'ي', 'ا', 'ن', '<##>', 'م', 'ا', '<##>', 'ع', 'ج', 'ز', '<##>', 'ع', 'ن', 'ه', '<##>', 'ب', 'ل', 'غ', 'ا', 'ء', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', '<##>', 'و', 'ق', 'د', '<##>', 'و', 'ح', 'د', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ت', 'و', 'ح', 'ي', 'د', 'ا', '<##>', 'ك', 'ا', 'م', 'ل', 'ا', '<##>', 'و', 'ح', 'ف', 'ظ', 'ه', 'ا', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ت', 'ل', 'ا', 'ش', 'ي', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ق', 'ر', 'ا', 'ض', '<##>', 'ك', 'م', 'ا', '<##>', 'ح', 'د', 'ث', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ع', 'د', 'ي', 'د', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ل', 'غ', 'ا', 'ت', '<##>', 'ا', 'ل', 'س', 'ا', 'م', 'ي', 'ة', '<##>', 'ا', 'ل', 'ا', 'خ', 'ر', 'ى', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ا', 'ض', 'ح', 'ت', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ب', 'ا', 'ى', 'د', 'ة', '<##>', 'و', 'ا', 'ن', 'د', 'ث', 'ر', 'ت', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ز', 'م', 'ن', '<##>', 'ا', 'و', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ط', 'ا', 'ل', 'ه', 'ا', '<##>', 'ا', 'ل', 'ض', 'ع', 'ف', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ح', 'ط', 'ا', 'ط', '<##>', 'و', 'ب', 'ا', 'ل', 'ت', 'ا', 'ل', 'ي', '<##>', 'ع', 'د', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ر', 'ة', '<##>', 'ع', 'ل', 'ى', '<##>', 'م', 'س', 'ا', 'ي', 'ر', 'ة', '<##>', 'ا', 'ل', 'ت', 'غ', 'ي', 'ي', 'ر', 'ا', 'ت', '<##>', 'و', 'ا', 'ل', 'ت', 'ج', 'ا', 'ذ', 'ب', 'ا', 'ت', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ت', 'ع', 'ر', 'ف', 'ه', 'ا', '<##>', 'ا', 'ل', 'ح', 'ض', 'ا', 'ر', 'ة', '<##>', 'و', 'ش', 'ع', 'و', 'ب', '<##>', 'ا', 'ل', 'ع', 'ا', 'ل', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ي', 'م', '<##>', 'و', 'ا', 'ل', 'ح', 'د', 'ي', 'ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'ي', 'ع', 'و', 'د', '<##>', 'ا', 'ل', 'ف', 'ض', 'ل', '<##>', 'ف', 'ي', '<##>', 'ت', 'و', 'ح', 'ي', 'د', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ن', 'ز', 'و', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ح', 'ي', 'ث', '<##>', 'ل', 'م', '<##>', 'ت', 'ك', 'ن', '<##>', 'م', 'و', 'ح', 'د', 'ة', '<##>', 'ق', 'ب', 'ل', '<##>', 'ه', 'ذ', 'ا', '<##>', 'ا', 'ل', 'ع', 'ه', 'د', '<##>', 'ر', 'غ', 'م', '<##>', 'ا', 'ن', 'ه', 'ا', '<##>', 'ك', 'ا', 'ن', 'ت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غ', 'ن', 'ى', '<##>', 'و', 'م', 'ر', 'و', 'ن', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ا', 'ن', '<##>', 'ن', 'ز', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'و', 'ت', 'ح', 'د', 'ى', '<##>', 'ا', 'ل', 'ج', 'م', 'و', 'ع', '<##>', 'ب', 'ب', 'ي', 'ا', 'ن', 'ه', '<##>', 'و', 'ا', 'ع', 'ط', 'ى', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'س', 'ي', 'ل', 'ا', '<##>', 'م', 'ن', '<##>', 'ح', 'س', 'ن', '<##>', 'ا', 'ل', 'س', 'ب', 'ك', '<##>', 'و', 'ع', 'ذ', 'و', 'ب', 'ة', '<##>', 'ا', 'ل', 'س', 'ج', 'ع', '<##>', 'و', 'م', 'ن', '<##>', 'ا', 'ل', 'ب', 'ل', 'ا', 'غ', 'ة', '<##>', 'و', 'ا', 'ل', 'ب', 'ي', 'ا', 'ن', '<##>', 'م', 'ا', '<##>', 'ع', 'ج', 'ز', '<##>', 'ع', 'ن', 'ه', '<##>', 'ب', 'ل', 'غ', 'ا', 'ء', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', '<##>', 'و', 'ق', 'د', '<##>', 'و', 'ح', 'د', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ت', 'و', 'ح', 'ي', 'د', 'ا', '<##>', 'ك', 'ا', 'م', 'ل', 'ا', '<##>', 'و', 'ح', 'ف', 'ظ', 'ه', 'ا', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ت', 'ل', 'ا', 'ش', 'ي', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ق', 'ر', 'ا', 'ض', '<##>', 'ك', 'م', 'ا', '<##>', 'ح', 'د', 'ث', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ع', 'د', 'ي', 'د', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ل', 'غ', 'ا', 'ت', '<##>', 'ا', 'ل', 'س', 'ا', 'م', 'ي', 'ة', '<##>', 'ا', 'ل', 'ا', 'خ', 'ر', 'ى', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ا', 'ض', 'ح', 'ت', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ب', 'ا', 'ى', 'د', 'ة', '<##>', 'و', 'ا', 'ن', 'د', 'ث', 'ر', 'ت', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ز', 'م', 'ن', '<##>', 'ا', 'و', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ط', 'ا', 'ل', 'ه', 'ا', '<##>', 'ا', 'ل', 'ض', 'ع', 'ف', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ح', 'ط', 'ا', 'ط', '<##>', 'و', 'ب', 'ا', 'ل', 'ت', 'ا', 'ل', 'ي', '<##>', 'ع', 'د', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ر', 'ة', '<##>', 'ع', 'ل', 'ى', '<##>', 'م', 'س', 'ا', 'ي', 'ر', 'ة', '<##>', 'ا', 'ل', 'ت', 'غ', 'ي', 'ي', 'ر', 'ا', 'ت', '<##>', 'و', 'ا', 'ل', 'ت', 'ج', 'ا', 'ذ', 'ب', 'ا', 'ت', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ت', 'ع', 'ر', 'ف', 'ه', 'ا', '<##>', 'ا', 'ل', 'ح', 'ض', 'ا', 'ر', 'ة', '<##>', 'و', 'ش', 'ع', 'و', 'ب', '<##>', 'ا', 'ل', 'ع', 'ا', 'ل', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ي', 'م', '<##>', 'و', 'ا', 'ل', 'ح', 'د', 'ي', 'ث']\n"
     ]
    }
   ],
   "source": [
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.split_text(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotless-arabic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5256e330910480c51c2a86292ea06783a785bff23f3345acb952af191eac8f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
