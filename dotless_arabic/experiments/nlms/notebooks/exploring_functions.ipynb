{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Explorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to check and validate the functions in the NLMs training pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, to properly run this noteobook, set the root folder to be the root of this repository at the same level as dotless_arabic.\n",
    "To set this in jupyter vscode:\n",
    "\n",
    "go to setting > search for \"Notebook File Root\" > change its value from `${fileDirname}` to `${workspaceFolder}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkseem as tk\n",
    "from dotless_arabic.processing import process\n",
    "from dotless_arabic.experiments.nlms.src.utils import get_tokenizer\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset as collect_quran_dataset\n",
    "from dotless_arabic.tokenizers import FarasaMorphologicalTokenizer, DisjointLetterTokenizer, CharacterTokenizer,WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6236"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = collect_quran_dataset()\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to compare `tokenize` and `tokenize_from_splits` methods of the tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-02-05 21:28:06,829 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FarasaMorphologicalTokenizer...\n"
     ]
    }
   ],
   "source": [
    "farasa_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=FarasaMorphologicalTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ويعود الفضل في توحيد اللغة العربية الى نزول القران الكريم حيث لم تكن موحدة قبل هذا العهد رغم انها كانت ذات غنى ومرونة الى ان نزل القران الكريم وتحدى الجموع ببيانه واعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب وقد وحد القران الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض كما حدث مع العديد من اللغات السامية الاخرى التي اضحت لغات باىدة واندثرت مع الزمن او لغات طالها الضعف والانحطاط وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%82%D8%B1%D8%A2%D9%86\n",
    "sample_text = process(\"\"\"\n",
    "ويعود الفضل في توحيد اللغة العربية إلى نزول القرآن الكريم، حيث لم تكن موحدة قبل هذا العهد رغم أنها كانت ذات غنى ومرونة، إلى أن نزل القرآن الكريم وتحدى الجموع ببيانه، وأعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب. وقد وحد القرآن الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض، كما حدث مع العديد من اللغات السامية الأخرى، التي أضحت لغات بائدة واندثرت مع الزمن، أو لغات طالها الضعف والانحطاط، وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث\n",
    "\"\"\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', '\"غير_معروف\"', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', '\"غير_معروف\"', '\"غير_معروف\"', 'انها', '\"غير_معروف\"', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'وحد', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', 'حدث', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعود', '<##>', 'ال', 'فضل', '<##>', 'في', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'الى', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', '\"غير_معروف\"', 'ة', '<##>', 'قبل', '<##>', 'هذا', '<##>', 'ال', 'عهد', '<##>', '\"غير_معروف\"', '<##>', 'ان', 'ها', '<##>', 'كان', 'ت', '<##>', 'ذات', '<##>', '\"غير_معروف\"', '<##>', 'و', '\"غير_معروف\"', 'ة', '<##>', 'الى', '<##>', 'ان', '<##>', 'نزل', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'و', '\"غير_معروف\"', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'ب', 'بيان', 'ه', '<##>', 'و', 'أعطى', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', 'سيل', 'ا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', '\"غير_معروف\"', 'ة', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', 'من', '<##>', 'ال', 'بلاغ', 'ة', '<##>', 'و', 'ال', 'بيان', '<##>', 'ما', '<##>', '\"غير_معروف\"', '<##>', 'عن', 'ه', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'عرب', '<##>', 'و', 'قد', '<##>', 'وحد', '<##>', 'ال', 'قران', '<##>', 'ال', 'كريم', '<##>', 'ال', 'لغ', 'ة', '<##>', 'ال', 'عربي', 'ة', '<##>', '\"غير_معروف\"', 'ا', '<##>', 'كامل', 'ا', '<##>', 'و', 'حفظ', 'ها', '<##>', 'من', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'و', 'ال', '\"غير_معروف\"', '<##>', 'كما', '<##>', 'حدث', '<##>', 'مع', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'من', '<##>', 'ال', 'لغ', 'ات', '<##>', 'ال', '\"غير_معروف\"', 'ة', '<##>', 'ال', 'أخرى', '<##>', 'التي', '<##>', '\"غير_معروف\"', 'ت', '<##>', 'لغ', 'ات', '<##>', 'ب', '\"غير_معروف\"', 'ة', '<##>', 'و', '\"غير_معروف\"', 'ت', '<##>', 'مع', '<##>', 'ال', '\"غير_معروف\"', '<##>', 'أو', '<##>', 'لغ', 'ات', '<##>', 'طال', 'ها', '<##>', 'ال', 'ضعف', '<##>', 'و', 'ال', '\"غير_معروف\"', '<##>', 'و', 'ب', 'ال', 'تالي', '<##>', '\"غير_معروف\"', '<##>', 'ال', 'قدر', 'ة', '<##>', 'على', '<##>', '\"غير_معروف\"', 'ة', '<##>', 'ال', '\"غير_معروف\"', 'ات', '<##>', 'و', 'ال', '\"غير_معروف\"', 'ات', '<##>', 'التي', '<##>', 'تعرف', 'ها', '<##>', 'ال', '\"غير_معروف\"', 'ة', '<##>', 'و', 'شعوب', '<##>', 'ال', 'عالم', '<##>', 'ال', 'قديم', '<##>', 'و', 'ال', 'حديث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "disjoint_letters_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=DisjointLetterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##يعو', '##د', 'ا', '##لفضل', 'في', 'تو', '##حيد', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'ا', '##لى', 'نز', '##و', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'حيث', 'لم', 'تكن', 'مو', '##حد', '##ة', 'قبل', 'هذ', '##ا', 'ا', '##لعهد', 'ر', '##غ', '##م', 'ا', '##نها', 'كا', '##نت', 'ذ', '##ا', '##ت', '\"غير_معروف\"', 'و', '##مر', '##و', '##نة', 'ا', '##لى', 'ا', '##ن', 'نز', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'و', '##تحد', '##ى', 'ا', '##لج', '##مو', '##ع', '\"غير_معروف\"', 'و', '##ا', '##عطى', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'سيل', '##ا', 'من', 'حسن', 'ا', '##لس', '##بك', 'و', '##عذ', '##و', '##بة', 'ا', '##لس', '##جع', 'و', '##من', 'ا', '##لبلا', '##غة', 'و', '##ا', '##لبيا', '##ن', 'ما', '\"غير_معروف\"', 'عنه', 'بلغا', '##ء', 'ا', '##لعر', '##ب', 'و', '##قد', 'و', '##حد', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'تو', '##حيد', '##ا', 'كا', '##ملا', 'و', '##ح', '##ف', '##ظ', '##ها', 'من', 'ا', '##لتلا', '##ش', '##ي', 'و', '##ا', '##لا', '##نقر', '##ا', '##ض', 'كما', 'حد', '##ث', 'مع', 'ا', '##لعد', '##يد', 'من', 'ا', '##ل', '##لغا', '##ت', 'ا', '##لسا', '##مي', '##ة', 'ا', '##لا', '##خر', '##ى', 'ا', '##لتي', 'ا', '##ض', '##ح', '##ت', 'لغا', '##ت', 'با', '##ىد', '##ة', 'و', '##ا', '##ند', '##ثر', '##ت', 'مع', 'ا', '##لز', '##من', 'ا', '##و', 'لغا', '##ت', 'طا', '##لها', 'ا', '##لضعف', 'و', '##ا', '##لا', '##ن', '##ح', '##طا', '##ط', 'و', '##با', '##لتا', '##لي', 'عد', '##م', 'ا', '##لقد', '##ر', '##ة', 'على', 'مسا', '##ير', '##ة', 'ا', '##لت', '##غ', '##ي', '##ير', '##ا', '##ت', 'و', '##ا', '##لتجا', '##ذ', '##با', '##ت', 'ا', '##لتي', 'تعر', '##فها', 'ا', '##لح', '##ضا', '##ر', '##ة', 'و', '##شعو', '##ب', 'ا', '##لعا', '##لم', 'ا', '##لقد', '##يم', 'و', '##ا', '##لحد', '##يث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعو', 'د', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'في', '<##>', 'تو', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'نز', 'و', '\"غير_معروف\"', '<##>', 'ا', 'لقر', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'مو', 'حد', '\"غير_معروف\"', '<##>', 'قبل', '<##>', 'هذ', 'ا', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ر', 'غم', '<##>', 'ا', 'نها', '<##>', 'كا', '\"غير_معروف\"', '<##>', 'ذ', 'ا', '\"غير_معروف\"', '<##>', '\"غير_معروف\"', '<##>', 'و', 'مر', 'و', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'نز', '\"غير_معروف\"', '<##>', 'ا', 'لقر', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'تحد', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', '\"غير_معروف\"', '<##>', 'من', '<##>', 'حسن', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'عذ', 'و', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'من', '<##>', 'ا', 'لبلا', '\"غير_معروف\"', '<##>', 'و', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'ما', '<##>', '\"غير_معروف\"', '<##>', 'عنه', '<##>', 'بلغا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'قد', '<##>', 'و', 'حد', '<##>', 'ا', 'لقر', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'تو', '\"غير_معروف\"', 'ا', '<##>', 'كا', 'ملا', '<##>', 'و', '\"غير_معروف\"', '<##>', 'من', '<##>', 'ا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'ا', 'لا', 'نقر', 'ا', '\"غير_معروف\"', '<##>', 'كما', '<##>', 'حد', '\"غير_معروف\"', '<##>', 'مع', '<##>', 'ا', 'لعد', 'يد', '<##>', 'من', '<##>', 'ا', 'للغا', '\"غير_معروف\"', '<##>', 'ا', 'لسا', '\"غير_معروف\"', '<##>', 'ا', 'لا', 'خر', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'لغا', '\"غير_معروف\"', '<##>', 'با', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'ا', 'ند', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'مع', '<##>', 'ا', 'لز', 'من', '<##>', 'ا', 'و', '<##>', 'لغا', '\"غير_معروف\"', '<##>', 'طا', 'لها', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'و', 'ا', 'لا', '\"غير_معروف\"', '\"غير_معروف\"', '<##>', 'و', 'با', 'لتا', 'لي', '<##>', 'عد', '\"غير_معروف\"', '<##>', 'ا', 'لقد', 'ر', '\"غير_معروف\"', '<##>', 'على', '<##>', 'مسا', 'ير', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', 'ا', '\"غير_معروف\"', '<##>', 'و', 'ا', '\"غير_معروف\"', 'ذ', 'با', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', '<##>', 'تعر', '\"غير_معروف\"', '<##>', 'ا', '\"غير_معروف\"', 'ر', '\"غير_معروف\"', '<##>', 'و', 'شعو', '\"غير_معروف\"', '<##>', 'ا', 'لعا', 'لم', '<##>', 'ا', 'لقد', '\"غير_معروف\"', '<##>', 'و', 'ا', 'لحد', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'يعو', 'د', '<##>', 'ا', 'لفضل', '<##>', 'في', '<##>', 'تو', 'حيد', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'ا', 'لى', '<##>', 'نز', 'و', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'حيث', '<##>', 'لم', '<##>', 'تكن', '<##>', 'مو', 'حد', 'ة', '<##>', 'قبل', '<##>', 'هذ', 'ا', '<##>', 'ا', 'لعهد', '<##>', 'ر', 'غم', '<##>', 'ا', 'نها', '<##>', 'كا', 'نت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غنى', '<##>', 'و', 'مر', 'و', 'نة', '<##>', 'ا', 'لى', '<##>', 'ا', 'ن', '<##>', 'نز', 'ل', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'و', 'تحد', 'ى', '<##>', 'ا', 'لجمو', 'ع', '<##>', 'ببيا', 'نه', '<##>', 'و', 'ا', 'عطى', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'سيلا', '<##>', 'من', '<##>', 'حسن', '<##>', 'ا', 'لسبك', '<##>', 'و', 'عذ', 'و', 'بة', '<##>', 'ا', 'لسجع', '<##>', 'و', 'من', '<##>', 'ا', 'لبلا', 'غة', '<##>', 'و', 'ا', 'لبيا', 'ن', '<##>', 'ما', '<##>', 'عجز', '<##>', 'عنه', '<##>', 'بلغا', 'ء', '<##>', 'ا', 'لعر', 'ب', '<##>', 'و', 'قد', '<##>', 'و', 'حد', '<##>', 'ا', 'لقر', 'ا', 'ن', '<##>', 'ا', 'لكر', 'يم', '<##>', 'ا', 'للغة', '<##>', 'ا', 'لعر', 'بية', '<##>', 'تو', 'حيد', 'ا', '<##>', 'كا', 'ملا', '<##>', 'و', 'حفظها', '<##>', 'من', '<##>', 'ا', 'لتلا', 'شي', '<##>', 'و', 'ا', 'لا', 'نقر', 'ا', 'ض', '<##>', 'كما', '<##>', 'حد', 'ث', '<##>', 'مع', '<##>', 'ا', 'لعد', 'يد', '<##>', 'من', '<##>', 'ا', 'للغا', 'ت', '<##>', 'ا', 'لسا', 'مية', '<##>', 'ا', 'لا', 'خر', 'ى', '<##>', 'ا', 'لتي', '<##>', 'ا', 'ضحت', '<##>', 'لغا', 'ت', '<##>', 'با', 'ىد', 'ة', '<##>', 'و', 'ا', 'ند', 'ثر', 'ت', '<##>', 'مع', '<##>', 'ا', 'لز', 'من', '<##>', 'ا', 'و', '<##>', 'لغا', 'ت', '<##>', 'طا', 'لها', '<##>', 'ا', 'لضعف', '<##>', 'و', 'ا', 'لا', 'نحطا', 'ط', '<##>', 'و', 'با', 'لتا', 'لي', '<##>', 'عد', 'م', '<##>', 'ا', 'لقد', 'ر', 'ة', '<##>', 'على', '<##>', 'مسا', 'ير', 'ة', '<##>', 'ا', 'لتغيير', 'ا', 'ت', '<##>', 'و', 'ا', 'لتجا', 'ذ', 'با', 'ت', '<##>', 'ا', 'لتي', '<##>', 'تعر', 'فها', '<##>', 'ا', 'لحضا', 'ر', 'ة', '<##>', 'و', 'شعو', 'ب', '<##>', 'ا', 'لعا', 'لم', '<##>', 'ا', 'لقد', 'يم', '<##>', 'و', 'ا', 'لحد', 'يث']\n"
     ]
    }
   ],
   "source": [
    "# A fixed bug discovered in the disjoint letters tokenizer. It was putting ## as a token, hence it adds a very high frequency to it.\n",
    "import re\n",
    "# rx = re.compile(r\"([اأإآءؤﻵﻹﻷدذرزو])\")\n",
    "# text = rx.sub(r\"\\1## \", sample_text)\n",
    "# text = text.replace(\"## \", \" ##\")\n",
    "# text\n",
    "print(disjoint_letters_tokenizer.split_text(sample_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "words_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "chars_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=CharacterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'ي', 'ع', 'و', 'د', '<##>', 'ا', 'ل', 'ف', 'ض', 'ل', '<##>', 'ف', 'ي', '<##>', 'ت', 'و', 'ح', 'ي', 'د', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ن', 'ز', 'و', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ح', 'ي', 'ث', '<##>', 'ل', 'م', '<##>', 'ت', 'ك', 'ن', '<##>', 'م', 'و', 'ح', 'د', 'ة', '<##>', 'ق', 'ب', 'ل', '<##>', 'ه', 'ذ', 'ا', '<##>', 'ا', 'ل', 'ع', 'ه', 'د', '<##>', 'ر', 'غ', 'م', '<##>', 'ا', 'ن', 'ه', 'ا', '<##>', 'ك', 'ا', 'ن', 'ت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غ', 'ن', 'ى', '<##>', 'و', 'م', 'ر', 'و', 'ن', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ا', 'ن', '<##>', 'ن', 'ز', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'و', 'ت', 'ح', 'د', 'ى', '<##>', 'ا', 'ل', 'ج', 'م', 'و', 'ع', '<##>', 'ب', 'ب', 'ي', 'ا', 'ن', 'ه', '<##>', 'و', 'ا', 'ع', 'ط', 'ى', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'س', 'ي', 'ل', 'ا', '<##>', 'م', 'ن', '<##>', 'ح', 'س', 'ن', '<##>', 'ا', 'ل', 'س', 'ب', 'ك', '<##>', 'و', 'ع', 'ذ', 'و', 'ب', 'ة', '<##>', 'ا', 'ل', 'س', 'ج', 'ع', '<##>', 'و', 'م', 'ن', '<##>', 'ا', 'ل', 'ب', 'ل', 'ا', 'غ', 'ة', '<##>', 'و', 'ا', 'ل', 'ب', 'ي', 'ا', 'ن', '<##>', 'م', 'ا', '<##>', 'ع', 'ج', 'ز', '<##>', 'ع', 'ن', 'ه', '<##>', 'ب', 'ل', 'غ', 'ا', 'ء', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', '<##>', 'و', 'ق', 'د', '<##>', 'و', 'ح', 'د', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ت', 'و', 'ح', 'ي', 'د', 'ا', '<##>', 'ك', 'ا', 'م', 'ل', 'ا', '<##>', 'و', 'ح', 'ف', 'ظ', 'ه', 'ا', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ت', 'ل', 'ا', 'ش', 'ي', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ق', 'ر', 'ا', 'ض', '<##>', 'ك', 'م', 'ا', '<##>', 'ح', 'د', 'ث', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ع', 'د', 'ي', 'د', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ل', 'غ', 'ا', 'ت', '<##>', 'ا', 'ل', 'س', 'ا', 'م', 'ي', 'ة', '<##>', 'ا', 'ل', 'ا', 'خ', 'ر', 'ى', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ا', 'ض', 'ح', 'ت', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ب', 'ا', 'ى', 'د', 'ة', '<##>', 'و', 'ا', 'ن', 'د', 'ث', 'ر', 'ت', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ز', 'م', 'ن', '<##>', 'ا', 'و', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ط', 'ا', 'ل', 'ه', 'ا', '<##>', 'ا', 'ل', 'ض', 'ع', 'ف', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ح', 'ط', 'ا', 'ط', '<##>', 'و', 'ب', 'ا', 'ل', 'ت', 'ا', 'ل', 'ي', '<##>', 'ع', 'د', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ر', 'ة', '<##>', 'ع', 'ل', 'ى', '<##>', 'م', 'س', 'ا', 'ي', 'ر', 'ة', '<##>', 'ا', 'ل', 'ت', 'غ', 'ي', 'ي', 'ر', 'ا', 'ت', '<##>', 'و', 'ا', 'ل', 'ت', 'ج', 'ا', 'ذ', 'ب', 'ا', 'ت', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ت', 'ع', 'ر', 'ف', 'ه', 'ا', '<##>', 'ا', 'ل', 'ح', 'ض', 'ا', 'ر', 'ة', '<##>', 'و', 'ش', 'ع', 'و', 'ب', '<##>', 'ا', 'ل', 'ع', 'ا', 'ل', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ي', 'م', '<##>', 'و', 'ا', 'ل', 'ح', 'د', 'ي', 'ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'ي', 'ع', 'و', 'د', '<##>', 'ا', 'ل', 'ف', 'ض', 'ل', '<##>', 'ف', 'ي', '<##>', 'ت', 'و', 'ح', 'ي', 'د', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ن', 'ز', 'و', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ح', 'ي', 'ث', '<##>', 'ل', 'م', '<##>', 'ت', 'ك', 'ن', '<##>', 'م', 'و', 'ح', 'د', 'ة', '<##>', 'ق', 'ب', 'ل', '<##>', 'ه', 'ذ', 'ا', '<##>', 'ا', 'ل', 'ع', 'ه', 'د', '<##>', 'ر', 'غ', 'م', '<##>', 'ا', 'ن', 'ه', 'ا', '<##>', 'ك', 'ا', 'ن', 'ت', '<##>', 'ذ', 'ا', 'ت', '<##>', 'غ', 'ن', 'ى', '<##>', 'و', 'م', 'ر', 'و', 'ن', 'ة', '<##>', 'ا', 'ل', 'ى', '<##>', 'ا', 'ن', '<##>', 'ن', 'ز', 'ل', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'و', 'ت', 'ح', 'د', 'ى', '<##>', 'ا', 'ل', 'ج', 'م', 'و', 'ع', '<##>', 'ب', 'ب', 'ي', 'ا', 'ن', 'ه', '<##>', 'و', 'ا', 'ع', 'ط', 'ى', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'س', 'ي', 'ل', 'ا', '<##>', 'م', 'ن', '<##>', 'ح', 'س', 'ن', '<##>', 'ا', 'ل', 'س', 'ب', 'ك', '<##>', 'و', 'ع', 'ذ', 'و', 'ب', 'ة', '<##>', 'ا', 'ل', 'س', 'ج', 'ع', '<##>', 'و', 'م', 'ن', '<##>', 'ا', 'ل', 'ب', 'ل', 'ا', 'غ', 'ة', '<##>', 'و', 'ا', 'ل', 'ب', 'ي', 'ا', 'ن', '<##>', 'م', 'ا', '<##>', 'ع', 'ج', 'ز', '<##>', 'ع', 'ن', 'ه', '<##>', 'ب', 'ل', 'غ', 'ا', 'ء', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', '<##>', 'و', 'ق', 'د', '<##>', 'و', 'ح', 'د', '<##>', 'ا', 'ل', 'ق', 'ر', 'ا', 'ن', '<##>', 'ا', 'ل', 'ك', 'ر', 'ي', 'م', '<##>', 'ا', 'ل', 'ل', 'غ', 'ة', '<##>', 'ا', 'ل', 'ع', 'ر', 'ب', 'ي', 'ة', '<##>', 'ت', 'و', 'ح', 'ي', 'د', 'ا', '<##>', 'ك', 'ا', 'م', 'ل', 'ا', '<##>', 'و', 'ح', 'ف', 'ظ', 'ه', 'ا', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ت', 'ل', 'ا', 'ش', 'ي', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ق', 'ر', 'ا', 'ض', '<##>', 'ك', 'م', 'ا', '<##>', 'ح', 'د', 'ث', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ع', 'د', 'ي', 'د', '<##>', 'م', 'ن', '<##>', 'ا', 'ل', 'ل', 'غ', 'ا', 'ت', '<##>', 'ا', 'ل', 'س', 'ا', 'م', 'ي', 'ة', '<##>', 'ا', 'ل', 'ا', 'خ', 'ر', 'ى', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ا', 'ض', 'ح', 'ت', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ب', 'ا', 'ى', 'د', 'ة', '<##>', 'و', 'ا', 'ن', 'د', 'ث', 'ر', 'ت', '<##>', 'م', 'ع', '<##>', 'ا', 'ل', 'ز', 'م', 'ن', '<##>', 'ا', 'و', '<##>', 'ل', 'غ', 'ا', 'ت', '<##>', 'ط', 'ا', 'ل', 'ه', 'ا', '<##>', 'ا', 'ل', 'ض', 'ع', 'ف', '<##>', 'و', 'ا', 'ل', 'ا', 'ن', 'ح', 'ط', 'ا', 'ط', '<##>', 'و', 'ب', 'ا', 'ل', 'ت', 'ا', 'ل', 'ي', '<##>', 'ع', 'د', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ر', 'ة', '<##>', 'ع', 'ل', 'ى', '<##>', 'م', 'س', 'ا', 'ي', 'ر', 'ة', '<##>', 'ا', 'ل', 'ت', 'غ', 'ي', 'ي', 'ر', 'ا', 'ت', '<##>', 'و', 'ا', 'ل', 'ت', 'ج', 'ا', 'ذ', 'ب', 'ا', 'ت', '<##>', 'ا', 'ل', 'ت', 'ي', '<##>', 'ت', 'ع', 'ر', 'ف', 'ه', 'ا', '<##>', 'ا', 'ل', 'ح', 'ض', 'ا', 'ر', 'ة', '<##>', 'و', 'ش', 'ع', 'و', 'ب', '<##>', 'ا', 'ل', 'ع', 'ا', 'ل', 'م', '<##>', 'ا', 'ل', 'ق', 'د', 'ي', 'م', '<##>', 'و', 'ا', 'ل', 'ح', 'د', 'ي', 'ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotless-arabic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5256e330910480c51c2a86292ea06783a785bff23f3345acb952af191eac8f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
