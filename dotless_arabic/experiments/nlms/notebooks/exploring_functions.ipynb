{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Explorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to check and validate the functions in the NLMs training pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, to properly run this noteobook, set the root folder to be the root of this repository at the same level as dotless_arabic.\n",
    "To set this in jupyter vscode:\n",
    "\n",
    "go to setting > search for \"Notebook File Root\" > change its value from `${fileDirname}` to `${workspaceFolder}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkseem as tk\n",
    "from dotless_arabic.processing import process\n",
    "from dotless_arabic.experiments.nlms.src.utils import get_tokenizer\n",
    "from dotless_arabic.experiments.nlms.quran_dataset.collect import collect_dataset as collect_quran_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Number of samples:\n",
      "6236\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6236"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = collect_quran_dataset()\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to compare `tokenize` and `tokenize_from_splits` methods of the tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Farasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-27 22:14:41,717 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FarasaMorphologicalTokenizer...\n"
     ]
    }
   ],
   "source": [
    "farasa_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=tk.FarasaMorphologicalTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ويعود الفضل في توحيد اللغة العربية الى نزول القران الكريم حيث لم تكن موحدة قبل هذا العهد رغم انها كانت ذات غنى ومرونة الى ان نزل القران الكريم وتحدى الجموع ببيانه واعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب وقد وحد القران الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض كما حدث مع العديد من اللغات السامية الاخرى التي اضحت لغات باىدة واندثرت مع الزمن او لغات طالها الضعف والانحطاط وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%82%D8%B1%D8%A2%D9%86\n",
    "sample_text = process(\"\"\"\n",
    "ويعود الفضل في توحيد اللغة العربية إلى نزول القرآن الكريم، حيث لم تكن موحدة قبل هذا العهد رغم أنها كانت ذات غنى ومرونة، إلى أن نزل القرآن الكريم وتحدى الجموع ببيانه، وأعطى اللغة العربية سيلا من حسن السبك وعذوبة السجع ومن البلاغة والبيان ما عجز عنه بلغاء العرب. وقد وحد القرآن الكريم اللغة العربية توحيدا كاملا وحفظها من التلاشي والانقراض، كما حدث مع العديد من اللغات السامية الأخرى، التي أضحت لغات بائدة واندثرت مع الزمن، أو لغات طالها الضعف والانحطاط، وبالتالي عدم القدرة على مسايرة التغييرات والتجاذبات التي تعرفها الحضارة وشعوب العالم القديم والحديث\n",
    "\"\"\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الف', '##ضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', 'ال', '##عربي', '##ة', 'الى', '\"غير_معروف\"', 'الق', '##ر', '##ان', 'ال', '##كريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'ال', '##عهد', '\"غير_معروف\"', 'انها', 'كان', '##ت', 'ذات', '\"غير_معروف\"', 'و', '##مرو', '##ن', '##ة', 'الى', 'ان', 'نزل', 'الق', '##ر', '##ان', 'ال', '##كريم', '\"غير_معروف\"', '\"غير_معروف\"', 'ب', '##بيان', '##ه', '\"غير_معروف\"', '\"غير_معروف\"', 'ال', '##عربي', '##ة', 'سيل', '##ا', 'من', 'حسن', 'ال', '##س', '##بك', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', 'ال', '##بلاغ', '##ة', 'وال', '##بيان', 'ما', '\"غير_معروف\"', 'عن', '##ه', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'وحد', 'الق', '##ر', '##ان', 'ال', '##كريم', '\"غير_معروف\"', 'ال', '##عربي', '##ة', '\"غير_معروف\"', 'كامل', '##ا', 'و', '##حفظ', '##ها', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', 'ال', '##س', '##امي', '##ة', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', 'لغ', '##ات', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', 'الزم', '##ن', 'او', 'لغ', '##ات', 'طال', '##ها', 'ال', '##ضعف', '\"غير_معروف\"', 'و', '##ب', '##ال', '##تالي', '\"غير_معروف\"', '\"غير_معروف\"', 'على', 'مس', '##اي', '##ر', '##ة', 'ال', '##ت', '##غي', '##ي', '##ر', '##ات', '\"غير_معروف\"', 'التي', 'تعرف', '##ها', '\"غير_معروف\"', '\"غير_معروف\"', 'ال', '##عالم', '\"غير_معروف\"', 'وال', '##حديث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '\"غير_معروف\"', 'ال', '##فضل', 'في', '\"غير_معروف\"', 'ال', '\"غير_معروف\"', '##ة', 'ال', '##عربي', '##ة', 'الى', '\"غير_معروف\"', 'ال', '##قران', 'ال', '##كريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', '##ة', 'قبل', 'هذا', 'ال', '##عهد', '\"غير_معروف\"', 'ان', '##ها', 'كان', '##ت', 'ذات', '\"غير_معروف\"', 'و', '\"غير_معروف\"', '##ة', 'الى', 'ان', 'نزل', 'ال', '##قران', 'ال', '##كريم', 'و', '\"غير_معروف\"', 'ال', '\"غير_معروف\"', 'ب', '##بيان', '##ه', 'و', '##أعطى', 'ال', '\"غير_معروف\"', '##ة', 'ال', '##عربي', '##ة', 'سيل', '##ا', 'من', 'حسن', 'ال', '\"غير_معروف\"', 'و', '\"غير_معروف\"', '##ة', 'ال', '\"غير_معروف\"', 'و', '##من', 'ال', '##بلاغ', '##ة', 'و', '##ال', '##بيان', 'ما', '\"غير_معروف\"', 'عن', '##ه', '\"غير_معروف\"', 'ال', '\"غير_معروف\"', 'و', '##قد', 'وحد', 'ال', '##قران', 'ال', '##كريم', 'ال', '\"غير_معروف\"', '##ة', 'ال', '##عربي', '##ة', '\"غير_معروف\"', '##ا', 'كامل', '##ا', 'و', '##حفظ', '##ها', 'من', 'ال', '\"غير_معروف\"', 'و', '##ال', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', 'ال', '\"غير_معروف\"', 'من', 'ال', '\"غير_معروف\"', '##ات', 'ال', '\"غير_معروف\"', '##ة', 'ال', '##أخرى', 'التي', '\"غير_معروف\"', '##ت', 'لغ', '##ات', 'ب', '\"غير_معروف\"', '##ة', 'و', '\"غير_معروف\"', '##ت', 'مع', 'ال', '\"غير_معروف\"', 'أو', 'لغ', '##ات', 'طال', '##ها', 'ال', '##ضعف', 'و', '##ال', '\"غير_معروف\"', 'و', '##ب', '##ال', '##تالي', '\"غير_معروف\"', 'ال', '##قدر', '##ة', 'على', '\"غير_معروف\"', '##ة', 'ال', '\"غير_معروف\"', '##ات', 'و', '##ال', '\"غير_معروف\"', '##ات', 'التي', 'تعرف', '##ها', 'ال', '\"غير_معروف\"', '##ة', 'و', '\"غير_معروف\"', 'ال', '##عالم', 'ال', '##قديم', 'و', '##ال', '##حديث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in farasa_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "disjoint_letters_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=tk.DisjointLetterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##يعو', '##د', 'ا', '##لفضل', 'في', 'تو', '##حيد', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'ا', '##لى', 'نز', '##و', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'حيث', 'لم', 'تكن', 'مو', '##حد', '##ة', 'قبل', 'هذ', '##ا', 'ا', '##لعهد', 'ر', '##غ', '##م', 'ا', '##نها', 'كا', '##نت', 'ذ', '##ا', '##ت', '\"غير_معروف\"', 'و', '##مر', '##و', '##نة', 'ا', '##لى', 'ا', '##ن', 'نز', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'و', '##تحد', '##ى', 'ا', '##لج', '##مو', '##ع', '\"غير_معروف\"', 'و', '##ا', '##عطى', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'سيل', '##ا', 'من', 'حسن', 'ا', '##لس', '##بك', 'و', '##عذ', '##و', '##بة', 'ا', '##لس', '##جع', 'و', '##من', 'ا', '##لبلا', '##غة', 'و', '##ا', '##لبيا', '##ن', 'ما', '\"غير_معروف\"', 'عنه', 'بلغا', '##ء', 'ا', '##لعر', '##ب', 'و', '##قد', 'و', '##حد', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'ا', '##ل', '##لغة', 'ا', '##لعر', '##بية', 'تو', '##حيد', '##ا', 'كا', '##ملا', 'و', '##ح', '##ف', '##ظ', '##ها', 'من', 'ا', '##لتلا', '##ش', '##ي', 'و', '##ا', '##لا', '##نقر', '##ا', '##ض', 'كما', 'حد', '##ث', 'مع', 'ا', '##لعد', '##يد', 'من', 'ا', '##ل', '##لغا', '##ت', 'ا', '##لسا', '##مي', '##ة', 'ا', '##لا', '##خر', '##ى', 'ا', '##لتي', 'ا', '##ض', '##ح', '##ت', 'لغا', '##ت', 'با', '##ىد', '##ة', 'و', '##ا', '##ند', '##ثر', '##ت', 'مع', 'ا', '##لز', '##من', 'ا', '##و', 'لغا', '##ت', 'طا', '##لها', 'ا', '##لضعف', 'و', '##ا', '##لا', '##ن', '##ح', '##طا', '##ط', 'و', '##با', '##لتا', '##لي', 'عد', '##م', 'ا', '##لقد', '##ر', '##ة', 'على', 'مسا', '##ير', '##ة', 'ا', '##لت', '##غ', '##ي', '##ير', '##ا', '##ت', 'و', '##ا', '##لتجا', '##ذ', '##با', '##ت', 'ا', '##لتي', 'تعر', '##فها', 'ا', '##لح', '##ضا', '##ر', '##ة', 'و', '##شعو', '##ب', 'ا', '##لعا', '##لم', 'ا', '##لقد', '##يم', 'و', '##ا', '##لحد', '##يث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##يعو', '##د', 'ا', '##لفضل', 'في', 'تو', '##حيد', 'ا', '\"غير_معروف\"', 'ا', '##لعر', '##بية', 'ا', '##لى', 'نز', '##و', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'حيث', 'لم', 'تكن', 'مو', '##حد', '##ة', 'قبل', 'هذ', '##ا', 'ا', '##لعهد', 'ر', '\"غير_معروف\"', 'ا', '##نها', 'كا', '##نت', 'ذ', '##ا', '##ت', '\"غير_معروف\"', 'و', '##مر', '##و', '##نة', 'ا', '##لى', 'ا', '##ن', 'نز', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'و', '##تحد', '##ى', 'ا', '\"غير_معروف\"', '##ع', '\"غير_معروف\"', '##نه', 'و', '##ا', '##عطى', 'ا', '\"غير_معروف\"', 'ا', '##لعر', '##بية', '\"غير_معروف\"', 'من', 'حسن', 'ا', '\"غير_معروف\"', 'و', '##عذ', '##و', '##بة', 'ا', '\"غير_معروف\"', 'و', '##من', 'ا', '##لبلا', '##غة', 'و', '##ا', '##لبيا', '##ن', 'ما', '\"غير_معروف\"', 'عنه', 'بلغا', '##ء', 'ا', '##لعر', '##ب', 'و', '##قد', 'و', '##حد', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'ا', '\"غير_معروف\"', 'ا', '##لعر', '##بية', 'تو', '##حيد', '##ا', 'كا', '##ملا', 'و', '\"غير_معروف\"', 'من', 'ا', '##لتلا', '\"غير_معروف\"', 'و', '##ا', '##لا', '##نقر', '##ا', '##ض', 'كما', 'حد', '##ث', 'مع', 'ا', '##لعد', '##يد', 'من', 'ا', '\"غير_معروف\"', '##ت', 'ا', '##لسا', '\"غير_معروف\"', 'ا', '##لا', '##خر', '##ى', 'ا', '##لتي', 'ا', '\"غير_معروف\"', 'لغا', '##ت', 'با', '##ىد', '##ة', 'و', '##ا', '##ند', '##ثر', '##ت', 'مع', 'ا', '##لز', '##من', 'ا', '##و', 'لغا', '##ت', 'طا', '##لها', 'ا', '##لضعف', 'و', '##ا', '##لا', '\"غير_معروف\"', '##ط', 'و', '##با', '##لتا', '##لي', 'عد', '##م', 'ا', '##لقد', '##ر', '##ة', 'على', 'مسا', '##ير', '##ة', 'ا', '\"غير_معروف\"', '##ا', '##ت', 'و', '##ا', '##لتجا', '##ذ', '##با', '##ت', 'ا', '##لتي', 'تعر', '##فها', 'ا', '\"غير_معروف\"', '##ر', '##ة', 'و', '##شعو', '##ب', 'ا', '##لعا', '##لم', 'ا', '##لقد', '##يم', 'و', '##ا', '##لحد', '##يث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in disjoint_letters_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##يعو', '##د', 'ا', '##لفضل', 'في', 'تو', '##حيد', 'ا', '##للغة', 'ا', '##لعر', '##بية', 'ا', '##لى', 'نز', '##و', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'حيث', 'لم', 'تكن', 'مو', '##حد', '##ة', 'قبل', 'هذ', '##ا', 'ا', '##لعهد', 'ر', '##غم', 'ا', '##نها', 'كا', '##نت', 'ذ', '##ا', '##ت', 'غنى', 'و', '##مر', '##و', '##نة', 'ا', '##لى', 'ا', '##ن', 'نز', '##ل', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'و', '##تحد', '##ى', 'ا', '##لجمو', '##ع', 'ببيا', '##نه', 'و', '##ا', '##عطى', 'ا', '##للغة', 'ا', '##لعر', '##بية', 'سيلا', 'من', 'حسن', 'ا', '##لسبك', 'و', '##عذ', '##و', '##بة', 'ا', '##لسجع', 'و', '##من', 'ا', '##لبلا', '##غة', 'و', '##ا', '##لبيا', '##ن', 'ما', 'عجز', 'عنه', 'بلغا', '##ء', 'ا', '##لعر', '##ب', 'و', '##قد', 'و', '##حد', 'ا', '##لقر', '##ا', '##ن', 'ا', '##لكر', '##يم', 'ا', '##للغة', 'ا', '##لعر', '##بية', 'تو', '##حيد', '##ا', 'كا', '##ملا', 'و', '##حفظها', 'من', 'ا', '##لتلا', '##شي', 'و', '##ا', '##لا', '##نقر', '##ا', '##ض', 'كما', 'حد', '##ث', 'مع', 'ا', '##لعد', '##يد', 'من', 'ا', '##للغا', '##ت', 'ا', '##لسا', '##مية', 'ا', '##لا', '##خر', '##ى', 'ا', '##لتي', 'ا', '##ضحت', 'لغا', '##ت', 'با', '##ىد', '##ة', 'و', '##ا', '##ند', '##ثر', '##ت', 'مع', 'ا', '##لز', '##من', 'ا', '##و', 'لغا', '##ت', 'طا', '##لها', 'ا', '##لضعف', 'و', '##ا', '##لا', '##نحطا', '##ط', 'و', '##با', '##لتا', '##لي', 'عد', '##م', 'ا', '##لقد', '##ر', '##ة', 'على', 'مسا', '##ير', '##ة', 'ا', '##لتغيير', '##ا', '##ت', 'و', '##ا', '##لتجا', '##ذ', '##با', '##ت', 'ا', '##لتي', 'تعر', '##فها', 'ا', '##لحضا', '##ر', '##ة', 'و', '##شعو', '##ب', 'ا', '##لعا', '##لم', 'ا', '##لقد', '##يم', 'و', '##ا', '##لحد', '##يث']\n"
     ]
    }
   ],
   "source": [
    "# A fixed bug discovered in the disjoint letters tokenizer. It was putting ## as a token, hence it adds a very high frequency to it.\n",
    "import re\n",
    "# rx = re.compile(r\"([اأإآءؤﻵﻹﻷدذرزو])\")\n",
    "# text = rx.sub(r\"\\1## \", sample_text)\n",
    "# text = text.replace(\"## \", \" ##\")\n",
    "# text\n",
    "print(disjoint_letters_tokenizer.split_text(sample_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "words_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=tk.WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"غير_معروف\"', 'الفضل', 'في', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', '\"غير_معروف\"', 'القران', 'الكريم', 'حيث', 'لم', 'تكن', '\"غير_معروف\"', 'قبل', 'هذا', 'العهد', '\"غير_معروف\"', 'انها', 'كانت', 'ذات', '\"غير_معروف\"', '\"غير_معروف\"', 'الى', 'ان', 'نزل', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', 'حسن', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'ومن', '\"غير_معروف\"', '\"غير_معروف\"', 'ما', '\"غير_معروف\"', 'عنه', '\"غير_معروف\"', '\"غير_معروف\"', 'وقد', '\"غير_معروف\"', 'القران', 'الكريم', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'كما', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'من', '\"غير_معروف\"', '\"غير_معروف\"', 'الاخرى', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'مع', '\"غير_معروف\"', 'او', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'على', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'التي', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', '\"غير_معروف\"', 'القديم', '\"غير_معروف\"']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in words_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "chars_tokenizer = get_tokenizer(\n",
    "    vocab_size=10_000,\n",
    "    train_dataset=list(map(process,dataset)),\n",
    "    tokenizer_class=tk.CharacterTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##ي', '##ع', '##و', '##د', 'ا', '##ل', '##ف', '##ض', '##ل', 'ف', '##ي', 'ت', '##و', '##ح', '##ي', '##د', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'ا', '##ل', '##ى', 'ن', '##ز', '##و', '##ل', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'ح', '##ي', '##ث', 'ل', '##م', 'ت', '##ك', '##ن', 'م', '##و', '##ح', '##د', '##ة', 'ق', '##ب', '##ل', 'ه', '##ذ', '##ا', 'ا', '##ل', '##ع', '##ه', '##د', 'ر', '##غ', '##م', 'ا', '##ن', '##ه', '##ا', 'ك', '##ا', '##ن', '##ت', 'ذ', '##ا', '##ت', 'غ', '##ن', '##ى', 'و', '##م', '##ر', '##و', '##ن', '##ة', 'ا', '##ل', '##ى', 'ا', '##ن', 'ن', '##ز', '##ل', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'و', '##ت', '##ح', '##د', '##ى', 'ا', '##ل', '##ج', '##م', '##و', '##ع', 'ب', '##ب', '##ي', '##ا', '##ن', '##ه', 'و', '##ا', '##ع', '##ط', '##ى', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'س', '##ي', '##ل', '##ا', 'م', '##ن', 'ح', '##س', '##ن', 'ا', '##ل', '##س', '##ب', '##ك', 'و', '##ع', '##ذ', '##و', '##ب', '##ة', 'ا', '##ل', '##س', '##ج', '##ع', 'و', '##م', '##ن', 'ا', '##ل', '##ب', '##ل', '##ا', '##غ', '##ة', 'و', '##ا', '##ل', '##ب', '##ي', '##ا', '##ن', 'م', '##ا', 'ع', '##ج', '##ز', 'ع', '##ن', '##ه', 'ب', '##ل', '##غ', '##ا', '##ء', 'ا', '##ل', '##ع', '##ر', '##ب', 'و', '##ق', '##د', 'و', '##ح', '##د', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'ت', '##و', '##ح', '##ي', '##د', '##ا', 'ك', '##ا', '##م', '##ل', '##ا', 'و', '##ح', '##ف', '##ظ', '##ه', '##ا', 'م', '##ن', 'ا', '##ل', '##ت', '##ل', '##ا', '##ش', '##ي', 'و', '##ا', '##ل', '##ا', '##ن', '##ق', '##ر', '##ا', '##ض', 'ك', '##م', '##ا', 'ح', '##د', '##ث', 'م', '##ع', 'ا', '##ل', '##ع', '##د', '##ي', '##د', 'م', '##ن', 'ا', '##ل', '##ل', '##غ', '##ا', '##ت', 'ا', '##ل', '##س', '##ا', '##م', '##ي', '##ة', 'ا', '##ل', '##ا', '##خ', '##ر', '##ى', 'ا', '##ل', '##ت', '##ي', 'ا', '##ض', '##ح', '##ت', 'ل', '##غ', '##ا', '##ت', 'ب', '##ا', '##ى', '##د', '##ة', 'و', '##ا', '##ن', '##د', '##ث', '##ر', '##ت', 'م', '##ع', 'ا', '##ل', '##ز', '##م', '##ن', 'ا', '##و', 'ل', '##غ', '##ا', '##ت', 'ط', '##ا', '##ل', '##ه', '##ا', 'ا', '##ل', '##ض', '##ع', '##ف', 'و', '##ا', '##ل', '##ا', '##ن', '##ح', '##ط', '##ا', '##ط', 'و', '##ب', '##ا', '##ل', '##ت', '##ا', '##ل', '##ي', 'ع', '##د', '##م', 'ا', '##ل', '##ق', '##د', '##ر', '##ة', 'ع', '##ل', '##ى', 'م', '##س', '##ا', '##ي', '##ر', '##ة', 'ا', '##ل', '##ت', '##غ', '##ي', '##ي', '##ر', '##ا', '##ت', 'و', '##ا', '##ل', '##ت', '##ج', '##ا', '##ذ', '##ب', '##ا', '##ت', 'ا', '##ل', '##ت', '##ي', 'ت', '##ع', '##ر', '##ف', '##ه', '##ا', 'ا', '##ل', '##ح', '##ض', '##ا', '##ر', '##ة', 'و', '##ش', '##ع', '##و', '##ب', 'ا', '##ل', '##ع', '##ا', '##ل', '##م', 'ا', '##ل', '##ق', '##د', '##ي', '##م', 'و', '##ا', '##ل', '##ح', '##د', '##ي', '##ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem _base tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', '##ي', '##ع', '##و', '##د', 'ا', '##ل', '##ف', '##ض', '##ل', 'ف', '##ي', 'ت', '##و', '##ح', '##ي', '##د', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'ا', '##ل', '##ى', 'ن', '##ز', '##و', '##ل', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'ح', '##ي', '##ث', 'ل', '##م', 'ت', '##ك', '##ن', 'م', '##و', '##ح', '##د', '##ة', 'ق', '##ب', '##ل', 'ه', '##ذ', '##ا', 'ا', '##ل', '##ع', '##ه', '##د', 'ر', '##غ', '##م', 'ا', '##ن', '##ه', '##ا', 'ك', '##ا', '##ن', '##ت', 'ذ', '##ا', '##ت', 'غ', '##ن', '##ى', 'و', '##م', '##ر', '##و', '##ن', '##ة', 'ا', '##ل', '##ى', 'ا', '##ن', 'ن', '##ز', '##ل', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'و', '##ت', '##ح', '##د', '##ى', 'ا', '##ل', '##ج', '##م', '##و', '##ع', 'ب', '##ب', '##ي', '##ا', '##ن', '##ه', 'و', '##ا', '##ع', '##ط', '##ى', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'س', '##ي', '##ل', '##ا', 'م', '##ن', 'ح', '##س', '##ن', 'ا', '##ل', '##س', '##ب', '##ك', 'و', '##ع', '##ذ', '##و', '##ب', '##ة', 'ا', '##ل', '##س', '##ج', '##ع', 'و', '##م', '##ن', 'ا', '##ل', '##ب', '##ل', '##ا', '##غ', '##ة', 'و', '##ا', '##ل', '##ب', '##ي', '##ا', '##ن', 'م', '##ا', 'ع', '##ج', '##ز', 'ع', '##ن', '##ه', 'ب', '##ل', '##غ', '##ا', '##ء', 'ا', '##ل', '##ع', '##ر', '##ب', 'و', '##ق', '##د', 'و', '##ح', '##د', 'ا', '##ل', '##ق', '##ر', '##ا', '##ن', 'ا', '##ل', '##ك', '##ر', '##ي', '##م', 'ا', '##ل', '##ل', '##غ', '##ة', 'ا', '##ل', '##ع', '##ر', '##ب', '##ي', '##ة', 'ت', '##و', '##ح', '##ي', '##د', '##ا', 'ك', '##ا', '##م', '##ل', '##ا', 'و', '##ح', '##ف', '##ظ', '##ه', '##ا', 'م', '##ن', 'ا', '##ل', '##ت', '##ل', '##ا', '##ش', '##ي', 'و', '##ا', '##ل', '##ا', '##ن', '##ق', '##ر', '##ا', '##ض', 'ك', '##م', '##ا', 'ح', '##د', '##ث', 'م', '##ع', 'ا', '##ل', '##ع', '##د', '##ي', '##د', 'م', '##ن', 'ا', '##ل', '##ل', '##غ', '##ا', '##ت', 'ا', '##ل', '##س', '##ا', '##م', '##ي', '##ة', 'ا', '##ل', '##ا', '##خ', '##ر', '##ى', 'ا', '##ل', '##ت', '##ي', 'ا', '##ض', '##ح', '##ت', 'ل', '##غ', '##ا', '##ت', 'ب', '##ا', '##ى', '##د', '##ة', 'و', '##ا', '##ن', '##د', '##ث', '##ر', '##ت', 'م', '##ع', 'ا', '##ل', '##ز', '##م', '##ن', 'ا', '##و', 'ل', '##غ', '##ا', '##ت', 'ط', '##ا', '##ل', '##ه', '##ا', 'ا', '##ل', '##ض', '##ع', '##ف', 'و', '##ا', '##ل', '##ا', '##ن', '##ح', '##ط', '##ا', '##ط', 'و', '##ب', '##ا', '##ل', '##ت', '##ا', '##ل', '##ي', 'ع', '##د', '##م', 'ا', '##ل', '##ق', '##د', '##ر', '##ة', 'ع', '##ل', '##ى', 'م', '##س', '##ا', '##ي', '##ر', '##ة', 'ا', '##ل', '##ت', '##غ', '##ي', '##ي', '##ر', '##ا', '##ت', 'و', '##ا', '##ل', '##ت', '##ج', '##ا', '##ذ', '##ب', '##ا', '##ت', 'ا', '##ل', '##ت', '##ي', 'ت', '##ع', '##ر', '##ف', '##ه', '##ا', 'ا', '##ل', '##ح', '##ض', '##ا', '##ر', '##ة', 'و', '##ش', '##ع', '##و', '##ب', 'ا', '##ل', '##ع', '##ا', '##ل', '##م', 'ا', '##ل', '##ق', '##د', '##ي', '##م', 'و', '##ا', '##ل', '##ح', '##د', '##ي', '##ث']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using tkseem farasa_morphological_tokenizer tokenize method\n",
    "print(['\"غير_معروف\"' if item == '<UNK>' else item for item in chars_tokenizer.tokenize_from_splits(sample_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotless-arabic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5256e330910480c51c2a86292ea06783a785bff23f3345acb952af191eac8f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
