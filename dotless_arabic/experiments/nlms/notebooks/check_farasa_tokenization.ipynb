{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this file is to check whether farasa tokenization is context sensitive. That is, will farasa tokenize separate words that are out of sentences in the same manner as if they are in a sentence? let us see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Experiments/DotlessArabic\n"
     ]
    }
   ],
   "source": [
    "%cd ../../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from dotless_arabic.processing import process\n",
    "from dotless_arabic.datasets.utils import tokens_frequency\n",
    "from dotless_arabic.tokenizers import FarasaMorphologicalTokenizer\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_analysis as collect_quran_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بسم الله الرحمن الرحيم', 'الحمد لله رب العالمين']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(map(process,collect_quran_dataset()))\n",
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-29 07:48:26,961 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "segmenter = FarasaSegmenter(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdee151918d4bbca07b83bdf0ef004e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = list(map(lambda item: FarasaMorphologicalTokenizer.split_text(item,segmenter=segmenter),tqdm(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['بسم', '<##>', 'الله', '<##>', 'ال', 'رحمن', '<##>', 'ال', 'رحيم'],\n",
       " ['ال', 'حمد', '<##>', 'ل', 'الله', '<##>', 'رب', '<##>', 'ال', 'عالم', 'ين']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618358d6189f46dd81d0c26f3ae08d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_vocab_from_normal_text = [\n",
    "    subword\n",
    "    for vocab in tokens_frequency(tuple(dataset)).keys()\n",
    "    for subword in FarasaMorphologicalTokenizer.split_text(vocab, segmenter=segmenter)\n",
    "]\n",
    "tokenized_vocab_from_tokenized_text = list(\n",
    "    set(vocab for document in tokenized_dataset for vocab in document)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_vocab_from_normal_text = sorted(tokenized_vocab_from_normal_text)\n",
    "tokenized_vocab_from_tokenized_text = sorted(tokenized_vocab_from_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<##>', 'آتي', 'آجل', 'آخر', 'آفاق', 'آلن', 'أ', 'أبد', 'أبرار', 'أبصر']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_vocab_from_tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<##>', 'آتي', 'آجل', 'آخر', 'آفاق', 'آلن', 'أ', 'أبد', 'أبرار', 'أبصر']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_vocab_from_tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_vocab_from_tokenized_text == tokenized_vocab_from_tokenized_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
