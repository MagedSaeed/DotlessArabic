{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "\n",
    "import torch\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import datasets\n",
    "\n",
    "from dotless_arabic.tokenizers import WordTokenizer\n",
    "from dotless_arabic.experiments.nlms.src.training_pipeline import training_pipeline\n",
    "\n",
    "from dotless_arabic.datasets.utils import (\n",
    "    tokens_frequency,\n",
    "    calculate_entropy,\n",
    "    tokenize_dataset_for_statistics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_MODE']='disabled'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1' # to see CUDA errors\n",
    "torch.cuda.empty_cache() # to free gpu memory\n",
    "seed_everything(42,workers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092c70bf66c041ddad0138dd0bff3da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_LETTERS = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_english(text):\n",
    "    # add spaces between punctuations, if there is not\n",
    "    text = text.lower()\n",
    "    text = re.sub(\n",
    "        r\"\"\"([.,!?()\\/\\\\،\"'\\{\\}\\(\\)\\[\\]؟<>`؛=+\\-\\*\\&\\^\\%\\$\\#\\@\\!])\"\"\",\n",
    "        r\" \\1 \",\n",
    "        text,\n",
    "    )\n",
    "    # remove any non arabic character\n",
    "    text = \"\".join(\n",
    "        [c for c in text if c in ENGLISH_LETTERS or c.isspace()]\n",
    "    )  # keep only english chars and spaces\n",
    "    text = re.sub(\"\\s{2,}\", \" \", text).strip()  # remove multiple spaces\n",
    "    \"\"\"\n",
    "      interestingly, there is a difference betwen re.sub('\\s+',' ',s) and re.sub('\\s{2,}',' ',s)\n",
    "      the first one remove newlines while the second does not.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_vowels(text):\n",
    "    text_with_no_vowels = re.sub(r'[AEIOU]','',text,flags=re.IGNORECASE)\n",
    "    return text_with_no_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(example):\n",
    "    example['processed_text'] = process_english(example['text'])\n",
    "    example['consonants'] = strip_vowels(example['processed_text'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-12e7c27a0fe0238b.arrow\n",
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-907ca0784453a0f1.arrow\n",
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d613c4b133405eb2.arrow\n",
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-80eeb90e40a69518.arrow\n",
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d613c4b133405eb2.arrow\n",
      "Loading cached processed dataset at /home/magedsaeed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-80eeb90e40a69518.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'processed_text', 'consonants'],\n",
       "        num_rows: 13651\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'processed_text', 'consonants'],\n",
       "        num_rows: 13651\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'processed_text', 'consonants'],\n",
       "        num_rows: 13651\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].filter(lambda example:len(example['text'].split()) > 50).map(prepare_example)\n",
    "dataset['validation'] = dataset['train'].filter(lambda example:len(example['text'].split()) > 50).map(prepare_example)\n",
    "dataset['test'] = dataset['train'].filter(lambda example:len(example['text'].split()) > 50).map(prepare_example)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senj no valkyria unrecorded chronicles japanese lit valkyria of the battlefield commonly referred to as valkyria chronicles iii outside japan is a tactical role playing video game developed by sega and media vision for the playstation portable released in january in japan it is the third game in the valkyria series employing the same fusion of tactical and real time gameplay as its predecessors the story runs parallel to the first game and follows the nameless a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit calamaty raven',\n",
       " 'the game began development in carrying over a large portion of the work done on valkyria chronicles ii while it retained the standard features of the series it also underwent multiple adjustments such as making the game more forgiving for series newcomers character designer raita honjou and composer hitoshi sakimoto both returned from previous entries along with valkyria chronicles ii director takeshi ozawa a large team of writers handled the script the game s opening theme was sung by may n',\n",
       " 'it met with positive sales in japan and was praised by both japanese and western critics after release it received downloadable content along with an expanded edition in november of that year it was also adapted into manga and an original video animation series due to low sales of valkyria chronicles ii valkyria chronicles iii was not localized but a fan translation compatible with the game s expanded edition was released in media vision would return to the franchise with the development of valkyria azure revolution for the playstation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['processed_text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['snj n vlkyr nrcrdd chrncls jpns lt vlkyr f th bttlfld cmmnly rfrrd t s vlkyr chrncls  tsd jpn s  tctcl rl plyng vd gm dvlpd by sg nd md vsn fr th plysttn prtbl rlsd n jnry n jpn t s th thrd gm n th vlkyr srs mplyng th sm fsn f tctcl nd rl tm gmply s ts prdcssrs th stry rns prlll t th frst gm nd fllws th nmlss  pnl mltry nt srvng th ntn f gll drng th scnd rpn wr wh prfrm scrt blck prtns nd r pttd gnst th mprl nt clmty rvn',\n",
       " 'th gm bgn dvlpmnt n crryng vr  lrg prtn f th wrk dn n vlkyr chrncls  whl t rtnd th stndrd ftrs f th srs t ls ndrwnt mltpl djstmnts sch s mkng th gm mr frgvng fr srs nwcmrs chrctr dsgnr rt hnj nd cmpsr htsh skmt bth rtrnd frm prvs ntrs lng wth vlkyr chrncls  drctr tksh zw  lrg tm f wrtrs hndld th scrpt th gm s pnng thm ws sng by my n',\n",
       " 't mt wth pstv sls n jpn nd ws prsd by bth jpns nd wstrn crtcs ftr rls t rcvd dwnldbl cntnt lng wth n xpndd dtn n nvmbr f tht yr t ws ls dptd nt mng nd n rgnl vd nmtn srs d t lw sls f vlkyr chrncls  vlkyr chrncls  ws nt lclzd bt  fn trnsltn cmptbl wth th gm s xpndd dtn ws rlsd n md vsn wld rtrn t th frnchs wth th dvlpmnt f vlkyr zr rvltn fr th plysttn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['consonants'][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91d4f43f9fd47cda4554581c2bb2bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8c1bfbbc54418ca36fe99592d13a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69b6625ce904b40b3cd6b84a677d7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_tokens_frequency = tokens_frequency(tuple(dataset['train']['text']))\n",
    "processed_text_tokens_frequency = tokens_frequency(tuple(dataset['train']['processed_text']))\n",
    "consonants_tokens_frequency = tokens_frequency(tuple(dataset['train']['consonants']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41037, 60111)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict(sorted(consonants_tokens_frequency.items(),key=lambda item:item[1],reverse=True))),len(dict(sorted(processed_text_tokens_frequency.items(),key=lambda item:item[1],reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.089455279909119, 10.749834530719115)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(dict(sorted(consonants_tokens_frequency.items(),key=lambda item:item[1],reverse=True))),calculate_entropy(dict(sorted(processed_text_tokens_frequency.items(),key=lambda item:item[1],reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(dataset['train']['processed_text'])\n",
    "val_dataset = list(dataset['validation']['processed_text'])\n",
    "test_dataset = list(dataset['test']['processed_text'])\n",
    "\n",
    "consonants_train_dataset = list(dataset['train']['consonants'])\n",
    "consonants_val_dataset =  list(dataset['validation']['consonants'])\n",
    "test_dataset = list(dataset['test']['consonants'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Train Samples: 11,670\n",
      "Val Samples: 615\n",
      "Test Samples: 1,366\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Calculating vocab size using WordTokenizer:\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9629055c73de41fd85e64449f97880ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Considered Vocab (from WordTokenizer): 30,723\n",
      "All Vocab (WordTokenizer): 55,968\n",
      "####################################################################################################\n",
      "Training WordTokenizer ...\n",
      "####################################################################################################\n",
      "Tokenizer Vocab Size: 30,723\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Calculating Sequence Length:\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfa43aa6f6e4aa7b0e81864285491b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1385ea89a4f18a04309152067f065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Sequence Length: 298\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Building DataLoaders\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0ae8c728e4411f8d81755860cc6ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3245599784475d8e1493c69d0ba55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/615 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85173feaa30d48349c4d6e8c7f00db80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Train DataLoader: 364\n",
      "Val DataLoader: 19\n",
      "Test DataLoader: 42\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magedsaeed/.virtualenvs/dotless-arabic/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/magedsaeed/.virtualenvs/dotless-arabic/lib/pyt ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "| Name               | Type      | Params\n",
      "-------------------------------------------------\n",
      "0 | embedding_layer    | Embedding | 15.7 M\n",
      "1 | gru_layer          | GRU       | 6.3 M\n",
      "2 | first_dense_layer  | Linear    | 262 K\n",
      "3 | dropout_layer      | Dropout   | 0\n",
      "4 | relu               | ReLU      | 0\n",
      "5 | second_dense_layer | Linear    | 15.8 M\n",
      "-------------------------------------------------\n",
      "22.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.3 M    Total params\n",
      "89.309    Total estimated model params size (MB)\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magedsaeed/.virtualenvs/dotless-arabic/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/magedsaeed/.virtualenvs/dotless-arabic/lib/pyt ...\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36486445f9e24bd2b57938ac4c27d999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.533679008483887     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.533679008483887    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ embedding_layer    │ Embedding │ 15.7 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ gru_layer          │ GRU       │  6.3 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ first_dense_layer  │ Linear    │  262 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ dropout_layer      │ Dropout   │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ relu               │ ReLU      │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ second_dense_layer │ Linear    │ 15.8 M │\n",
       "└───┴────────────────────┴───────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ embedding_layer    │ Embedding │ 15.7 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ gru_layer          │ GRU       │  6.3 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ first_dense_layer  │ Linear    │  262 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ dropout_layer      │ Dropout   │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ relu               │ ReLU      │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ second_dense_layer │ Linear    │ 15.8 M │\n",
       "└───┴────────────────────┴───────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 22.3 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 22.3 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 89                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 22.3 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 22.3 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 89                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14d67bc79144bdbaf54bad07f2cfc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_pipeline(\n",
    "    dataset = train_dataset,\n",
    "    is_dotted=True, # do not run the undot() method that is specific for Arabic.\n",
    "    dataset_id='processed_wikitext',\n",
    "    batch_size=32,\n",
    "    gpu_devices=1,\n",
    "    cpu_devices=1,\n",
    "    dataset_name='wikitext',\n",
    "    results_file=None,\n",
    "    vocab_coverage=0.98,\n",
    "    tokenizer_class=WordTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotless-arabic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
