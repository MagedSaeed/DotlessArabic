####################################################################################################
Undotted Training Started at 2024-03-16 12:26:54.977184 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
لكليهما مع وجود نسبة صغيرة على شكل بخار ماء معلق في الهواء على هيىة سحاب غيوم واحيانا اخرى على هيىة ضباب او ندى بالاضافة الى الزخات المطرية او الثلجية
اما في الطبيعة فتتغير حالة الماء بين الحالات الثلاثة للمادة على سطح الارض باستمرار من خلال ما يعرف باسم الدورة الماىية او دورة الماء والتي تتضمن حدوث تبخر ونتح نتح تبخري ثم تكثيف فهطول ثم جريان لتصل الى المصب في المسطحات الماىية
وفي العقود الاخيرة سجلت حالات شح في المياه العذبة في مناطق عديدة من العالم ولقد قدرت احصاءات الامم المتحدة ان حوالي مليار شخص على سطح الارض لا يزالون يفتقرون الوساىل المتاحة للوصول الى مصدر امن لمياه الشرب وان حوالي
ظهر في سنة تقرير عن اكتشاف سحابة هاىلة من بخار الماء في الكون وبكميات تفوق الكمية الموجودة على الارض ب تريليون مرة في محيط نجم زاىف يبعد حوالي مليار سنة ضوىية عن الارض
يوجد الماء في الكون على العموم بحالاته الثلاثة الصلبة والساىلة والغازية بالاضافة لامكانية افتراضية لوجوده على شكل يدعى ماء فاىق التاين حيث يتبلور الاكسجين وتبقى ايونات الهيدروجين عاىمة بشكل حر داخل الشبكة البلورية للاكسجين
####################################################################################################
####################################################################################################
Train Samples: 1,248,491
Val Samples: 65,711
Test Samples: 146,023
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 70,939
All Vocab (WordTokenizer): 730,889
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 70,940
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 128
####################################################################################################
####################################################################################################
Getting Vocab counts
####################################################################################################
####################################################################################################
train vocab count: 970,567
train tokens count: 54,008,231
----------------------------------------
val vocab count: 211,025
val tokens count: 2,836,439
----------------------------------------
test vocab count: 323,331
test tokens count: 6,308,651
----------------------------------------
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 19,500
Val DataLoader: 1,026
Test DataLoader: 2,280
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | pos_encoder         | PositionalEncoding | 0
1 | embedding           | Embedding          | 36.3 M
2 | transformer_encoder | TransformerEncoder | 9.5 M
3 | linear              | Linear             | 36.4 M
4 | train_ppl           | Perplexity         | 0
5 | val_ppl             | Perplexity         | 0
6 | test_ppl            | Perplexity         | 0
-----------------------------------------------------------
82.2 M    Trainable params
0         Non-trainable params
82.2 M    Total params
328.683   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Perplexity Results for Train,Validation, and Test Dataloaders:
[
{
"test_ppl/dataloader_idx_0": 90.97078704833984,
"test_loss/dataloader_idx_0": 4.503325462341309
},
{
"test_ppl/dataloader_idx_1": 199.62701416015625,
"test_loss/dataloader_idx_1": 5.285583972930908
},
{
"test_ppl/dataloader_idx_2": 198.10934448242188,
"test_loss/dataloader_idx_2": 5.277791500091553
}
]
####################################################################################################
####################################################################################################
Training OOVs rate: 4.98
Validation OOVs rate: 4.98
Test OOVs rate: 4.98
####################################################################################################
####################################################################################################
Training Time: 70379.403 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 5413.800 seconds
####################################################################################################
####################################################################################################
<bos>مںٮٮںاهمماٮمٮرهدهالمٮطڡههواںكلماڡامٮهالمعرٮهووحودعدهڡصورڡىالمٮطڡهالٮىكاٮٮٮحٮلهاالمٮطڡهمں
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer WordTokenizer at 2024-03-17 08:58:10.110795
####################################################################################################
