####################################################################################################
Undotted Training Started at 2024-03-13 21:42:04.827260 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Train Samples: 5,331
Val Samples: 281
Test Samples: 624
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 8,783
All Vocab (WordTokenizer): 12,106
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 8,784
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 47
####################################################################################################
####################################################################################################
Getting Vocab counts
####################################################################################################
####################################################################################################
train vocab count: 13,435
train tokens count: 66,473
----------------------------------------
val vocab count: 1,720
val tokens count: 3,425
----------------------------------------
test vocab count: 3,161
test tokens count: 7,899
----------------------------------------
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 83
Val DataLoader: 4
Test DataLoader: 9
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | pos_encoder         | PositionalEncoding | 0
1 | embedding           | Embedding          | 4.5 M
2 | transformer_encoder | TransformerEncoder | 9.5 M
3 | linear              | Linear             | 4.5 M
4 | train_ppl           | Perplexity         | 0
5 | val_ppl             | Perplexity         | 0
6 | test_ppl            | Perplexity         | 0
-----------------------------------------------------------
18.5 M    Trainable params
0         Non-trainable params
18.5 M    Total params
73.843    Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Perplexity Results for Train,Validation, and Test Dataloaders:
[
{
"test_ppl/dataloader_idx_0": 19.18128204345703,
"test_loss/dataloader_idx_0": 2.9501125812530518
},
{
"test_ppl/dataloader_idx_1": 113.36209106445312,
"test_loss/dataloader_idx_1": 4.725360870361328
},
{
"test_ppl/dataloader_idx_2": 117.739013671875,
"test_loss/dataloader_idx_2": 4.760225296020508
}
]
####################################################################################################
####################################################################################################
Training OOVs rate: 5.00
Validation OOVs rate: 5.00
Test OOVs rate: 5.00
####################################################################################################
####################################################################################################
Training Time: 205.644 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 18.694 seconds
####################################################################################################
####################################################################################################
<bos>اںالدٮںامٮواوعملواالصالحاٮلهمحٮاٮالٮعٮممںٮحٮهاالاٮهارحالدٮںڡٮهااٮدالهمعداٮالٮمٮماكاٮواٮعملوںٮصٮركاٮوالكممںالله
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer WordTokenizer at 2024-03-13 21:46:11.559519
####################################################################################################
