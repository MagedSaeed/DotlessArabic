####################################################################################################
Undotted Training Started at 2023-09-21 13:29:08.629111 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Train Samples: 5,331
Val Samples: 281
Test Samples: 624
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 8,783
All Vocab (WordTokenizer): 12,106
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 8,783
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 47
####################################################################################################
####################################################################################################
Getting Vocab counts
####################################################################################################
####################################################################################################
train vocab count: 13,435
train tokens count: 66,473
----------------------------------------
val vocab count: 1,720
val tokens count: 3,425
----------------------------------------
test vocab count: 3,161
test tokens count: 7,899
----------------------------------------
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 83
Val DataLoader: 4
Test DataLoader: 9
####################################################################################################
####################################################################################################
| Name               | Type       | Params
--------------------------------------------------
0 | embedding_layer    | Embedding  | 2.2 M
1 | rnn                | GRU        | 789 K
2 | dropout_layer      | Dropout    | 0
3 | relu               | LeakyReLU  | 0
4 | first_dense_layer  | Linear     | 65.8 K
5 | second_dense_layer | Linear     | 2.3 M
6 | train_ppl          | Perplexity | 0
7 | val_ppl            | Perplexity | 0
8 | test_ppl           | Perplexity | 0
--------------------------------------------------
3.1 M     Trainable params
0         Non-trainable params
3.1 M     Total params
12.450    Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Perplexity Results for Train,Validation, and Test Dataloaders:
[
{
"test_ppl/dataloader_idx_0": 111.256591796875,
"test_loss/dataloader_idx_0": 4.704885482788086
},
{
"test_ppl/dataloader_idx_1": 178.364990234375,
"test_loss/dataloader_idx_1": 5.177260398864746
},
{
"test_ppl/dataloader_idx_2": 195.68544006347656,
"test_loss/dataloader_idx_2": 5.262955188751221
}
]
####################################################################################################
####################################################################################################
Training OOVs rate: 5.00
Validation OOVs rate: 5.00
Test OOVs rate: 5.00
####################################################################################################
####################################################################################################
Training Time: 188.116 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 13.437 seconds
####################################################################################################
####################################################################################################
<bos> ڡل ٮا موسى ادا كاں مں ڡٮل الا مں ٮعد مں دوں الله مں رٮك مں ٮساء وما كاٮوا ڡٮه ٮسٮهرىوں الا مں سٮء اں
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer WordTokenizer at 2023-09-21 13:33:01.977972
####################################################################################################
