####################################################################################################
Number datasets samples:
254630
####################################################################################################
####################################################################################################
Number datasets samples after filtering non accepted meters:
148603
####################################################################################################
####################################################################################################
Sample of datasets samples:
أَصبَحَ المُلك لِلَّذي فَطر الخَل قَ بِتَقديرٍ للعَزيز العَليمِ غافر الذَنب للمسيءِ بِعَفوٍ قابل التَوب ذي العَطاء العَميمِ مُرسل المُصطَفى البَشير إِلَينا رَحمة مِنهُ بِالكَلام القَديمِ رَبَنا رَبّنا إِلَيكَ أَنينا فَأَجرنا مِن حَر نار الجَحيمِ وَاكفِنا شَرّ ما نَخاف بِلُطفٍ يا عَظيماً يَرجى لِكُل عَظيمِ وَتَقبل أَعمالَنا وَاعفُ عَنا وَأَنلنا دُخول دار النَعيمِ
بِنَبي بَعثَتهُ فَهَدانا لِصِراط مِن الهُدى مُستَقيمِ وَبِمَن نَحنُ في حِماهُ مَدى الدَهر أَخيهِ يَحيى الحصور الكَريمِ أَدرك أَدرك قَوماً أَتوا بافتقار وَاِنكِسار وَمَدمَع مَسجومِ شَهدت أَرواحَهُم أَنكَ اللَهُ وَجاءوا بِكُل قَلبٍ سَليم
####################################################################################################
####################################################################################################
Number of poems:
384,872
####################################################################################################
####################################################################################################
Dotted Training Started at 2023-11-13 13:56:24.719464 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم مرسل المصطفى البشير الينا رحمة منه بالكلام القديم ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم وتقبل اعمالنا واعف عنا وانلنا دخول دار النعيم
بنبي بعثته فهدانا لصراط من الهدى مستقيم وبمن نحن في حماه مدى الدهر اخيه يحيى الحصور الكريم ادرك ادرك قوما اتوا بافتقار وانكسار ومدمع مسجوم شهدت ارواحهم انك الله وجاءوا بكل قلب سليم
من اي مولى ارتجي ولاي باب التجي والله حي رازق يعطي الجزيل لمرتجي رب جواد لم يزل من كل ضيق مخرجي ان رحت ارجوغيره خاب الرواح مع المجي يا عيس امالي اقصدي باب الكريم وعرجي وضعي رحالك وارتعي فالام حمل المزعج
وتوسلي بمحمد وباله كي تنتجي الهاشمي المصطفى صج الهدى المتبلج وبشيبة الصديق صا حب كل فضل ابهج والسيد الفاروق من بسوى الهدى لم يلهج وبصنوه عثمان ذي الن نورين اقوم منهج وعلي الكرار فا تح كل باب مرتج
وبقية الصحب الكرا م اولي الثنا المتارج هم ابحر الفضل الذي ن بغيرهم لم تفرج وكذا السفينة ان نجت فجميع من فيها نجي
####################################################################################################
####################################################################################################
Train Samples: 329,064
Val Samples: 17,320
Test Samples: 38,488
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 184,570
All Vocab (WordTokenizer): 656,895
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 36
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 333
####################################################################################################
####################################################################################################
Getting Vocab counts
####################################################################################################
####################################################################################################
train vocab count: 656,891
train tokens count: 15,062,406
----------------------------------------
val vocab count: 143,190
val tokens count: 795,551
----------------------------------------
test vocab count: 225,221
test tokens count: 1,763,288
----------------------------------------
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 5,141
Val DataLoader: 270
Test DataLoader: 601
####################################################################################################
####################################################################################################
| Name               | Type       | Params
--------------------------------------------------
0 | embedding_layer    | Embedding  | 18.4 K
1 | rnn                | GRU        | 6.3 M
2 | dropout_layer      | Dropout    | 0
3 | relu               | LeakyReLU  | 0
4 | first_dense_layer  | Linear     | 262 K
5 | second_dense_layer | Linear     | 18.5 K
6 | train_ppl          | Perplexity | 0
7 | val_ppl            | Perplexity | 0
8 | test_ppl           | Perplexity | 0
--------------------------------------------------
6.6 M     Trainable params
0         Non-trainable params
6.6 M     Total params
26.339    Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Perplexity Results for Train,Validation, and Test Dataloaders:
[
{
"test_ppl/dataloader_idx_0": 6.142560005187988,
"test_loss/dataloader_idx_0": 1.8151037693023682
},
{
"test_ppl/dataloader_idx_1": 6.16245698928833,
"test_loss/dataloader_idx_1": 1.8183242082595825
},
{
"test_ppl/dataloader_idx_2": 6.166706562042236,
"test_loss/dataloader_idx_2": 1.819024920463562
}
]
####################################################################################################
####################################################################################################
Training OOVs rate: 0.00
Validation OOVs rate: 0.00
Test OOVs rate: 0.00
####################################################################################################
####################################################################################################
Training Time: 5427.807 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 904.634 seconds
####################################################################################################
####################################################################################################
<bos>   رالومالمنصل   ربالالال
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer CharacterTokenizer at 2023-11-13 21:34:31.476917
####################################################################################################
