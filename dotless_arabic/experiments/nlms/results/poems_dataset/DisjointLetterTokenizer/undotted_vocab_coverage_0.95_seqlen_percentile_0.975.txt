####################################################################################################
Undotted Training Started at 2023-11-04 23:56:19.052869 for tokenizer: DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم مرسل المصطفى البشير الينا رحمة منه بالكلام القديم ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم وتقبل اعمالنا واعف عنا وانلنا دخول دار النعيم
بنبي بعثته فهدانا لصراط من الهدى مستقيم وبمن نحن في حماه مدى الدهر اخيه يحيى الحصور الكريم ادرك ادرك قوما اتوا بافتقار وانكسار ومدمع مسجوم شهدت ارواحهم انك الله وجاءوا بكل قلب سليم
من اي مولى ارتجي ولاي باب التجي والله حي رازق يعطي الجزيل لمرتجي رب جواد لم يزل من كل ضيق مخرجي ان رحت ارجوغيره خاب الرواح مع المجي يا عيس امالي اقصدي باب الكريم وعرجي وضعي رحالك وارتعي فالام حمل المزعج
وتوسلي بمحمد وباله كي تنتجي الهاشمي المصطفى صج الهدى المتبلج وبشيبة الصديق صا حب كل فضل ابهج والسيد الفاروق من بسوى الهدى لم يلهج وبصنوه عثمان ذي الن نورين اقوم منهج وعلي الكرار فا تح كل باب مرتج
وبقية الصحب الكرا م اولي الثنا المتارج هم ابحر الفضل الذي ن بغيرهم لم تفرج وكذا السفينة ان نجت فجميع من فيها نجي
####################################################################################################
####################################################################################################
Train Samples: 329,064
Val Samples: 17,320
Test Samples: 38,488
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 95,922
All Vocab (WordTokenizer): 425,798
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 16,889
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 207
####################################################################################################
####################################################################################################
Getting Vocab counts
####################################################################################################
####################################################################################################
train vocab count: 656,891
train tokens count: 15,062,406
----------------------------------------
val vocab count: 143,190
val tokens count: 795,551
----------------------------------------
test vocab count: 225,221
test tokens count: 1,763,288
----------------------------------------
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 5,141
Val DataLoader: 270
Test DataLoader: 601
####################################################################################################
####################################################################################################
| Name               | Type       | Params
--------------------------------------------------
0 | embedding_layer    | Embedding  | 8.6 M
1 | rnn                | GRU        | 3.2 M
2 | dropout_layer      | Dropout    | 0
3 | relu               | LeakyReLU  | 0
4 | first_dense_layer  | Linear     | 262 K
5 | second_dense_layer | Linear     | 8.7 M
6 | train_ppl          | Perplexity | 0
7 | val_ppl            | Perplexity | 0
8 | test_ppl           | Perplexity | 0
--------------------------------------------------
12.1 M    Trainable params
0         Non-trainable params
12.1 M    Total params
48.314    Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Perplexity Results for Train,Validation, and Test Dataloaders:
[
{
"test_ppl/dataloader_idx_0": 16.6151123046875,
"test_loss/dataloader_idx_0": 2.8098912239074707
},
{
"test_ppl/dataloader_idx_1": 16.79686164855957,
"test_loss/dataloader_idx_1": 2.8207521438598633
},
{
"test_ppl/dataloader_idx_2": 16.801149368286133,
"test_loss/dataloader_idx_2": 2.821028470993042
}
]
####################################################################################################
####################################################################################################
Training OOVs rate: 32.47
Validation OOVs rate: 32.47
Test OOVs rate: 32.47
####################################################################################################
####################################################################################################
Training Time: 5971.349 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 853.049 seconds
####################################################################################################
####################################################################################################
<bos>اں<UNK>الحٮاه<UNK>ادا<UNK>اورٮ<UNK>الحوراء<UNK>واحد
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer DisjointLetterTokenizer at 2023-11-05 01:47:10.647166
####################################################################################################
