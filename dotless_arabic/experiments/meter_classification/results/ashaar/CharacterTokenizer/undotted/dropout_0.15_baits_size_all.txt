####################################################################################################
Number datasets samples:
254630
####################################################################################################
####################################################################################################
Number datasets samples after filtering non accepted meters:
144186
####################################################################################################
####################################################################################################
Sample of datasets samples:
أَصبَحَ المُلك لِلَّذي فَطر الخَل
قَ بِتَقديرٍ للعَزيز العَليمِ
غافر الذَنب للمسيءِ بِعَفوٍ
قابل التَوب ذي العَطاء العَميمِ
مُرسل المُصطَفى البَشير إِلَينا
رَحمة مِنهُ بِالكَلام القَديمِ
رَبَنا رَبّنا إِلَيكَ أَنينا
فَأَجرنا مِن حَر نار الجَحيمِ
وَاكفِنا شَرّ ما نَخاف بِلُطفٍ
يا عَظيماً يَرجى لِكُل عَظيمِ
####################################################################################################
####################################################################################################
Number of Baits:
1,794,858
####################################################################################################
####################################################################################################
Map meter names to classes:
####################################################################################################
####################################################################################################
dataset's classes:{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}
####################################################################################################
####################################################################################################
filter out empty baits or baits with one verse only.
####################################################################################################
####################################################################################################
baits in dataset after the last filteration: 1786467
####################################################################################################
####################################################################################################
processing the dataset
####################################################################################################
####################################################################################################
shuffling the dataset
####################################################################################################
####################################################################################################
split to train/val/test
####################################################################################################
####################################################################################################
x_train size:1612285
y_train size:1612285

x_val size:84858
y_val size:84858

x_test size:89324
y_test size:89324
####################################################################################################
####################################################################################################
Training Started at 2023-05-04 22:14:16.748306 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
ڡلا ٮرح الاسلام مٮك مسٮدا # ٮٮصر وسعد داٮم لا ٮراٮله
ولو كاٮٮ ارادٮه ٮعالى # ارادٮٮا لٮم لٮا المراد
واٮں ڡٮا دار الى حٮٮٮه # وحسٮك مصطاڡا هٮاك ومرٮعا
وٮالاڡك ما عٮرٮ لا ٮحڡٮڡه # ڡما الكٮر مں ساٮى ولا كٮٮ ڡى دل
ڡڡٮك للعٮں حسں راىٯ ٮهح # وعٮك للسمع ڡول طٮٮ طرٮ
####################################################################################################
####################################################################################################
Calculating vocab size using WordTokenizer:
####################################################################################################
####################################################################################################
Considered Vocab (from WordTokenizer): 425,517
All Vocab (WordTokenizer): 425,517
####################################################################################################
####################################################################################################
Tokenizer Vocab Size: 23
####################################################################################################
####################################################################################################
Calculating Sequence Length:
####################################################################################################
####################################################################################################
Sequence Length: 110
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 3,148
Val DataLoader: 165
Test DataLoader: 174
####################################################################################################
####################################################################################################
Training OOVs rate: 0.00
Validation OOVs rate: 0.00
Test OOVs rate: 0.00
####################################################################################################
####################################################################################################
| Name               | Type               | Params
----------------------------------------------------------
0 | train_accuracy     | MulticlassAccuracy | 0
1 | val_accuracy       | MulticlassAccuracy | 0
2 | test_accuracy      | MulticlassAccuracy | 0
3 | embedding_layer    | Embedding          | 5.9 K
4 | gru_layer          | GRU                | 5.5 M
5 | first_dense_layer  | Linear             | 32.9 K
6 | dropout_layer      | Dropout            | 0
7 | relu               | ReLU               | 0
8 | second_dense_layer | Linear             | 2.1 K
----------------------------------------------------------
5.6 M     Trainable params
0         Non-trainable params
5.6 M     Total params
22.245    Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 4515.09 seconds
####################################################################################################
####################################################################################################
Test Accuracy: 0.9423
Test Loss: 0.2445
####################################################################################################
####################################################################################################
Test Confusion Matrix:
tensor([[20800,     0,   102,    32,   265,     1,    24,    10,     7,     0,
42,     0,     0,     0,    14,     0],
[    1,   303,     3,     3,     6,     1,     3,    20,     4,     2,
28,     0,     0,     0,     1,     9],
[  110,     4, 12201,    37,   100,     4,    52,    13,    23,    36,
23,     4,     0,     0,    11,     3],
[   64,     4,    40,  6999,    59,    56,    21,    33,     8,     1,
39,     0,     0,     1,    38,     3],
[  242,     2,   114,    27, 18449,     4,   373,    46,   107,    26,
63,    26,     0,     0,    23,    14],
[    2,     4,     4,   161,     5,   313,     7,    10,     1,     1,
5,     3,     0,     1,     2,     3],
[   43,     2,    68,    33,   238,    16,  5272,    46,   123,    69,
42,    59,     1,     1,    28,    10],
[   36,    30,    23,    30,    55,    11,    67,  3691,    17,     4,
92,    31,     0,     2,    27,    13],
[   26,     7,    22,    11,    47,     0,   161,     6,  2664,    12,
17,     0,     0,     0,    26,     7],
[    8,     0,    24,     0,    27,     0,    19,     0,    17,  1305,
14,     0,     0,     0,     6,     2],
[  145,    24,    48,    25,    59,     2,    40,    48,    16,    15,
7957,     5,     0,     2,    19,     5],
[    3,     0,     7,     7,    29,     4,    44,    27,     6,     2,
13,   802,     3,     0,     4,     1],
[    0,     0,     0,     1,     0,     2,     0,     0,     0,     0,
0,     0,     3,     0,     0,     0],
[    0,     0,     3,     0,     0,     0,     5,     3,     0,     3,
5,     0,     0,    33,     0,     0],
[   26,     0,    10,    36,    12,     1,    21,    13,    36,     4,
14,     2,     1,     0,  3186,     7],
[    1,     1,     1,     3,    10,     1,     3,     4,     2,     0,
1,     0,     0,     0,     4,   190]])
####################################################################################################
####################################################################################################
Training Finished for tokenizer CharacterTokenizer at 2023-05-04 23:34:58.257974
####################################################################################################
