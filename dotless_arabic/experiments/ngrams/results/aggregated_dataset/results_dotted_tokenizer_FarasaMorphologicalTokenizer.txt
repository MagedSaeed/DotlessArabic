####################################################################################################
Dotted Training Started at 2023-02-01 20:41:56.608834 for tokenizer: FarasaMorphologicalTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
بسم الله ال ##رحمن ال ##رحيم
ال ##حمد ل ##الله رب ال ##عالم ##ين
ال ##رحمن ال ##رحيم
مالك يوم ال ##دين
اياك نعبد و ##اياك نستعين
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-aggregated_dataset": {
        "2": {
            "perplexity_with_OOVs": 130.99926057726373,
            "perplexity_without_OOVs": 128.4301060985662,
            "counts_of_OOVs": "59,047",
            "ngram_counts": "17,323,822"
        },
        "3": {
            "perplexity_with_OOVs": 63.82183502594435,
            "perplexity_without_OOVs": 62.50753091738669,
            "counts_of_OOVs": "59,047",
            "ngram_counts": "70,431,135"
        },
        "4": {
            "perplexity_with_OOVs": 41.98158867620103,
            "perplexity_without_OOVs": 41.094657202205525,
            "counts_of_OOVs": "59,047",
            "ngram_counts": "148,188,967"
        },
        "5": {
            "perplexity_with_OOVs": 35.17222892991974,
            "perplexity_without_OOVs": 34.42198222240259,
            "counts_of_OOVs": "59,047",
            "ngram_counts": "216,245,462"
        },
        "6": {
            "perplexity_with_OOVs": 32.58972091802739,
            "perplexity_without_OOVs": 31.891970359329157,
            "counts_of_OOVs": "59,047",
            "ngram_counts": "260,530,893"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer FarasaMorphologicalTokenizer at 2023-02-01 23:53:36.739272
####################################################################################################
