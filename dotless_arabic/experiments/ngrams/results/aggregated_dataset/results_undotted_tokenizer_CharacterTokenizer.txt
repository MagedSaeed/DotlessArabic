####################################################################################################
Undotted Training Started at 2023-03-29 12:30:13.990282 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
ٮ س م <##> ا ل ل ه <##> ا ل ر ح م ں <##> ا ل ر ح ٮ م
ا ل ح م د <##> ل ل ه <##> ر ٮ <##> ا ل ع ا ل م ٮ ں
ا ل ر ح م ں <##> ا ل ر ح ٮ م
م ا ل ك <##> ٮ و م <##> ا ل د ٮ ں
ا ٮ ا ك <##> ٮ ع ٮ د <##> و ا ٮ ا ك <##> ٮ س ٮ ع ٮ ں
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "undotted-aggregated_dataset": {
        "2": {
            "perplexity_with_OOVs": 9.725396249313434,
            "perplexity_without_OOVs": 9.725396249313434,
            "counts_of_OOVs": "0",
            "ngram_counts": "400"
        },
        "3": {
            "perplexity_with_OOVs": 7.858491816458432,
            "perplexity_without_OOVs": 7.858491816458432,
            "counts_of_OOVs": "0",
            "ngram_counts": "6,688"
        },
        "4": {
            "perplexity_with_OOVs": 6.271888773355084,
            "perplexity_without_OOVs": 6.271888773355084,
            "counts_of_OOVs": "0",
            "ngram_counts": "80,030"
        },
        "5": {
            "perplexity_with_OOVs": 5.291158647866847,
            "perplexity_without_OOVs": 5.291158647866847,
            "counts_of_OOVs": "0",
            "ngram_counts": "627,778"
        },
        "6": {
            "perplexity_with_OOVs": 4.7315506701846015,
            "perplexity_without_OOVs": 4.7315506701846015,
            "counts_of_OOVs": "0",
            "ngram_counts": "3,710,653"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer CharacterTokenizer at 2023-03-29 13:03:25.553179
####################################################################################################
