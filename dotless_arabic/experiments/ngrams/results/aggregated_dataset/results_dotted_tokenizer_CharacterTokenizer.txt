####################################################################################################
Dotted Training Started at 2023-02-01 14:11:49.175881 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
ب ##س ##م ا ##ل ##ل ##ه ا ##ل ##ر ##ح ##م ##ن ا ##ل ##ر ##ح ##ي ##م
ا ##ل ##ح ##م ##د ل ##ل ##ه ر ##ب ا ##ل ##ع ##ا ##ل ##م ##ي ##ن
ا ##ل ##ر ##ح ##م ##ن ا ##ل ##ر ##ح ##ي ##م
م ##ا ##ل ##ك ي ##و ##م ا ##ل ##د ##ي ##ن
ا ##ي ##ا ##ك ن ##ع ##ب ##د و ##ا ##ي ##ا ##ك ن ##س ##ت ##ع ##ي ##ن
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-aggregated_dataset": {
        "2": {
            "perplexity_with_OOVs": 19.50226842946335,
            "perplexity_without_OOVs": 19.50226842946335,
            "counts_of_OOVs": "0",
            "ngram_counts": "3,824"
        },
        "3": {
            "perplexity_with_OOVs": 12.765147451650048,
            "perplexity_without_OOVs": 12.765147451650048,
            "counts_of_OOVs": "0",
            "ngram_counts": "126,165"
        },
        "4": {
            "perplexity_with_OOVs": 8.368468383410175,
            "perplexity_without_OOVs": 8.368468383410175,
            "counts_of_OOVs": "0",
            "ngram_counts": "1,870,150"
        },
        "5": {
            "perplexity_with_OOVs": 6.470319464510865,
            "perplexity_without_OOVs": 6.470319464510865,
            "counts_of_OOVs": "0",
            "ngram_counts": "14,248,465"
        },
        "6": {
            "perplexity_with_OOVs": 5.518974892680441,
            "perplexity_without_OOVs": 5.518974892680441,
            "counts_of_OOVs": "0",
            "ngram_counts": "55,711,960"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer CharacterTokenizer at 2023-02-01 14:48:01.399392
####################################################################################################
