####################################################################################################
Dotted Training Started at 2023-11-13 17:22:51.148467 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
ب س م <##> ا ل ل ه <##> ا ل ر ح م ن <##> ا ل ر ح ي م
ا ل ح م د <##> ل ل ه <##> ر ب <##> ا ل ع ا ل م ي ن
ا ل ر ح م ن <##> ا ل ر ح ي م
م ا ل ك <##> ي و م <##> ا ل د ي ن
ا ي ا ك <##> ن ع ب د <##> و ا ي ا ك <##> ن س ت ع ي ن
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-aggregated_dataset": {
        "2": {
            "perplexity_with_OOVs": 12.887640504875748,
            "perplexity_without_OOVs": 12.887640504875748,
            "counts_of_OOVs": "0",
            "ngram_counts": "1,083"
        },
        "3": {
            "perplexity_with_OOVs": 8.992896682928647,
            "perplexity_without_OOVs": 8.992896682928647,
            "counts_of_OOVs": "0",
            "ngram_counts": "25,708"
        },
        "4": {
            "perplexity_with_OOVs": 6.264578743760657,
            "perplexity_without_OOVs": 6.264578743760657,
            "counts_of_OOVs": "0",
            "ngram_counts": "304,653"
        },
        "5": {
            "perplexity_with_OOVs": 5.087524781489698,
            "perplexity_without_OOVs": 5.087524781489698,
            "counts_of_OOVs": "0",
            "ngram_counts": "2,197,953"
        },
        "6": {
            "perplexity_with_OOVs": 4.514692475905148,
            "perplexity_without_OOVs": 4.514692475905148,
            "counts_of_OOVs": "0",
            "ngram_counts": "11,710,643"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer CharacterTokenizer at 2023-11-13 17:46:44.302383
####################################################################################################
