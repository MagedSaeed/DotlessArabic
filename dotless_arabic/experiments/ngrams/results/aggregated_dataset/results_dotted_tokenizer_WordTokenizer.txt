####################################################################################################
Dotted Training Started at 2023-02-01 06:18:38.461383 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
بسم الله الرحمن الرحيم
الحمد لله رب العالمين
الرحمن الرحيم
مالك يوم الدين
اياك نعبد واياك نستعين
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-aggregated_dataset": {
        "2": {
            "perplexity_with_OOVs": 1013.0324079849804,
            "perplexity_without_OOVs": 969.1044676907322,
            "counts_of_OOVs": "96,245",
            "ngram_counts": "55,173,264"
        },
        "3": {
            "perplexity_with_OOVs": 458.07107340010816,
            "perplexity_without_OOVs": 437.01218590689786,
            "counts_of_OOVs": "96,245",
            "ngram_counts": "133,624,220"
        },
        "4": {
            "perplexity_with_OOVs": 367.3561974064997,
            "perplexity_without_OOVs": 350.2345351562233,
            "counts_of_OOVs": "96,245",
            "ngram_counts": "171,504,029"
        },
        "5": {
            "perplexity_with_OOVs": 348.373131184275,
            "perplexity_without_OOVs": 332.08707372830344,
            "counts_of_OOVs": "96,245",
            "ngram_counts": "181,523,416"
        },
        "6": {
            "perplexity_with_OOVs": 343.73080142347015,
            "perplexity_without_OOVs": 327.65040468533635,
            "counts_of_OOVs": "96,245",
            "ngram_counts": "181,052,741"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer WordTokenizer at 2023-02-01 07:05:33.277397
####################################################################################################
