####################################################################################################
Undotted Training Started at 2023-02-07 20:46:46.796560 for tokenizer: DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
يتوضا ثلاثا يرفعه الى النبي صلى الله عليه وسلم
يغتسل من الجنابة ثم يجيء وله قفقفة فيستدفى بي ولم اغتسل
لا تبل قاىما فما بلت بعد قاىما
انت نهيت الناس ان يصلوا في نعالهم فقال لا لعمر الله ما نهيت الناس ان يصلوا في نعالهم غير اني ورب هذه الحرمة حتى قالها ثلاثا لقد رايت النبي صلى الله عليه وسلم ههنا عند المقام يصلي وعليه نعلاه ثم انصرف وهما عليه
ان انا صدقت فصدقني وان انا كذبت فكذبني قال فافعل قال فانشدك بالله هل سمعت رسول الله صلى الله عليه وسلم ينهى عن لبس الذهب قال نعم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
ىٮو صا <##> ٮلا ٮا <##> ىر ڡعه <##> ا لى <##> ا لٮٮى <##> صلى <##> ا لله <##> علىه <##> و سلم
ىعٮسل <##> مں <##> ا لحٮا ٮه <##> ٮم <##> ىحىء <##> و له <##> ڡڡڡڡه <##> ڡىسٮد ڡى <##> ٮى <##> و لم <##> ا عٮسل
لا <##> ٮٮل <##> ڡا ىما <##> ڡما <##> ٮلٮ <##> ٮعد <##> ڡا ىما
ا ٮٮ <##> ٮهىٮ <##> ا لٮا س <##> ا ں <##> ىصلو ا <##> ڡى <##> ٮعا لهم <##> ڡڡا ل <##> لا <##> لعمر <##> ا لله <##> ما <##> ٮهىٮ <##> ا لٮا س <##> ا ں <##> ىصلو ا <##> ڡى <##> ٮعا لهم <##> عىر <##> ا ٮى <##> و ر ٮ <##> هد ه <##> ا لحر مه <##> حٮى <##> ڡا لها <##> ٮلا ٮا <##> لڡد <##> ر ا ىٮ <##> ا لٮٮى <##> صلى <##> ا لله <##> علىه <##> و سلم <##> ههٮا <##> عٮد <##> ا لمڡا م <##> ىصلى <##> و علىه <##> ٮعلا ه <##> ٮم <##> ا ٮصر ڡ <##> و هما <##> علىه
ا ں <##> ا ٮا <##> صد ڡٮ <##> ڡصد ڡٮى <##> و ا ں <##> ا ٮا <##> كد ٮٮ <##> ڡكد ٮٮى <##> ڡا ل <##> ڡا ڡعل <##> ڡا ل <##> ڡا ٮسد ك <##> ٮا لله <##> هل <##> سمعٮ <##> ر سو ل <##> ا لله <##> صلى <##> ا لله <##> علىه <##> و سلم <##> ىٮهى <##> عں <##> لٮس <##> ا لد هٮ <##> ڡا ل <##> ٮعم
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "undotted-sanadset_hadeeth_dataset": {
        "2": {
            "perplexity_with_OOVs": 16.36577886141889,
            "perplexity_without_OOVs": 16.321579173666095,
            "counts_of_OOVs": "1,466",
            "ngram_counts": "203,153"
        },
        "3": {
            "perplexity_with_OOVs": 10.338597933220994,
            "perplexity_without_OOVs": 10.30960717690001,
            "counts_of_OOVs": "1,466",
            "ngram_counts": "1,202,022"
        },
        "4": {
            "perplexity_with_OOVs": 7.279305784782492,
            "perplexity_without_OOVs": 7.258296938620603,
            "counts_of_OOVs": "1,466",
            "ngram_counts": "3,660,487"
        },
        "5": {
            "perplexity_with_OOVs": 5.618412672312421,
            "perplexity_without_OOVs": 5.601929789995481,
            "counts_of_OOVs": "1,466",
            "ngram_counts": "7,602,515"
        },
        "6": {
            "perplexity_with_OOVs": 4.48979197944507,
            "perplexity_without_OOVs": 4.476452583633432,
            "counts_of_OOVs": "1,466",
            "ngram_counts": "12,557,645"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer DisjointLetterTokenizer at 2023-02-07 20:54:22.021679
####################################################################################################
