####################################################################################################
Dotted Training Started at 2023-09-23 03:58:21.949089 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
يتوضا ثلاثا يرفعه الى النبي صلى الله عليه وسلم
يغتسل من الجنابة ثم يجيء وله قفقفة فيستدفى بي ولم اغتسل
لا تبل قاىما فما بلت بعد قاىما
انت نهيت الناس ان يصلوا في نعالهم فقال لا لعمر الله ما نهيت الناس ان يصلوا في نعالهم غير اني ورب هذه الحرمة حتى قالها ثلاثا لقد رايت النبي صلى الله عليه وسلم ههنا عند المقام يصلي وعليه نعلاه ثم انصرف وهما عليه
ان انا صدقت فصدقني وان انا كذبت فكذبني قال فافعل قال فانشدك بالله هل سمعت رسول الله صلى الله عليه وسلم ينهى عن لبس الذهب قال نعم
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
يتوضا ثلاثا يرفعه الى النبي صلى الله عليه وسلم
يغتسل من الجنابة ثم يجيء وله قفقفة فيستدفى بي ولم اغتسل
لا تبل قاىما فما بلت بعد قاىما
انت نهيت الناس ان يصلوا في نعالهم فقال لا لعمر الله ما نهيت الناس ان يصلوا في نعالهم غير اني ورب هذه الحرمة حتى قالها ثلاثا لقد رايت النبي صلى الله عليه وسلم ههنا عند المقام يصلي وعليه نعلاه ثم انصرف وهما عليه
ان انا صدقت فصدقني وان انا كذبت فكذبني قال فافعل قال فانشدك بالله هل سمعت رسول الله صلى الله عليه وسلم ينهى عن لبس الذهب قال نعم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
ي ت و ض ا <##> ث ل ا ث ا <##> ي ر ف ع ه <##> ا ل ى <##> ا ل ن ب ي <##> ص ل ى <##> ا ل ل ه <##> ع ل ي ه <##> و س ل م
ي غ ت س ل <##> م ن <##> ا ل ج ن ا ب ة <##> ث م <##> ي ج ي ء <##> و ل ه <##> ق ف ق ف ة <##> ف ي س ت د ف ى <##> ب ي <##> و ل م <##> ا غ ت س ل
ل ا <##> ت ب ل <##> ق ا ى م ا <##> ف م ا <##> ب ل ت <##> ب ع د <##> ق ا ى م ا
ا ن ت <##> ن ه ي ت <##> ا ل ن ا س <##> ا ن <##> ي ص ل و ا <##> ف ي <##> ن ع ا ل ه م <##> ف ق ا ل <##> ل ا <##> ل ع م ر <##> ا ل ل ه <##> م ا <##> ن ه ي ت <##> ا ل ن ا س <##> ا ن <##> ي ص ل و ا <##> ف ي <##> ن ع ا ل ه م <##> غ ي ر <##> ا ن ي <##> و ر ب <##> ه ذ ه <##> ا ل ح ر م ة <##> ح ت ى <##> ق ا ل ه ا <##> ث ل ا ث ا <##> ل ق د <##> ر ا ي ت <##> ا ل ن ب ي <##> ص ل ى <##> ا ل ل ه <##> ع ل ي ه <##> و س ل م <##> ه ه ن ا <##> ع ن د <##> ا ل م ق ا م <##> ي ص ل ي <##> و ع ل ي ه <##> ن ع ل ا ه <##> ث م <##> ا ن ص ر ف <##> و ه م ا <##> ع ل ي ه
ا ن <##> ا ن ا <##> ص د ق ت <##> ف ص د ق ن ي <##> و ا ن <##> ا ن ا <##> ك ذ ب ت <##> ف ك ذ ب ن ي <##> ق ا ل <##> ف ا ف ع ل <##> ق ا ل <##> ف ا ن ش د ك <##> ب ا ل ل ه <##> ه ل <##> س م ع ت <##> ر س و ل <##> ا ل ل ه <##> ص ل ى <##> ا ل ل ه <##> ع ل ي ه <##> و س ل م <##> ي ن ه ى <##> ع ن <##> ل ب س <##> ا ل ذ ه ب <##> ق ا ل <##> ن ع م
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-sanadset_hadeeth_dataset": {
        "2": {
            "perplexity_with_OOVs": 11.33272456621274,
            "perplexity_without_OOVs": 11.33272456621274,
            "counts_of_OOVs": "0",
            "ngram_counts": "953"
        },
        "3": {
            "perplexity_with_OOVs": 6.935828254438724,
            "perplexity_without_OOVs": 6.935828254438724,
            "counts_of_OOVs": "0",
            "ngram_counts": "17,021"
        },
        "4": {
            "perplexity_with_OOVs": 4.858868096726811,
            "perplexity_without_OOVs": 4.858868096726811,
            "counts_of_OOVs": "0",
            "ngram_counts": "154,417"
        },
        "5": {
            "perplexity_with_OOVs": 4.013919801607789,
            "perplexity_without_OOVs": 4.013919801607789,
            "counts_of_OOVs": "0",
            "ngram_counts": "823,674"
        },
        "6": {
            "perplexity_with_OOVs": 3.4841096478720153,
            "perplexity_without_OOVs": 3.4841096478720153,
            "counts_of_OOVs": "0",
            "ngram_counts": "2,892,015"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer CharacterTokenizer at 2023-09-23 04:01:24.548563
####################################################################################################
