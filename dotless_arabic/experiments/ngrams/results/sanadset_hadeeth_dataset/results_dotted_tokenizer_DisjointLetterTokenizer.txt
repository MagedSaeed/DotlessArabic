####################################################################################################
Dotted Training Started at 2023-02-07 20:43:02.384954 for tokenizer: DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
يتوضا ثلاثا يرفعه الى النبي صلى الله عليه وسلم
يغتسل من الجنابة ثم يجيء وله قفقفة فيستدفى بي ولم اغتسل
لا تبل قاىما فما بلت بعد قاىما
انت نهيت الناس ان يصلوا في نعالهم فقال لا لعمر الله ما نهيت الناس ان يصلوا في نعالهم غير اني ورب هذه الحرمة حتى قالها ثلاثا لقد رايت النبي صلى الله عليه وسلم ههنا عند المقام يصلي وعليه نعلاه ثم انصرف وهما عليه
ان انا صدقت فصدقني وان انا كذبت فكذبني قال فافعل قال فانشدك بالله هل سمعت رسول الله صلى الله عليه وسلم ينهى عن لبس الذهب قال نعم
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
يتوضا ثلاثا يرفعه الى النبي صلى الله عليه وسلم
يغتسل من الجنابة ثم يجيء وله قفقفة فيستدفى بي ولم اغتسل
لا تبل قاىما فما بلت بعد قاىما
انت نهيت الناس ان يصلوا في نعالهم فقال لا لعمر الله ما نهيت الناس ان يصلوا في نعالهم غير اني ورب هذه الحرمة حتى قالها ثلاثا لقد رايت النبي صلى الله عليه وسلم ههنا عند المقام يصلي وعليه نعلاه ثم انصرف وهما عليه
ان انا صدقت فصدقني وان انا كذبت فكذبني قال فافعل قال فانشدك بالله هل سمعت رسول الله صلى الله عليه وسلم ينهى عن لبس الذهب قال نعم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
يتو ضا <##> ثلا ثا <##> ير فعه <##> ا لى <##> ا لنبي <##> صلى <##> ا لله <##> عليه <##> و سلم
يغتسل <##> من <##> ا لجنا بة <##> ثم <##> يجيء <##> و له <##> قفقفة <##> فيستد فى <##> بي <##> و لم <##> ا غتسل
لا <##> تبل <##> قا ىما <##> فما <##> بلت <##> بعد <##> قا ىما
ا نت <##> نهيت <##> ا لنا س <##> ا ن <##> يصلو ا <##> في <##> نعا لهم <##> فقا ل <##> لا <##> لعمر <##> ا لله <##> ما <##> نهيت <##> ا لنا س <##> ا ن <##> يصلو ا <##> في <##> نعا لهم <##> غير <##> ا ني <##> و ر ب <##> هذ ه <##> ا لحر مة <##> حتى <##> قا لها <##> ثلا ثا <##> لقد <##> ر ا يت <##> ا لنبي <##> صلى <##> ا لله <##> عليه <##> و سلم <##> ههنا <##> عند <##> ا لمقا م <##> يصلي <##> و عليه <##> نعلا ه <##> ثم <##> ا نصر ف <##> و هما <##> عليه
ا ن <##> ا نا <##> صد قت <##> فصد قني <##> و ا ن <##> ا نا <##> كذ بت <##> فكذ بني <##> قا ل <##> فا فعل <##> قا ل <##> فا نشد ك <##> با لله <##> هل <##> سمعت <##> ر سو ل <##> ا لله <##> صلى <##> ا لله <##> عليه <##> و سلم <##> ينهى <##> عن <##> لبس <##> ا لذ هب <##> قا ل <##> نعم
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-sanadset_hadeeth_dataset": {
        "2": {
            "perplexity_with_OOVs": 17.255648379429317,
            "perplexity_without_OOVs": 17.17565027600944,
            "counts_of_OOVs": "2,534",
            "ngram_counts": "292,612"
        },
        "3": {
            "perplexity_with_OOVs": 10.281034687028917,
            "perplexity_without_OOVs": 10.231507744307297,
            "counts_of_OOVs": "2,534",
            "ngram_counts": "1,529,694"
        },
        "4": {
            "perplexity_with_OOVs": 7.084204446310088,
            "perplexity_without_OOVs": 7.049171623095085,
            "counts_of_OOVs": "2,534",
            "ngram_counts": "4,253,733"
        },
        "5": {
            "perplexity_with_OOVs": 5.495567923871847,
            "perplexity_without_OOVs": 5.467976227341797,
            "counts_of_OOVs": "2,534",
            "ngram_counts": "8,303,864"
        },
        "6": {
            "perplexity_with_OOVs": 4.429012955498946,
            "perplexity_without_OOVs": 4.406540239658131,
            "counts_of_OOVs": "2,534",
            "ngram_counts": "13,207,184"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer DisjointLetterTokenizer at 2023-02-07 20:46:46.796028
####################################################################################################
