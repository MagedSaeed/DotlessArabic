####################################################################################################
Undotted Training Started at 2023-03-29 07:17:48.263067 for tokenizer: DisjointLetterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم
غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم
مرسل المصطفى البشير الينا رحمة منه بالكلام القديم
ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم
واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
ا صٮح <##> ا لملك <##> للد ى <##> ڡطر <##> ا لحل <##> ٯ <##> ٮٮڡد ٮر <##> للعر ٮر <##> ا لعلٮم
عا ڡر <##> ا لد ٮٮ <##> للمسٮء <##> ٮعڡو <##> ڡا ٮل <##> ا لٮو ٮ <##> د ى <##> ا لعطا ء <##> ا لعمٮم
مر سل <##> ا لمصطڡى <##> ا لٮسٮر <##> ا لٮٮا <##> ر حمه <##> مٮه <##> ٮا لكلا م <##> ا لڡد ٮم
ر ٮٮا <##> ر ٮٮا <##> ا لٮك <##> ا ٮٮٮا <##> ڡا حر ٮا <##> مں <##> حر <##> ٮا ر <##> ا لححٮم
و ا كڡٮا <##> سر <##> ما <##> ٮحا ڡ <##> ٮلطڡ <##> ٮا <##> عطٮما <##> ٮر حى <##> لكل <##> عطٮم
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "undotted-poems_dataset": {
        "2": {
            "perplexity_with_OOVs": 30.25083155819769,
            "perplexity_without_OOVs": 30.138218547902543,
            "counts_of_OOVs": "2,336",
            "ngram_counts": "391,365"
        },
        "3": {
            "perplexity_with_OOVs": 23.218058035349483,
            "perplexity_without_OOVs": 23.12884824518047,
            "counts_of_OOVs": "2,336",
            "ngram_counts": "2,882,992"
        },
        "4": {
            "perplexity_with_OOVs": 19.698377814151968,
            "perplexity_without_OOVs": 19.621246844615765,
            "counts_of_OOVs": "2,336",
            "ngram_counts": "10,104,509"
        },
        "5": {
            "perplexity_with_OOVs": 16.930965996048624,
            "perplexity_without_OOVs": 16.863656143194827,
            "counts_of_OOVs": "2,336",
            "ngram_counts": "22,057,647"
        },
        "6": {
            "perplexity_with_OOVs": 14.63377056331812,
            "perplexity_without_OOVs": 14.574906460296503,
            "counts_of_OOVs": "2,336",
            "ngram_counts": "34,973,924"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer DisjointLetterTokenizer at 2023-03-29 07:28:29.891956
####################################################################################################
