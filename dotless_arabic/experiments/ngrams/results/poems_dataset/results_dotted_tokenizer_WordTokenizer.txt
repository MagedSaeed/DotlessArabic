####################################################################################################
Dotted Training Started at 2023-03-16 04:12:57.415061 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before training:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم
غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم
مرسل المصطفى البشير الينا رحمة منه بالكلام القديم
ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم
واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم
غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم
مرسل المصطفى البشير الينا رحمة منه بالكلام القديم
ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم
واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم
غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم
مرسل المصطفى البشير الينا رحمة منه بالكلام القديم
ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم
واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "dotted-poems_dataset": {
        "2": {
            "perplexity_with_OOVs": 2763.0762999052777,
            "perplexity_without_OOVs": 2465.1284109896924,
            "counts_of_OOVs": "40,293",
            "ngram_counts": "12,776,724"
        },
        "3": {
            "perplexity_with_OOVs": 1126.7310220362615,
            "perplexity_without_OOVs": 994.5121966738823,
            "counts_of_OOVs": "40,293",
            "ngram_counts": "20,734,696"
        },
        "4": {
            "perplexity_with_OOVs": 899.0314555872028,
            "perplexity_without_OOVs": 791.8025725406964,
            "counts_of_OOVs": "40,293",
            "ngram_counts": "20,607,625"
        },
        "5": {
            "perplexity_with_OOVs": 885.5097723285319,
            "perplexity_without_OOVs": 779.8414146323133,
            "counts_of_OOVs": "40,293",
            "ngram_counts": "18,619,534"
        },
        "6": {
            "perplexity_with_OOVs": 889.0272493583656,
            "perplexity_without_OOVs": 782.9920255840763,
            "counts_of_OOVs": "40,293",
            "ngram_counts": "16,311,361"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer WordTokenizer at 2023-03-16 04:19:12.930634
####################################################################################################
