####################################################################################################
Undotted Training Started at 2023-02-01 12:22:02.309807 for tokenizer: CharacterTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples after undotting:
ا ##ص ##ٮ ##ح ا ##ل ##م ##ل ##ك ل ##ل ##د ##ى ڡ ##ط ##ر ا ##ل ##ح ##ل ٯ ٮ ##ٮ ##ٯ ##د ##ى ##ر ل ##ل ##ع ##ر ##ى ##ر ا ##ل ##ع ##ل ##ى ##م
ع ##ا ##ڡ ##ر ا ##ل ##د ##ں ##ٮ ل ##ل ##م ##س ##ى ##ء ٮ ##ع ##ڡ ##و ٯ ##ا ##ٮ ##ل ا ##ل ##ٮ ##و ##ٮ د ##ى ا ##ل ##ع ##ط ##ا ##ء ا ##ل ##ع ##م ##ى ##م
م ##ر ##س ##ل ا ##ل ##م ##ص ##ط ##ڡ ##ى ا ##ل ##ٮ ##س ##ى ##ر ا ##ل ##ى ##ں ##ا ر ##ح ##م ##ه م ##ں ##ه ٮ ##ا ##ل ##ك ##ل ##ا ##م ا ##ل ##ٯ ##د ##ى ##م
ر ##ٮ ##ں ##ا ر ##ٮ ##ں ##ا ا ##ل ##ى ##ك ا ##ں ##ى ##ں ##ا ڡ ##ا ##ح ##ر ##ں ##ا م ##ں ح ##ر ں ##ا ##ر ا ##ل ##ح ##ح ##ى ##م
و ##ا ##ك ##ڡ ##ں ##ا س ##ر م ##ا ں ##ح ##ا ##ڡ ٮ ##ل ##ط ##ڡ ى ##ا ع ##ط ##ى ##م ##ا ى ##ر ##ح ##ى ل ##ك ##ل ع ##ط ##ى ##م
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "undotted-poems_dataset": {
        "2": {
            "perplexity_with_OOVs": 18.124237949937797,
            "perplexity_without_OOVs": 18.124237949937797,
            "counts_of_OOVs": "0",
            "ngram_counts": "1,465"
        },
        "3": {
            "perplexity_with_OOVs": 14.028494670783585,
            "perplexity_without_OOVs": 14.028494670783585,
            "counts_of_OOVs": "0",
            "ngram_counts": "39,920"
        },
        "4": {
            "perplexity_with_OOVs": 11.19761059544063,
            "perplexity_without_OOVs": 11.19761059544063,
            "counts_of_OOVs": "0",
            "ngram_counts": "592,417"
        },
        "5": {
            "perplexity_with_OOVs": 9.627292337498737,
            "perplexity_without_OOVs": 9.627292337498737,
            "counts_of_OOVs": "0",
            "ngram_counts": "4,644,089"
        },
        "6": {
            "perplexity_with_OOVs": 8.751250965112563,
            "perplexity_without_OOVs": 8.751250965112563,
            "counts_of_OOVs": "0",
            "ngram_counts": "17,922,766"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer CharacterTokenizer at 2023-02-01 12:32:05.813913
####################################################################################################
