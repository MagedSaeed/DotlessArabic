####################################################################################################
Undotted Training Started at 2023-03-29 03:46:45.796994 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Some of the Dataset Samples before tokenization:
اصبح الملك للذي فطر الخل ق بتقدير للعزيز العليم
غافر الذنب للمسيء بعفو قابل التوب ذي العطاء العميم
مرسل المصطفى البشير الينا رحمة منه بالكلام القديم
ربنا ربنا اليك انينا فاجرنا من حر نار الجحيم
واكفنا شر ما نخاف بلطف يا عظيما يرجى لكل عظيم
####################################################################################################
####################################################################################################
Tokenize the dataset
####################################################################################################
####################################################################################################
Some of the Dataset Samples after tokenization:
اصٮح الملك للدى ڡطر الحل ٯ ٮٮڡدٮر للعرٮر العلٮم
عاڡر الدٮٮ للمسٮء ٮعڡو ڡاٮل الٮوٮ دى العطاء العمٮم
مرسل المصطڡى الٮسٮر الٮٮا رحمه مٮه ٮالكلام الڡدٮم
رٮٮا رٮٮا الٮك اٮٮٮا ڡاحرٮا مں حر ٮار الححٮم
واكڡٮا سر ما ٮحاڡ ٮلطڡ ٮا عطٮما ٮرحى لكل عطٮم
####################################################################################################
####################################################################################################
TRAINING STARTED
####################################################################################################
####################################################################################################
{
    "undotted-poems_dataset": {
        "2": {
            "perplexity_with_OOVs": 2271.394242574899,
            "perplexity_without_OOVs": 2125.46719462955,
            "counts_of_OOVs": "23,040",
            "ngram_counts": "11,808,956"
        },
        "3": {
            "perplexity_with_OOVs": 904.9645509005761,
            "perplexity_without_OOVs": 841.4591286277156,
            "counts_of_OOVs": "23,040",
            "ngram_counts": "20,462,692"
        },
        "4": {
            "perplexity_with_OOVs": 701.4335301743179,
            "perplexity_without_OOVs": 651.2819541596753,
            "counts_of_OOVs": "23,040",
            "ngram_counts": "20,524,775"
        },
        "5": {
            "perplexity_with_OOVs": 688.8734360554413,
            "perplexity_without_OOVs": 639.5869361435612,
            "counts_of_OOVs": "23,040",
            "ngram_counts": "18,554,803"
        },
        "6": {
            "perplexity_with_OOVs": 691.499609129193,
            "perplexity_without_OOVs": 642.0488554961778,
            "counts_of_OOVs": "23,040",
            "ngram_counts": "16,251,493"
        }
    }
}
####################################################################################################
####################################################################################################
TRAINING FINISHED
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer WordTokenizer at 2023-03-29 03:53:53.318586
####################################################################################################
