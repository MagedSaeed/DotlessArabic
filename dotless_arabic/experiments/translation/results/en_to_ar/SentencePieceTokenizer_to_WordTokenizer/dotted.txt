####################################################################################################
Dotted Training Started at 2023-11-03 18:09:45.964554 for tokenizer: SentencePieceTokenizer
####################################################################################################
####################################################################################################
Collecting dataset splits:
####################################################################################################
####################################################################################################
Processing source and target sequences:
####################################################################################################
####################################################################################################
Segmenting texts with moses:
####################################################################################################
####################################################################################################
Building source and target tokenizers
####################################################################################################
####################################################################################################
Source vocab size: 4000
Target vocab size: 92659
####################################################################################################
####################################################################################################
Calculate sequence length:
####################################################################################################
####################################################################################################
Source Max Doc Length: 80
Target Max Doc Length: 48
####################################################################################################
####################################################################################################
Sequence Length: 80
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 3,621
Val DataLoader: 14
Test DataLoader: 19
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | transformer         | Transformer        | 44.1 M
1 | src_tok_emb         | TokenEmbedding     | 2.0 M
2 | tgt_tok_emb         | TokenEmbedding     | 47.4 M
3 | positional_encoding | PositionalEncoding | 0
4 | dense               | Linear             | 47.5 M
-----------------------------------------------------------
141 M     Trainable params
0         Non-trainable params
141 M     Total params
564.656   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 22226.61 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 1587.614 seconds
####################################################################################################
####################################################################################################
Losses: [
{
"test_loss/dataloader_idx_0": 3.209751844406128
},
{
"test_loss/dataloader_idx_1": 4.257646560668945
},
{
"test_loss/dataloader_idx_2": 4.372974395751953
}
]
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode): 9.496
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode): 6.2
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer SentencePieceTokenizer at 2023-11-04 01:24:24.728131
####################################################################################################
