####################################################################################################
Dotted Training Started at 2024-03-11 18:46:16.755985 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Collecting dataset splits:
####################################################################################################
####################################################################################################
Processing source and target sequences:
####################################################################################################
####################################################################################################
Building source and target tokenizers
####################################################################################################
####################################################################################################
Source vocab size: 39377
Target vocab size: 97387
####################################################################################################
####################################################################################################
Calculate sequence length:
####################################################################################################
####################################################################################################
Source Max Doc Length: 70
Target Max Doc Length: 56
####################################################################################################
####################################################################################################
Sequence Length: 70
####################################################################################################
####################################################################################################
Calculating Batch Size as sequence_length/4_000
####################################################################################################
####################################################################################################
Batch size: 57
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 4,066
Val DataLoader: 16
Test DataLoader: 22
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | train_ppl           | Perplexity         | 0
1 | val_ppl             | Perplexity         | 0
2 | test_ppl            | Perplexity         | 0
3 | transformer         | Transformer        | 44.1 M
4 | src_tok_emb         | TokenEmbedding     | 20.2 M
5 | tgt_tok_emb         | TokenEmbedding     | 49.9 M
6 | positional_encoding | PositionalEncoding | 0
7 | dense               | Linear             | 50.0 M
-----------------------------------------------------------
164 M     Trainable params
0         Non-trainable params
164 M     Total params
656.493   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 19753.76 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 1519.519 seconds
####################################################################################################
####################################################################################################
Losses: [
{
"test_ppl/dataloader_idx_0": 24.866228103637695,
"test_loss/dataloader_idx_0": 4.266421318054199
},
{
"test_ppl/dataloader_idx_1": 41.313804626464844,
"test_loss/dataloader_idx_1": 4.629452705383301
},
{
"test_ppl/dataloader_idx_2": 77.53272247314453,
"test_loss/dataloader_idx_2": 4.780221939086914
}
]
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for best model: 11.506
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for averaged model: 11.835
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode): 11.816
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode) for averaged model: 12.568
####################################################################################################
####################################################################################################
Dotted Training Finished for tokenizer WordTokenizer at 2024-03-12 01:37:52.509861
####################################################################################################
