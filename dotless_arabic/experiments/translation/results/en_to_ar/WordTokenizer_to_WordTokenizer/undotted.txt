####################################################################################################
Undotted Training Started at 2024-03-12 01:37:52.510121 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Collecting dataset splits:
####################################################################################################
####################################################################################################
Processing source and target sequences:
####################################################################################################
####################################################################################################
Undot Arabic text
####################################################################################################
####################################################################################################
Building source and target tokenizers
####################################################################################################
####################################################################################################
Source vocab size: 39377
Target vocab size: 81272
####################################################################################################
####################################################################################################
Calculate sequence length:
####################################################################################################
####################################################################################################
Source Max Doc Length: 70
Target Max Doc Length: 56
####################################################################################################
####################################################################################################
Sequence Length: 70
####################################################################################################
####################################################################################################
Calculating Batch Size as sequence_length/4_000
####################################################################################################
####################################################################################################
Batch size: 57
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 4,066
Val DataLoader: 16
Test DataLoader: 22
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | train_ppl           | Perplexity         | 0
1 | val_ppl             | Perplexity         | 0
2 | test_ppl            | Perplexity         | 0
3 | transformer         | Transformer        | 44.1 M
4 | src_tok_emb         | TokenEmbedding     | 20.2 M
5 | tgt_tok_emb         | TokenEmbedding     | 41.6 M
6 | positional_encoding | PositionalEncoding | 0
7 | dense               | Linear             | 41.7 M
-----------------------------------------------------------
147 M     Trainable params
0         Non-trainable params
147 M     Total params
590.421   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 22584.05 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 1411.502 seconds
####################################################################################################
####################################################################################################
Losses: [
{
"test_ppl/dataloader_idx_0": 15.89028263092041,
"test_loss/dataloader_idx_0": 3.8586878776550293
},
{
"test_ppl/dataloader_idx_1": 37.1846809387207,
"test_loss/dataloader_idx_1": 4.512443542480469
},
{
"test_ppl/dataloader_idx_2": 81.46480560302734,
"test_loss/dataloader_idx_2": 4.6838812828063965
}
]
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for best model: 13.328
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for averaged model: 12.999
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode): 13.551
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode) for averaged model: 13.322
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (greedy-decode) for best model: 12.242
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (greedy-decode) for averaged model: 11.988
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (beam-search-decode) for best model: 12.484
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (beam-search-decode) for averaged model: 12.313
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer WordTokenizer at 2024-03-12 10:23:18.084581
####################################################################################################
