####################################################################################################
Undotted Training Started at 2024-03-10 16:18:28.949561 for tokenizer: SentencePieceTokenizer
####################################################################################################
####################################################################################################
Collecting dataset splits:
####################################################################################################
####################################################################################################
Processing source and target sequences:
####################################################################################################
####################################################################################################
Undot Arabic text
####################################################################################################
####################################################################################################
Building source and target tokenizers
####################################################################################################
####################################################################################################
Source vocab size: 4000
Target vocab size: 4000
####################################################################################################
####################################################################################################
Calculate sequence length:
####################################################################################################
####################################################################################################
Source Max Doc Length: 91
Target Max Doc Length: 81
####################################################################################################
####################################################################################################
Sequence Length: 91
####################################################################################################
####################################################################################################
Calculating Batch Size as sequence_length/4_000
####################################################################################################
####################################################################################################
Batch size: 43
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 5,389
Val DataLoader: 21
Test DataLoader: 29
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | train_ppl           | Perplexity         | 0
1 | val_ppl             | Perplexity         | 0
2 | test_ppl            | Perplexity         | 0
3 | transformer         | Transformer        | 44.1 M
4 | src_tok_emb         | TokenEmbedding     | 2.0 M
5 | tgt_tok_emb         | TokenEmbedding     | 2.0 M
6 | positional_encoding | PositionalEncoding | 0
7 | dense               | Linear             | 2.1 M
-----------------------------------------------------------
50.3 M    Trainable params
0         Non-trainable params
50.3 M    Total params
201.154   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 35794.03 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 1193.133 seconds
####################################################################################################
####################################################################################################
Losses: [
{
"test_ppl/dataloader_idx_0": 3.4284913539886475,
"test_loss/dataloader_idx_0": 2.2779293060302734
},
{
"test_ppl/dataloader_idx_1": 9.703343391418457,
"test_loss/dataloader_idx_1": 3.1333367824554443
},
{
"test_ppl/dataloader_idx_2": 14.662611961364746,
"test_loss/dataloader_idx_2": 3.219675064086914
}
]
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for best model: 17.075
####################################################################################################
####################################################################################################
Test sacre blue score (greedy decode) for averaged model: 16.992
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode): 17.114
####################################################################################################
####################################################################################################
Test sacre blue score (beam search decode) for averaged model: 17.346
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (greedy-decode) for best model: 15.57
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (greedy-decode) for averaged model: 15.538
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (beam-search-decode) for best model: 15.663
####################################################################################################
####################################################################################################
Test sacre blue score after dotting predictions (beam-search-decode) for averaged model: 16.014
####################################################################################################
####################################################################################################
Undotted Training Finished for tokenizer SentencePieceTokenizer at 2024-03-11 05:20:19.893248
####################################################################################################
