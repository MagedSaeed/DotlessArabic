####################################################################################################
Dotted Training Started at 2023-09-30 18:21:52.890561 for tokenizer: WordTokenizer
####################################################################################################
####################################################################################################
Processing source and target sequences:
####################################################################################################
####################################################################################################
Calculate sequence length:
####################################################################################################
####################################################################################################
Source Max Doc Length: 62
Target Max Doc Length: 52
setting the sequence length to be the avg of the two
####################################################################################################
####################################################################################################
Sequence Length: 57
####################################################################################################
####################################################################################################
Getting source and target tokenizers
####################################################################################################
####################################################################################################
Source vocab size: 35523
Target vocab size: 51315
####################################################################################################
####################################################################################################
Building DataLoaders
####################################################################################################
####################################################################################################
Train DataLoader: 1,409
Val DataLoader: 157
Test DataLoader: 155
####################################################################################################
####################################################################################################
| Name                | Type               | Params
-----------------------------------------------------------
0 | transformer         | Transformer        | 5.8 M
1 | src_tok_emb         | TokenEmbedding     | 9.1 M
2 | tgt_tok_emb         | TokenEmbedding     | 13.1 M
3 | positional_encoding | PositionalEncoding | 0
4 | dense               | Linear             | 13.2 M
-----------------------------------------------------------
41.2 M    Trainable params
0         Non-trainable params
41.2 M    Total params
164.829   Total estimated model params size (MB)
####################################################################################################
####################################################################################################
Training Time: 0.00 seconds
####################################################################################################
####################################################################################################
Average training Time for one epoch: 0.000 seconds
####################################################################################################
####################################################################################################
Losses: [
{
"test_loss/dataloader_idx_0": 3.5936460494995117
},
{
"test_loss/dataloader_idx_1": 5.272866249084473
},
{
"test_loss/dataloader_idx_2": 5.60308313369751
}
]
####################################################################################################
