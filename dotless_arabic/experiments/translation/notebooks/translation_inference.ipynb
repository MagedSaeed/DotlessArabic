{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from pytorch_lightning import seed_everything\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-08 15:53:36,537 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    }
   ],
   "source": [
    "from dotless_arabic.experiments.translation.src.settings import (\n",
    "    configure_environment,\n",
    ")\n",
    "from dotless_arabic.tokenizers import SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotless_arabic.experiments.translation.src.processing import (\n",
    "    process_en,\n",
    "    process_ar,\n",
    ")\n",
    "from dotless_arabic.experiments.translation.src.utils import get_source_tokenizer, get_target_tokenizer\n",
    "from dotless_arabic.callbacks import EpochTimerCallback\n",
    "from dotless_arabic.experiments.translation.src.datasets import get_dataloader\n",
    "from dotless_arabic.experiments.translation.src.models import TranslationTransformer\n",
    "from dotless_arabic.experiments.translation.src.utils import get_best_checkpoint, get_sequence_length, train_translator\n",
    "from dotless_arabic.experiments.translation.src.utils import get_blue_score\n",
    "from dotless_arabic.experiments.translation.src import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/majed_alshaibani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "torch.cuda.empty_cache()  # to free gpu memory\n",
    "nltk.download(\"stopwords\")\n",
    "seed_everything(seed, workers=True)\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # to see CUDA errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotless_arabic.datasets.iwslt2017.collect import (\n",
    "    collect_parallel_train_dataset_for_translation,\n",
    "    collect_parallel_val_dataset_for_translation,\n",
    "    collect_parallel_test_dataset_for_translation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = collect_parallel_train_dataset_for_translation()\n",
    "val_dataset = collect_parallel_val_dataset_for_translation()\n",
    "test_dataset = collect_parallel_test_dataset_for_translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.to_pandas()\n",
    "val_dataset = val_dataset.to_pandas()\n",
    "test_dataset = test_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d48104393d476f8093c0959ac8b034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c872811425da4e13ba887404428b3b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfeb64f3f8b4a768fed703e588f577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4607c1e2d44df08cbf37fc52808485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31acbe6177ff45d284bcc8d8e84d8c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0db6cea2b74879b7f9da37b7fd89ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset[\"ar\"] = train_dataset[\"ar\"].progress_map(lambda text: process_ar(text))\n",
    "val_dataset[\"ar\"] = val_dataset[\"ar\"].progress_map(lambda text: process_ar(text))\n",
    "test_dataset[\"ar\"] = test_dataset[\"ar\"].progress_map(lambda text: process_ar(text))\n",
    "\n",
    "train_dataset[\"en\"] = train_dataset[\"en\"].progress_map(lambda text: process_en(text))\n",
    "val_dataset[\"en\"] = val_dataset[\"en\"].progress_map(lambda text: process_en(text))\n",
    "test_dataset[\"en\"] = test_dataset[\"en\"].progress_map(lambda text: process_en(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f57f8b4394b228315ddada1f8f55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6b1812df0146ec9893c19934480ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa12d9a1264cb99db34bbcc054e5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7004eb7aef4045cc935bd0625a130c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b81fe381a9f4b8c8483e1f36ca983c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7334e9eab4e45e1bdbd76f993483ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moses_tokenizer = MosesTokenizer()\n",
    "train_dataset[\"en\"] = train_dataset[\"en\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")\n",
    "val_dataset[\"en\"] = val_dataset[\"en\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")\n",
    "test_dataset[\"en\"] = test_dataset[\"en\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")\n",
    "train_dataset[\"ar\"] = train_dataset[\"ar\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")\n",
    "val_dataset[\"ar\"] = val_dataset[\"ar\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")\n",
    "test_dataset[\"ar\"] = test_dataset[\"ar\"].progress_map(\n",
    "    lambda text: moses_tokenizer.tokenize(\n",
    "        text,\n",
    "        return_str=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokenizer_class = SentencePieceTokenizer\n",
    "target_tokenizer_class = SentencePieceTokenizer\n",
    "source_language_code = \"ar\"\n",
    "target_language_code = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae12cd2d8b14591b1974237151a0947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6b3f2ee46042619f938980e86c9bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece ...\n"
     ]
    }
   ],
   "source": [
    "source_tokenizer = get_source_tokenizer(\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer_class=source_tokenizer_class,\n",
    "    source_language_code=source_language_code,\n",
    "    undot_text=False,\n",
    ")\n",
    "target_tokenizer = get_target_tokenizer(\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer_class=target_tokenizer_class,\n",
    "    target_language_code=target_language_code,\n",
    "    undot_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecf7acaf2c241a0b3ea378984c9836b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Experiments/DotlessArabic/dotless_arabic/tokenizers.py:324: UserWarning: sentencepiece tokenizer cannot split text unless with PBE mode. It needs to be trained first!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fafb60601dd4a01ac22facb1a403858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbeeffc8d184f5b8d94f0cba970dac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a073e5ec64284d079430f3db1bcf47a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/231713 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_max_sequence_length = get_sequence_length(\n",
    "    dataset=list(\n",
    "        map(\n",
    "            source_tokenizer.split_text,\n",
    "            tqdm(train_dataset[source_language_code]),\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "target_max_sequence_length = get_sequence_length(\n",
    "    dataset=list(\n",
    "        map(\n",
    "            target_tokenizer.split_text,\n",
    "            tqdm(train_dataset[target_language_code]),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sequence_length = max(source_max_sequence_length, target_max_sequence_length)\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best ckpt: NMT/ar_to_en/SentencePieceTokenizer_to_SentencePieceTokenizer/dotted/checkpoints/epoch=22-val_loss=2.599-step=123947.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TranslationTransformer(\n",
       "  (train_ppl): Perplexity()\n",
       "  (val_ppl): Perplexity()\n",
       "  (test_ppl): Perplexity()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(4000, 512, padding_idx=1)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(4000, 512, padding_idx=1)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dense): Linear(in_features=512, out_features=4000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TranslationTransformer.load_from_checkpoint(\n",
    "        get_best_checkpoint(\n",
    "            is_dotted=True,\n",
    "            source_language_code=source_language_code,\n",
    "            target_language_code=target_language_code,\n",
    "            source_tokenizer_class=source_tokenizer_class,\n",
    "            target_tokenizer_class=target_tokenizer_class,\n",
    "        )\n",
    "    ).to(constants.DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d71b46e1a74e58b9d0c49795558abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6659e5a572542ed9d1bb09026f5ae0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: سأحدثكم اليوم عن 30 عاما من تاريخ الهندسة.\n",
      "Prediction: i'm going to talk to you today about 30 years of engineering history.\n",
      "Target: today i'm going to speak to you about the last 30 years of architectural history.\n",
      "********************************************************************************\n",
      "Source: هذا أمر كبير جدا لألخصه في 18 دقيقة.\n",
      "Prediction: this is a very big thing to sum up in 18 minutes.\n",
      "Target: that's a lot to pack into 18 minutes.\n",
      "********************************************************************************\n",
      "Source: إنه موضوع معقد ، لذلك فإننا سنتوجه مباشرة إلى مكان معقد: إلى نيو جيرسي ،\n",
      "Prediction: it's a complicated topic, so we're going to go straight to a complicated place: to new jersey.\n",
      "Target: it's a complex topic, so we're just going to dive right in at a complex place: new jersey.\n",
      "********************************************************************************\n",
      "Source: لأنه منذ 30 سنة ، أنا من نيوجيرسي ، كنت في السادسة من عمري ، وكنت أعيش هناك مع والدي في مدينة تدعى ليفينغستون ، وكانت هذه غرفة نومي.\n",
      "Prediction: because 30 years ago, i was from new jersey, i was six, and i was living there with my father in a city called levinestone, and this was a bedroom.\n",
      "Target: because 30 years ago, i'm from jersey, and i was six, and i lived there in my parents' house in a town called livingston, and this was my childhood bedroom.\n",
      "********************************************************************************\n",
      "Source: عند زاوية غرفتي كان هناك حمام أتقاسمه مع أختي.\n",
      "Prediction: at my corner of my room, there was a bathroom that i shared with my sister.\n",
      "Target: around the corner from my bedroom was the bathroom that i used to share with my sister.\n",
      "********************************************************************************\n",
      "sacre bleu 29.261\n",
      "sacre bleu signature: nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.261"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blue_score(\n",
    "            model=model,\n",
    "            is_dotted=True,\n",
    "            show_translations_for=5,\n",
    "            decode_with_beam_search=False,\n",
    "            source_tokenizer=source_tokenizer,\n",
    "            target_tokenizer=target_tokenizer,\n",
    "            save_predictions_and_targets=False,\n",
    "            max_sequence_length=sequence_length,\n",
    "            source_language_code=source_language_code,\n",
    "            target_language_code=target_language_code,\n",
    "            source_sentences=test_dataset[source_language_code],\n",
    "            target_sentences=test_dataset[target_language_code],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e15316819f4c65b6b8369ea35b006b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c0bc953fd84996828a781016ccf3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacre bleu 29.285\n",
      "sacre bleu signature: nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blue_score(\n",
    "            model=model,\n",
    "            is_dotted=True,\n",
    "            show_translations_for=0,\n",
    "            decode_with_beam_search=True,\n",
    "            source_tokenizer=source_tokenizer,\n",
    "            target_tokenizer=target_tokenizer,\n",
    "            save_predictions_and_targets=False,\n",
    "            max_sequence_length=sequence_length,\n",
    "            source_language_code=source_language_code,\n",
    "            target_language_code=target_language_code,\n",
    "            source_sentences=test_dataset[source_language_code],\n",
    "            target_sentences=test_dataset[target_language_code],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "#     get_blue_score(\n",
    "#             model=model,\n",
    "#             is_dotted=True,\n",
    "#             show_translations_for=0,\n",
    "#             decode_with_beam_search=False,\n",
    "#             source_tokenizer=source_tokenizer,\n",
    "#             target_tokenizer=target_tokenizer,\n",
    "#             max_sequence_length=sequence_length,\n",
    "#             source_language_code=source_language_code,\n",
    "#             target_language_code=target_language_code,\n",
    "#             source_sentences=test_dataset[source_language_code][:100],\n",
    "#             target_sentences=test_dataset[target_language_code][:100],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(5):\n",
    "#     get_blue_score(\n",
    "#             model=model,\n",
    "#             is_dotted=True,\n",
    "#             show_translations_for=0,\n",
    "#             decode_with_beam_search=True,\n",
    "#             source_tokenizer=source_tokenizer,\n",
    "#             target_tokenizer=target_tokenizer,\n",
    "#             max_sequence_length=sequence_length,\n",
    "#             source_language_code=source_language_code,\n",
    "#             target_language_code=target_language_code,\n",
    "#             source_sentences=test_dataset[source_language_code][:100],\n",
    "#             target_sentences=test_dataset[target_language_code][:100],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_checkpoints(checkpoint_paths):\n",
    "    # Load checkpoints and extract model weights\n",
    "    loaded_checkpoints = [torch.load(path) for path in checkpoint_paths]\n",
    "    model_weights = [checkpoint['state_dict'] for checkpoint in loaded_checkpoints]\n",
    "\n",
    "    # Average weights\n",
    "    avg_weights = {}\n",
    "    num_checkpoints = len(checkpoint_paths)\n",
    "    for key in model_weights[0].keys():\n",
    "        avg_weights[key] = sum([model_weights[i][key] for i in range(num_checkpoints)]) / num_checkpoints\n",
    "\n",
    "    # Create a new model with averaged weights\n",
    "    averaged_model = TranslationTransformer(\n",
    "        src_vocab_size=source_tokenizer.vocab_size,\n",
    "        tgt_vocab_size=target_tokenizer.vocab_size,\n",
    "        pad_token_id=source_tokenizer.token_to_id(source_tokenizer.pad_token),\n",
    "    )\n",
    "    averaged_model.load_state_dict(avg_weights)\n",
    "    averaged_model = averaged_model.to(constants.DEVICE)\n",
    "\n",
    "    return averaged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_dir = 'NMT/ar_to_en/SentencePieceTokenizer_to_SentencePieceTokenizer/dotted/checkpoints'\n",
    "# sorted_models_paths = []\n",
    "# for filename in os.listdir(ckpt_dir):\n",
    "#     if filename.startswith('epoch'):\n",
    "#         sorted_models_paths.append(filename)\n",
    "# sorted_models_paths = sorted(\n",
    "#     sorted_models_paths,\n",
    "#     key=lambda filename:''.join(c for c in filename.split('=')[2] if c.isdigit() or c=='.')\n",
    "# )\n",
    "# sorted_models_paths = list(map(lambda filename:f'{ckpt_dir}/{filename}',sorted_models_paths))\n",
    "# sorted_models_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# averaged_model = TranslationTransformer(\n",
    "#     src_vocab_size=source_tokenizer.vocab_size,\n",
    "#     tgt_vocab_size=target_tokenizer.vocab_size,\n",
    "#     pad_token_id=source_tokenizer.token_to_id(source_tokenizer.pad_token),\n",
    "# ).to(constants.DEVICE)\n",
    "# models = []\n",
    "# for model_path in sorted_models_paths:\n",
    "#     models.append(\n",
    "#         TranslationTransformer(\n",
    "#     src_vocab_size=source_tokenizer.vocab_size,\n",
    "#     tgt_vocab_size=target_tokenizer.vocab_size,\n",
    "#     pad_token_id=source_tokenizer.token_to_id(source_tokenizer.pad_token),\n",
    "# ).load_from_checkpoint(model_path).to(constants.DEVICE)\n",
    "#     )\n",
    "# for ps in zip(*[m.parameters() for m in [averaged_model] + models]):\n",
    "#     ps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged_model = TranslationTransformer(\n",
    "#         src_vocab_size=source_tokenizer.vocab_size,\n",
    "#         tgt_vocab_size=target_tokenizer.vocab_size,\n",
    "#         pad_token_id=source_tokenizer.token_to_id(source_tokenizer.pad_token),\n",
    "#     ).to(constants.DEVICE)\n",
    "# averaged_model_state_dict = averaged_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filepath in sorted_models_paths:\n",
    "#     print(f'averaging {filepath}')\n",
    "#     tmp_model = TranslationTransformer.load_from_checkpoint(\n",
    "#         filepath\n",
    "#     ).to(constants.DEVICE)\n",
    "#     tmp_model_state_dict = tmp_model.state_dict()\n",
    "#     for key in averaged_model_state_dict:\n",
    "#         averaged_model_state_dict[key] = (tmp_model_state_dict[key]+averaged_model_state_dict[key])/2\n",
    "\n",
    "# averaged_model.load_state_dict(averaged_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranslationTransformer(\n",
       "  (train_ppl): Perplexity()\n",
       "  (val_ppl): Perplexity()\n",
       "  (test_ppl): Perplexity()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(4000, 512, padding_idx=1)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(4000, 512, padding_idx=1)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dense): Linear(in_features=512, out_features=4000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = 'NMT/ar_to_en/SentencePieceTokenizer_to_SentencePieceTokenizer/dotted/checkpoints'\n",
    "averaged_model = average_checkpoints(checkpoint_paths=list(\n",
    "        map(\n",
    "            lambda item: f'{ckpt_dir}/{item}',\n",
    "            os.listdir(ckpt_dir),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "averaged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e92695d4d2e40438e36173b8a50b4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4b4375d3b8416d90307d8b103da453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacre bleu 29.605\n",
      "sacre bleu signature: nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29.605"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_blue_score(\n",
    "        is_dotted=True,\n",
    "        model=averaged_model,\n",
    "        show_translations_for=0,\n",
    "        decode_with_beam_search=False,\n",
    "        source_tokenizer=source_tokenizer,\n",
    "        target_tokenizer=target_tokenizer,\n",
    "        save_predictions_and_targets=False,\n",
    "        max_sequence_length=sequence_length,\n",
    "        source_language_code=source_language_code,\n",
    "        target_language_code=target_language_code,\n",
    "        source_sentences=test_dataset[source_language_code],\n",
    "        target_sentences=test_dataset[target_language_code],\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa4cf3183194fc8820234df616a1667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955428c5849a47bfada39d0531a46489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_blue_score(\n",
    "        is_dotted=True,\n",
    "        model=averaged_model,\n",
    "        show_translations_for=0,\n",
    "        decode_with_beam_search=True,\n",
    "        source_tokenizer=source_tokenizer,\n",
    "        target_tokenizer=target_tokenizer,\n",
    "        max_sequence_length=sequence_length,\n",
    "        save_predictions_and_targets=False,\n",
    "        source_language_code=source_language_code,\n",
    "        target_language_code=target_language_code,\n",
    "        source_sentences=test_dataset[source_language_code],\n",
    "        target_sentences=test_dataset[target_language_code],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dotless-arabic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
