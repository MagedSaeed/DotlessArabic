{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/g201381710/ExperimentsGH/DotlessArabic/dotless_arabic/analysis/notebooks',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/g201381710/.local/lib/python3.10/site-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/usr/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages/IPython/extensions',\n",
       " '/home/g201381710/.ipython',\n",
       " '/home/g201381710/ExperimentsGH/DotlessArabic',\n",
       " '/home/g201381710/.local/lib/python3.10/site-packages',\n",
       " '/home/g201381710/anaconda3/envs/dl4/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/g201381710/ExperimentsGH/DotlessArabic')\n",
    "sys.path.append('/home/g201381710/.local/lib/python3.10/site-packages')\n",
    "sys.path.append('/home/g201381710/anaconda3/envs/dl4/lib/python3.10/site-packages')\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/g201381710/ExperimentsGH/DotlessArabic\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,random,math\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from dotless_arabic.processing import process,undot\n",
    "\n",
    "from dotless_arabic.tokenizers import DisjointLetterTokenizer,FarasaMorphologicalTokenizer, WordTokenizer\n",
    "\n",
    "from dotless_arabic.datasets.news.collect import collect_dataset_for_analysis as collect_news_dataset_for_analysis\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_analysis as collect_quran_dataset_for_analysis\n",
    "from dotless_arabic.datasets.poems.collect import collect_dataset_for_analysis as collect_poems_dataset_for_analysis\n",
    "from dotless_arabic.datasets.wikipedia.collect import collect_dataset_for_analysis as collect_wikipedia_dataset_for_analysis\n",
    "from dotless_arabic.datasets.aggregated.collect import collect_dataset_for_analysis as collect_aggregated_dataset_for_analysis\n",
    "from dotless_arabic.datasets.sanadset_hadeeth.collect import collect_dataset_for_analysis as collect_sanadset_hadeeth_dataset_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = WordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def tokenize_dataset(dataset,tokenizer):\n",
    "    tokenized_dataset = list()\n",
    "    if isinstance(tokenizer,FarasaMorphologicalTokenizer):\n",
    "        segmenter = FarasaSegmenter(interactive=True)\n",
    "    for document in tqdm(dataset):\n",
    "        if isinstance(tokenizer,FarasaMorphologicalTokenizer):\n",
    "            tokenized_document = \" \".join(\n",
    "                tokenizer.split_text(\n",
    "                    document,\n",
    "                    segmenter=segmenter,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            tokenized_document = \" \".join(tokenizer.split_text(document))\n",
    "        tokenized_document = tokenized_document.replace(\"<##>\", \"\")\n",
    "        tokenized_document = re.sub(\"\\s+\", \" \", tokenized_document)\n",
    "        tokenized_dataset.append(tokenized_document)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def words_counter(dataset,use_tqdm=True):\n",
    "    if not use_tqdm:\n",
    "        return Counter(word for item in dataset for word in item.split())    \n",
    "    return Counter(word for item in tqdm(dataset) for word in item.split())\n",
    "@lru_cache()\n",
    "def undot_dataset(dataset):\n",
    "    return list(map(undot, tqdm(dataset)))\n",
    "\n",
    "@lru_cache()\n",
    "def process_dataset(dataset):\n",
    "    return list(map(process, tqdm(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54a81c98c11437fa0375f42b2123a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Original Number of Samples:\n",
      "650,986\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Number of Samples after dropping duplicates:\n",
      "637,565\n",
      "####################################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ab00fcba5f487780213c8d0902d063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration MagedSaeed--ashaar-719bb58a76ea0092\n",
      "Found cached dataset parquet (/home/g201381710/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e482f8aa1b4846be893f9d9489d47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8aa5af484e441bb174677bc132f2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cfa929387946c391aaa042062a8887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[WordTokenizer.__name__] = [\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_quran_dataset_for_analysis()),tokenizer=WordTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_sanadset_hadeeth_dataset_for_analysis()),tokenizer=WordTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_poems_dataset_for_analysis()),tokenizer=WordTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_wikipedia_dataset_for_analysis()),tokenizer=WordTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_news_dataset_for_analysis()),tokenizer=WordTokenizer()))),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-09 09:18:48,262 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-09 09:18:50,023 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63adc7387a404869b90b6d97ca7642ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Original Number of Samples:\n",
      "650,986\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Number of Samples after dropping duplicates:\n",
      "637,565\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-09 09:19:11,165 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-09 09:19:12,950 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46951a2504943d4835a4da5f1e8007c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1156236/2570083409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenized_datasets[FarasaMorphologicalTokenizer.__name__] = [\n\u001b[1;32m      3\u001b[0m         \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_quran_dataset_for_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_sanadset_hadeeth_dataset_for_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_poems_dataset_for_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_wikipedia_dataset_for_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1156236/2962586878.py\u001b[0m in \u001b[0;36mtokenize_dataset\u001b[0;34m(dataset, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             tokenized_document = \" \".join(\n\u001b[0;32m----> 9\u001b[0;31m                 tokenizer.split_text(\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0msegmenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegmenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ExperimentsGH/DotlessArabic/dotless_arabic/tokenizers.py\u001b[0m in \u001b[0;36msplit_text\u001b[0;34m(cls, text, interactive_segmentation, segmenter, undot_text)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mFarasaSegmenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         ), \"segmenter should be an instance of FarasaSegmenter\"\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmenter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" <##> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mundot_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mundot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/farasa/segmenter.py\u001b[0m in \u001b[0;36msegment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_desegment_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/farasa/__base.py\u001b[0m in \u001b[0;36m_do_task\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mstrip_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__interactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_task_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_task_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/farasa/__base.py\u001b[0m in \u001b[0;36m_do_task_interactive\u001b[0;34m(self, strip_text)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mnewlined_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mbyted_newlined_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewlined_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_task_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyted_newlined_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/farasa/__base.py\u001b[0m in \u001b[0;36m_run_task_interactive\u001b[0;34m(self, btext)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__task_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[FarasaMorphologicalTokenizer.__name__] = [\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_quran_dataset_for_analysis()),tokenizer=FarasaMorphologicalTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_sanadset_hadeeth_dataset_for_analysis()),tokenizer=FarasaMorphologicalTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_poems_dataset_for_analysis()),tokenizer=FarasaMorphologicalTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_wikipedia_dataset_for_analysis()),tokenizer=FarasaMorphologicalTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_news_dataset_for_analysis()),tokenizer=FarasaMorphologicalTokenizer()))),\n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[DisjointLetterTokenizer.__name__] = [\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_quran_dataset_for_analysis()),tokenizer=DisjointLetterTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_sanadset_hadeeth_dataset_for_analysis()),tokenizer=DisjointLetterTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_poems_dataset_for_analysis()),tokenizer=DisjointLetterTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_wikipedia_dataset_for_analysis()),tokenizer=DisjointLetterTokenizer()))),\n",
    "        process_dataset(dataset=tuple(tokenize_dataset(dataset=tuple(collect_news_dataset_for_analysis()),tokenizer=DisjointLetterTokenizer()))),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b215c8570d48449be59bf05ee8b941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'WordTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1156236/350477459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWordTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFarasaMorphologicalTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDisjointLetterTokenizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0maggregated_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_dataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0maggregated_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregated_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'WordTokenizer'"
     ]
    }
   ],
   "source": [
    "for tokenizer in tqdm([WordTokenizer,FarasaMorphologicalTokenizer,DisjointLetterTokenizer]):\n",
    "    aggregated_dataset = []\n",
    "    for _dataset in tokenized_datasets[tokenizer.__name__]:\n",
    "        aggregated_dataset.extend(_dataset)\n",
    "    tokenized_datasets.append(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DisjointLetterTokenizer'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in tqdm(all_datasets):\n",
    "    random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets_names = [\n",
    "    'quran',\n",
    "    'sanadset hadeeth',\n",
    "    'poems',\n",
    "    'wikipedia',\n",
    "    'news',\n",
    "    'aggregated',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may need to report the error on the regression line: MSE, MAE, etc.\n",
    "@lru_cache()\n",
    "def get_heaps_law_constants(dataset,get_points_list=False,clear_lru_cache=False):\n",
    "  X,Y = [],[]\n",
    "  vocabulary = set()\n",
    "  number_of_tokens = 0\n",
    "  for index in tqdm(range(len(dataset))):\n",
    "    \n",
    "    counter = words_counter(\n",
    "        tuple([dataset[index]]),\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    \n",
    "    number_of_tokens += sum(counter.values())\n",
    "    X.append(\n",
    "        math.log(\n",
    "            number_of_tokens,\n",
    "             math.e, #base\n",
    "          ),\n",
    "      )\n",
    "    \n",
    "    vocabulary.update(counter.keys())\n",
    "    Y.append(\n",
    "        math.log(\n",
    "            len(vocabulary),\n",
    "             math.e, #base\n",
    "          ),\n",
    "      )\n",
    "    \n",
    "    if clear_lru_cache:\n",
    "      words_counter.cache_clear()\n",
    "    \n",
    "  slope,intercept,*_= linregress(X,Y)\n",
    "\n",
    "  b = slope\n",
    "  k = math.e**intercept\n",
    "\n",
    "  if get_points_list:\n",
    "    return b,k,(X,Y)\n",
    "  return b,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heaps_law_constants(dataset=all_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heaps_multiple(figsize=(25,15)):\n",
    "\n",
    "  unlog = lambda points: list(map(lambda point: math.e**point,points))\n",
    "\n",
    "  tokenizers = (WordTokenizer,FarasaMorphologicalTokenizerDisjointLetterTokenizer,)\n",
    "\n",
    "  figure, axis = plt.subplots(nrows=len(tokenizers), ncols=2, figsize=figsize,dpi=200,layout='constrained')\n",
    "    \n",
    "  plt.rc('font',size=FONT_SIZE)\n",
    "  figure.supxlabel('N',fontsize=FONT_SIZE)\n",
    "  figure.supylabel('V',fontsize=FONT_SIZE)\n",
    "\n",
    "  for irow,tokenizer in enumerate(tokenizers):\n",
    "      # The first plot is for Quran dataset\n",
    "      beta,k,(X,Y) = get_heaps_law_constants(dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),get_points_list=True)\n",
    "      dataset_name = all_datasets_names[0]\n",
    "        \n",
    "      undotted_beta,undotted_k,(undotted_X,undotted_Y) = get_heaps_law_constants(dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),get_points_list=True)\n",
    "       \n",
    "      axis[irow,0].plot(unlog(X),unlog(Y),label=f'{dataset_name} dataset (k={k:.3f},β={beta:.3f})')\n",
    "      axis[irow,0].plot(\n",
    "          unlog(X),\n",
    "          [k*x**beta for x in unlog(X)],\n",
    "          label=f'dotted regression fit for {dataset_name}',\n",
    "          linestyle='dashed',\n",
    "      )\n",
    "    \n",
    "      axis[irow,0].plot(unlog(undotted_X),unlog(undotted_Y),label=f'undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})')\n",
    "      axis[irow,0].plot(\n",
    "          unlog(undotted_X),\n",
    "          [undotted_k*x**undotted_beta for x in unlog(undotted_X)],\n",
    "          label=f'undotted regression fit for {dataset_name}',\n",
    "          linestyle='dashed',\n",
    "      )\n",
    "\n",
    "\n",
    "     \n",
    "      for i,dataset_name in enumerate(all_datasets_names[1:]):\n",
    "\n",
    "          beta,k,(X,Y) = get_heaps_law_constants(dataset=tuple(tokenized_datasets[tokenizer.__name__][i+1]),get_points_list=True)\n",
    "            \n",
    "          undotted_beta,undotted_k,(undotted_X,undotted_Y) = get_heaps_law_constants(dataset=tuple(tokenized_datasets[tokenizer.__name__][i+1]),get_points_list=True)\n",
    "           \n",
    "          axis[irow,1].plot(unlog(X),unlog(Y),label=f'{dataset_name} dataset (k={k:.3f},β={beta:.3f})')\n",
    "          axis[irow,1].plot(\n",
    "              unlog(X),\n",
    "              [k*x**beta for x in unlog(X)],\n",
    "              label=f'dotted regression fit for {dataset_name}',\n",
    "              linestyle='dashed',\n",
    "          )\n",
    "        \n",
    "          axis[irow,1].plot(unlog(undotted_X),unlog(undotted_Y),label=f'undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})')\n",
    "          axis[irow,1].plot(\n",
    "              unlog(undotted_X),\n",
    "              [undotted_k*x**undotted_beta for x in unlog(undotted_X)],\n",
    "              label=f'undotted regression fit for {dataset_name}',\n",
    "              linestyle='dashed',\n",
    "          )  \n",
    "    \n",
    "      # plt.legend(loc='lower right',fontsize=14)\n",
    "      # plt.ylim(ymin=0)\n",
    "      # plt.xlim(xmin=0)\n",
    "      # https://stackoverflow.com/questions/6390393/matplotlib-make-tick-labels-font-size-smaller\n",
    "      # plt.xticks(fontsize=14)\n",
    "      # plt.yticks(fontsize=14)\n",
    "      # https://stackoverflow.com/questions/34227595/how-to-change-font-size-of-the-scientific-notation-in-matplotlib\n",
    "      # https://stackoverflow.com/questions/21512305/inconsistent-font-size-for-scientific-notation-in-axis\n",
    "      # plt.rc('font',size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heaps_multiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_heaps(\n",
    "#     datasets=[all_datasets[0]],\n",
    "#     datasets_names=[all_datasets_names[0]],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_heaps(\n",
    "#     datasets=all_datasets[:0:-1],\n",
    "#     datasets_names=all_datasets_names[:0:-1],\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
