{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,random,math\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from dotless_arabic.processing import process,undot\n",
    "\n",
    "from dotless_arabic.tokenizers import DisjointLetterTokenizer,FarasaMorphologicalTokenizer, WordTokenizer\n",
    "\n",
    "from dotless_arabic.datasets.news.collect import collect_dataset_for_analysis as collect_news_dataset_for_analysis\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_analysis as collect_quran_dataset_for_analysis\n",
    "from dotless_arabic.datasets.poems.collect import collect_dataset_for_analysis as collect_poems_dataset_for_analysis\n",
    "from dotless_arabic.datasets.wikipedia.collect import collect_dataset_for_analysis as collect_wikipedia_dataset_for_analysis\n",
    "from dotless_arabic.datasets.aggregated.collect import collect_dataset_for_analysis as collect_aggregated_dataset_for_analysis\n",
    "from dotless_arabic.datasets.sanadset_hadeeth.collect import collect_dataset_for_analysis as collect_sanadset_hadeeth_dataset_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_dataset = list()\n",
    "    if isinstance(tokenizer,FarasaMorphologicalTokenizer):\n",
    "        segmenter = FarasaSegmenter(interactive=True)\n",
    "    for document in tqdm(dataset):\n",
    "        if isinstance(tokenizer,FarasaMorphologicalTokenizer):\n",
    "            tokenized_document = \" \".join(\n",
    "                tokenizer.split_text(\n",
    "                    document,\n",
    "                    segmenter=segmenter,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            tokenized_document = \" \".join(tokenizer.split_text(document))\n",
    "        tokenized_document = tokenized_document.replace(\"<##>\", \"\")\n",
    "        tokenized_document = re.sub(\"\\s+\", \" \", tokenized_document)\n",
    "        tokenized_dataset.append(tokenized_document)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def words_counter(dataset,use_tqdm=True):\n",
    "    if not use_tqdm:\n",
    "        return Counter(word for item in dataset for word in item.split())    \n",
    "    return Counter(word for item in tqdm(dataset) for word in item.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "all_datasets = [\n",
    "    tokenize_dataset(dataset=tuple(collect_quran_dataset_for_analysis())),\n",
    "    tokenize_dataset(dataset=tuple(collect_sanadset_hadeeth_dataset_for_analysis())),\n",
    "    tokenize_dataset(dataset=tuple(collect_poems_dataset_for_analysis())),\n",
    "    tokenize_dataset(dataset=tuple(collect_wikipedia_dataset_for_analysis())),\n",
    "    tokenize_dataset(dataset=tuple(collect_news_dataset_for_analysis())),\n",
    "#     tokenize_dataset(dataset=tuple(collect_raw_aggregated_dataset())),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_dataset = []\n",
    "for _dataset in all_datasets:\n",
    "    aggregated_dataset.extend(_dataset)\n",
    "all_datasets.append(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process datasets\n",
    "all_datasets = [\n",
    "    process_dataset(dataset=tuple(all_datasets[0])),\n",
    "    process_dataset(dataset=tuple(all_datasets[1])),\n",
    "    process_dataset(dataset=tuple(all_datasets[2])),\n",
    "    process_dataset(dataset=tuple(all_datasets[3])),\n",
    "    process_dataset(dataset=tuple(all_datasets[4])),\n",
    "#     process_dataset(dataset=tuple(datasets[5])),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_dataset = []\n",
    "for _dataset in all_datasets:\n",
    "    aggregated_dataset.extend(_dataset)  \n",
    "all_datasets.append(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in tqdm(all_datasets):\n",
    "    random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets_names = [\n",
    "    'quran',\n",
    "    'sanadset hadeeth',\n",
    "    'poems',\n",
    "    'wikipedia',\n",
    "    'news',\n",
    "    'aggregated',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may need to report the error on the regression line: MSE, MAE, etc.\n",
    "def get_heaps_law_constants(dataset,get_points_list=False,clear_lru_cache=False):\n",
    "  X,Y = [],[]\n",
    "  vocabulary = set()\n",
    "  number_of_tokens = 0\n",
    "  for index in tqdm(range(len(dataset))):\n",
    "    \n",
    "    counter = words_counter(\n",
    "        tuple([dataset[index]]),\n",
    "        use_tqdm=False,\n",
    "    )\n",
    "    \n",
    "    number_of_tokens += sum(counter.values())\n",
    "    X.append(\n",
    "        math.log(\n",
    "            number_of_tokens,\n",
    "             math.e, #base\n",
    "          ),\n",
    "      )\n",
    "    \n",
    "    vocabulary.update(counter.keys())\n",
    "    Y.append(\n",
    "        math.log(\n",
    "            len(vocabulary),\n",
    "             math.e, #base\n",
    "          ),\n",
    "      )\n",
    "    \n",
    "    if clear_lru_cache:\n",
    "      words_counter.cache_clear()\n",
    "    \n",
    "  slope,intercept,*_= linregress(X,Y)\n",
    "\n",
    "  b = slope\n",
    "  k = math.e**intercept\n",
    "\n",
    "  if get_points_list:\n",
    "    return b,k,(X,Y)\n",
    "  return b,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heaps_law_constants(dataset=all_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heaps(\n",
    "    datasets=all_datasets,\n",
    "    datasets_names=all_datasets_names,\n",
    "    figsize=(25,15),\n",
    "  ):\n",
    "\n",
    "  unlog = lambda points: list(map(lambda point: math.e**point,points))\n",
    "\n",
    "  plt.figure(figsize=figsize)\n",
    "    \n",
    "  for dataset,dataset_name in zip(datasets,datasets_names):\n",
    "        \n",
    "      beta,k,(X,Y) = get_heaps_law_constants(dataset,get_points_list=True)\n",
    "      print()\n",
    "      print('-'*100)\n",
    "      print('-'*100)\n",
    "      print('beta: ',beta,' k ',k,' for dataset: ',dataset_name)\n",
    "\n",
    "      print('some samples of the dotted X,Y lists:')  \n",
    "      print(list(map(lambda item: round(item,2),unlog(X[:10]))))\n",
    "      print(list(map(lambda item: round(item,2),unlog(Y[:10]))))\n",
    "        \n",
    "      if 'aggregated' in dataset_name:\n",
    "        undotted_dataset = []\n",
    "        for dataset in all_datasets[:-1]:\n",
    "            undotted_dataset.extend(undot_dataset(dataset=tuple(dataset)))\n",
    "        random.shuffle(undotted_dataset)\n",
    "      else:\n",
    "        undotted_dataset = undot_dataset(dataset=tuple(dataset))\n",
    "        \n",
    "      undotted_beta,undotted_k,(undotted_X,undotted_Y) = get_heaps_law_constants(undotted_dataset,get_points_list=True)\n",
    "      print('-'*100)\n",
    "      print('undotted beta: ',undotted_beta,' undotted k ',undotted_k, ' for dataset: ',dataset_name)\n",
    "      print('-'*100)\n",
    "      print('-'*100)\n",
    "      print()\n",
    "       \n",
    "      plt.plot(unlog(X),unlog(Y),label=f'{dataset_name} dataset (k={k:.3f},β={beta:.3f})')\n",
    "      plt.plot(\n",
    "          unlog(X),\n",
    "          [k*x**beta for x in unlog(X)],\n",
    "          label=f'dotted regression fit for {dataset_name}',\n",
    "          linestyle='dashed',\n",
    "      )\n",
    "      # https://stackoverflow.com/a/41308516/4412324\n",
    "      # plt.text(\n",
    "      #     x=0.73,\n",
    "      #     y=0.95,\n",
    "      #     s=f'dotted: k={k:.3f},β={beta:.3f}',\n",
    "      #     transform=plt.gca().transAxes,\n",
    "      #     fontsize=12,\n",
    "      # )\n",
    "    \n",
    "      plt.plot(unlog(undotted_X),unlog(undotted_Y),label=f'undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})')\n",
    "      plt.plot(\n",
    "          unlog(undotted_X),\n",
    "          [undotted_k*x**undotted_beta for x in unlog(undotted_X)],\n",
    "          label=f'undotted regression fit for {dataset_name}',\n",
    "          linestyle='dashed',\n",
    "      )\n",
    "        \n",
    "      # plt.text(\n",
    "      #     x=0.73,\n",
    "      #     y=0.92,\n",
    "      #     s=f'undotted: k={undotted_k:.3f},β={undotted_beta:.3f}',\n",
    "      #     transform=plt.gca().transAxes,\n",
    "      #     fontsize=12,\n",
    "      # )\n",
    "    \n",
    "      plt.legend(loc='lower right',fontsize=14)\n",
    "      plt.ylim(ymin=0)\n",
    "      plt.xlim(xmin=0)\n",
    "      # https://stackoverflow.com/questions/6390393/matplotlib-make-tick-labels-font-size-smaller\n",
    "      plt.xticks(fontsize=14)\n",
    "      plt.yticks(fontsize=14)\n",
    "      # https://stackoverflow.com/questions/34227595/how-to-change-font-size-of-the-scientific-notation-in-matplotlib\n",
    "      # https://stackoverflow.com/questions/21512305/inconsistent-font-size-for-scientific-notation-in-axis\n",
    "      plt.rc('font',size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heaps(\n",
    "    datasets=[all_datasets[0]],\n",
    "    datasets_names=[all_datasets_names[0]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heaps(\n",
    "    datasets=all_datasets[:0:-1],\n",
    "    datasets_names=all_datasets_names[:0:-1],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
