{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/home/g201381710/ExperimentsGH/DotlessArabic')\n",
    "# sys.path.append('/home/g201381710/.local/lib/python3.10/site-packages')\n",
    "# sys.path.append('/home/g201381710/anaconda3/envs/dl4/lib/python3.10/site-packages')\n",
    "\n",
    "# sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Experiments/DotlessArabic\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re,random,math\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from dotless_arabic.processing import process,undot\n",
    "\n",
    "from dotless_arabic.tokenizers import DisjointLetterTokenizer,FarasaMorphologicalTokenizer, WordTokenizer\n",
    "\n",
    "from dotless_arabic.datasets.news.collect import collect_dataset_for_analysis as collect_news_dataset_for_analysis\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_analysis as collect_quran_dataset_for_analysis\n",
    "from dotless_arabic.datasets.poems.collect import collect_dataset_for_analysis as collect_poems_dataset_for_analysis\n",
    "from dotless_arabic.datasets.wikipedia.collect import collect_dataset_for_analysis as collect_wikipedia_dataset_for_analysis\n",
    "from dotless_arabic.datasets.aggregated.collect import collect_dataset_for_analysis as collect_aggregated_dataset_for_analysis\n",
    "from dotless_arabic.datasets.sanadset_hadeeth.collect import collect_dataset_for_analysis as collect_sanadset_hadeeth_dataset_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    tokenized_dataset = list()\n",
    "    if isinstance(tokenizer, FarasaMorphologicalTokenizer):\n",
    "        segmenter = FarasaSegmenter(interactive=True)\n",
    "    for document in tqdm(dataset):\n",
    "        if isinstance(tokenizer, FarasaMorphologicalTokenizer):\n",
    "            tokenized_document = \" \".join(\n",
    "                tokenizer.split_text(\n",
    "                    document,\n",
    "                    segmenter=segmenter,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            tokenized_document = \" \".join(tokenizer.split_text(document))\n",
    "        tokenized_document = tokenized_document.replace(\"<##>\", \"\")\n",
    "        tokenized_document = re.sub(\"\\s+\", \" \", tokenized_document)\n",
    "        tokenized_dataset.append(tokenized_document)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def words_counter(dataset, use_tqdm=True):\n",
    "    if not use_tqdm:\n",
    "        return Counter(word for item in dataset for word in item.split())\n",
    "    return Counter(word for item in tqdm(dataset) for word in item.split())\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def undot_dataset(dataset):\n",
    "    return list(map(undot, tqdm(dataset)))\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def process_dataset(dataset):\n",
    "    return list(map(process, tqdm(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Original Number of Samples:\n",
      "650,986\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Number of Samples after dropping duplicates:\n",
      "637,565\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/majed_alshaibani/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    collect_quran_dataset_for_analysis(),\n",
    "    collect_sanadset_hadeeth_dataset_for_analysis(),\n",
    "    collect_poems_dataset_for_analysis(),\n",
    "    collect_wikipedia_dataset_for_analysis(),\n",
    "    collect_news_dataset_for_analysis(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886821c166d544fb849b7067e9815dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e2518ad62f426183fa656f68e1790d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb3ea7d58564a1e827aee364ba1d1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469e0535531947f294146a2c5ac71536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95918b950185460dbdf9f86ca2fb27c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb3b75e9fe042fb97b4a825fc66459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57562f4ffc14dc2b346e674c317799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb3b442621b4ea2b96033035b7e2eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cef563929bd49d6a3eac54ce6c38431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d84b82fef24dfdb3bb3b9f3b3424ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[WordTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[0]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[1]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[2]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[3]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[4]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 16:56:19,192 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-28 16:56:20,882 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f69b1741f32484ba15f11b2d83263f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6256e10b9244b4f86d66b2d5019668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 16:56:29,237 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-28 16:56:30,926 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ae7b44c67a4609afaa8788f133670b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c89cf195aea4f238038c2a807eaaaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 17:12:36,406 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-28 17:12:38,149 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887e2de7f45b4e4ab27fd22f676158ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fe08e1667f4a099b7ca4cbf2bb452d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 17:45:42,318 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-28 17:45:44,083 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf109b9deea458fa587f363de15621c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f800f4a7dcae4189abe1e20962ccff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 19:45:53,580 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-28 19:45:55,300 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ec191e8f294c7ba474a464c92f4d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e180a7758f0d41d39c29b59939d8a86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[FarasaMorphologicalTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[0]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[1]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[2]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[3]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[4]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40873c4e54e43dfb9bd9a4d2a594bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a45aaec613422fb898b1fbbae88acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5007f76ecf45e487229a97941e43fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ec2c9c936f4999893d4f232309cd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df5ff4942da4203bc7390a0129bc2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3703a73d653d4f5cbe257bbf00884c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ea2c264cbc477d8bc327a4755c7d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f86d90ebd2641a8a6a5d8bd5f83dee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f011a120444e487fae08319922e39117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249cce86d88a410191182c98e6e4d2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[DisjointLetterTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[0]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[1]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[2]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[3]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[4]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ae9d2edc924fb18165ba21ad302bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tokenizer in tqdm(\n",
    "    [\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ]\n",
    "):\n",
    "    aggregated_dataset = []\n",
    "    for _dataset in tokenized_datasets[tokenizer.__name__]:\n",
    "        aggregated_dataset.extend(_dataset)\n",
    "    tokenized_datasets[tokenizer.__name__].append(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f59099b8ce4f5ba4dbaf845ce8b80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb34141086744ad9231d7b47fb329d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5783b4e1b10410497a768f337a2caa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f2022aadd4cec98bfdbd0c5152c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tokenizer in tqdm(\n",
    "    [\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ]\n",
    "):\n",
    "    for dataset in tqdm(tokenized_datasets[tokenizer.__name__]):\n",
    "        random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_datasets_names = [\n",
    "    \"quran\",\n",
    "    \"sanadset hadeeth\",\n",
    "    \"poems\",\n",
    "    \"wikipedia\",\n",
    "    \"news\",\n",
    "    \"aggregated\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we may need to report the error on the regression line: MSE, MAE, etc.\n",
    "@lru_cache()\n",
    "def get_heaps_law_constants(dataset, get_points_list=False, clear_lru_cache=False):\n",
    "    X, Y = [], []\n",
    "    vocabulary = set()\n",
    "    number_of_tokens = 0\n",
    "    for index in tqdm(range(len(dataset))):\n",
    "        counter = words_counter(\n",
    "            tuple([dataset[index]]),\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "\n",
    "        number_of_tokens += sum(counter.values())\n",
    "        X.append(\n",
    "            math.log(\n",
    "                number_of_tokens,\n",
    "                math.e,  # base\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        vocabulary.update(counter.keys())\n",
    "        Y.append(\n",
    "            math.log(\n",
    "                len(vocabulary),\n",
    "                math.e,  # base\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if clear_lru_cache:\n",
    "            words_counter.cache_clear()\n",
    "\n",
    "    slope, intercept, *_ = linregress(X, Y)\n",
    "\n",
    "    b = slope\n",
    "    k = math.e**intercept\n",
    "\n",
    "    if get_points_list:\n",
    "        return b, k, (X, Y)\n",
    "    return b, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02ec9b3d5c3454e8ab8d4ed5e91332d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7778029956777596, 2.6510038690153572)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_heaps_law_constants(dataset=tuple(datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fa64d194774937bfaff5ea729dc439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99be567c263143db8909ca0d733c4f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1336f13d4efe4644960eb145c05c1e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a530c2119b4bdeb11641c7acfb2ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe318a22d3b4970a3561773f96a3400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c08b9a290d423ea6bb3dc9b220961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16cf2354a2b4c1da7a62b16435a6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b153ec7b95874a9e92a4f2acad1e01e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2615007c6a4e4c0db68a7c32ab15fa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da72e5b98d04865b9f06c820106805d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f48b638903a49ce986cf767af8461fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48be87c33f34bddb5c2567b2d59b468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9e7940e6c640d4b4157afd96a2dad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbe44ddbb4a4d799171d3c52605a24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffe5537643843a297c5a74aea881a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a8798befa447208783f9ea8dbce810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa82b77b6c084215b7a62806dbe1c5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e82febd73ad4095842d098c84ccbd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "undotted_aggregated = {}\n",
    "for tokenizer in (\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ):\n",
    "    _undotted_dataset = []\n",
    "    for _dataset in tqdm(tokenized_datasets[tokenizer.__name__][:-1]):\n",
    "        _undotted_dataset.extend(undot_dataset(dataset=tuple(_dataset)))\n",
    "    random.shuffle(_undotted_dataset)\n",
    "    undotted_aggregated[tokenizer.__name__] = _undotted_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heaps_multiple(figsize=(22, 26), fontsize=11):\n",
    "    unlog = lambda points: list(map(lambda point: math.e**point, points))\n",
    "\n",
    "    tokenizers = (\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    )\n",
    "\n",
    "    figure, axis = plt.subplots(\n",
    "        nrows=len(tokenizers), ncols=2, figsize=figsize, dpi=300, layout=\"constrained\"\n",
    "    )\n",
    "\n",
    "    plt.rc(\"font\", size=fontsize)\n",
    "    figure.supxlabel(\"Running Text (N)\", fontsize=fontsize)\n",
    "    figure.supylabel(\"Vocabulary (V)\", fontsize=fontsize)\n",
    "\n",
    "    for irow, tokenizer in enumerate(tokenizers):\n",
    "        print('working with tokenizer: ',tokenizer.__name__)\n",
    "        # The first plot is for Quran dataset\n",
    "        beta, k, (X, Y) = get_heaps_law_constants(\n",
    "            dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),\n",
    "            get_points_list=True,\n",
    "        )\n",
    "        dataset_name = all_datasets_names[0]\n",
    "\n",
    "        undotted_beta, undotted_k, (undotted_X, undotted_Y) = get_heaps_law_constants(\n",
    "            dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),\n",
    "            get_points_list=True,\n",
    "        )\n",
    "\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(X), unlog(Y), label=f\"{dataset_name} dataset (k={k:.3f},β={beta:.3f})\"\n",
    "        )\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(X),\n",
    "            [k * x**beta for x in unlog(X)],\n",
    "            # label=f\"dotted regression fit for {dataset_name}\",\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(undotted_X),\n",
    "            unlog(undotted_Y),\n",
    "            label=f\"undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})\",\n",
    "        )\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(undotted_X),\n",
    "            [undotted_k * x**undotted_beta for x in unlog(undotted_X)],\n",
    "            # label=f\"undotted regression fit for {dataset_name}\",\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "        axis[irow, 0].legend(loc='lower right',fontsize=fontsize)\n",
    "        # https://stackoverflow.com/a/11745291/4412324\n",
    "        axis[irow, 0].set_ylim(bottom=0,top=None,auto=True)\n",
    "        axis[irow, 0].set_xlim(left=0,right=None,auto=True)\n",
    "        for i, dataset_name in enumerate(all_datasets_names[1:],start=1):\n",
    "            beta, k, (X, Y) = get_heaps_law_constants(\n",
    "                dataset=tuple(tokenized_datasets[tokenizer.__name__][i]),\n",
    "                get_points_list=True,\n",
    "            )\n",
    "\n",
    "            if \"aggregated\" in dataset_name:\n",
    "                undotted_dataset = undotted_aggregated[tokenizer.__name__]\n",
    "            else:\n",
    "                undotted_dataset = undot_dataset(\n",
    "                    dataset=tuple(tokenized_datasets[tokenizer.__name__][i])\n",
    "                )\n",
    "\n",
    "            (\n",
    "                undotted_beta,\n",
    "                undotted_k,\n",
    "                (undotted_X, undotted_Y),\n",
    "            ) = get_heaps_law_constants(\n",
    "                dataset=tuple(undotted_dataset),\n",
    "                get_points_list=True,\n",
    "            )\n",
    "\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(X),\n",
    "                unlog(Y),\n",
    "                label=f\"{dataset_name} dataset (k={k:.3f},β={beta:.3f})\",\n",
    "            )\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(X),\n",
    "                [k * x**beta for x in unlog(X)],\n",
    "                # # label=f\"dotted regression fit for {dataset_name}\",\n",
    "                linestyle=\"dashed\",\n",
    "            )\n",
    "\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(undotted_X),\n",
    "                unlog(undotted_Y),\n",
    "                label=f\"undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})\",\n",
    "            )\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(undotted_X),\n",
    "                [undotted_k * x**undotted_beta for x in unlog(undotted_X)],\n",
    "                # label=f\"undotted regression fit for {dataset_name}\",\n",
    "                linestyle=\"dashed\",\n",
    "            )\n",
    "            axis[irow, 1].legend(loc='lower right',fontsize=fontsize)\n",
    "            # https://stackoverflow.com/a/11745291/4412324\n",
    "            axis[irow, 1].set_ylim(bottom=0,top=None,auto=True)\n",
    "            axis[irow, 1].set_xlim(left=0,right=None,auto=True)\n",
    "        print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with tokenizer:  WordTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "working with tokenizer:  FarasaMorphologicalTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "working with tokenizer:  DisjointLetterTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "plot_heaps_multiple()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
