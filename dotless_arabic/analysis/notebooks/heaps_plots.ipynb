{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/majed_alshaibani/Experiments/DotlessArabic/dotless_arabic/analysis/notebooks',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/majed_alshaibani/Experiments/dotless-arabic/lib/python3.10/site-packages',\n",
       " '/home/g201381710/ExperimentsGH/DotlessArabic',\n",
       " '/home/g201381710/.local/lib/python3.10/site-packages',\n",
       " '/home/g201381710/anaconda3/envs/dl4/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/g201381710/ExperimentsGH/DotlessArabic')\n",
    "sys.path.append('/home/g201381710/.local/lib/python3.10/site-packages')\n",
    "sys.path.append('/home/g201381710/anaconda3/envs/dl4/lib/python3.10/site-packages')\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/Experiments/DotlessArabic\n"
     ]
    }
   ],
   "source": [
    "%cd ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re,random,math\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from dotless_arabic.processing import process,undot\n",
    "\n",
    "from dotless_arabic.tokenizers import DisjointLetterTokenizer,FarasaMorphologicalTokenizer, WordTokenizer\n",
    "\n",
    "from dotless_arabic.datasets.news.collect import collect_dataset_for_analysis as collect_news_dataset_for_analysis\n",
    "from dotless_arabic.datasets.quran.collect import collect_dataset_for_analysis as collect_quran_dataset_for_analysis\n",
    "from dotless_arabic.datasets.poems.collect import collect_dataset_for_analysis as collect_poems_dataset_for_analysis\n",
    "from dotless_arabic.datasets.wikipedia.collect import collect_dataset_for_analysis as collect_wikipedia_dataset_for_analysis\n",
    "from dotless_arabic.datasets.aggregated.collect import collect_dataset_for_analysis as collect_aggregated_dataset_for_analysis\n",
    "from dotless_arabic.datasets.sanadset_hadeeth.collect import collect_dataset_for_analysis as collect_sanadset_hadeeth_dataset_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    tokenized_dataset = list()\n",
    "    if isinstance(tokenizer, FarasaMorphologicalTokenizer):\n",
    "        segmenter = FarasaSegmenter(interactive=True)\n",
    "    for document in tqdm(dataset):\n",
    "        if isinstance(tokenizer, FarasaMorphologicalTokenizer):\n",
    "            tokenized_document = \" \".join(\n",
    "                tokenizer.split_text(\n",
    "                    document,\n",
    "                    segmenter=segmenter,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            tokenized_document = \" \".join(tokenizer.split_text(document))\n",
    "        tokenized_document = tokenized_document.replace(\"<##>\", \"\")\n",
    "        tokenized_document = re.sub(\"\\s+\", \" \", tokenized_document)\n",
    "        tokenized_dataset.append(tokenized_document)\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def words_counter(dataset, use_tqdm=True):\n",
    "    if not use_tqdm:\n",
    "        return Counter(word for item in dataset for word in item.split())\n",
    "    return Counter(word for item in tqdm(dataset) for word in item.split())\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def undot_dataset(dataset):\n",
    "    return list(map(undot, tqdm(dataset)))\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def process_dataset(dataset):\n",
    "    return list(map(process, tqdm(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Original Number of Samples:\n",
      "650,986\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Number of Samples after dropping duplicates:\n",
      "637,565\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/majed_alshaibani/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    collect_quran_dataset_for_analysis(),\n",
    "    collect_sanadset_hadeeth_dataset_for_analysis(),\n",
    "    collect_poems_dataset_for_analysis(),\n",
    "    collect_wikipedia_dataset_for_analysis(),\n",
    "    collect_news_dataset_for_analysis(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d303e8f5cc4546b1a31c4915b2544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bad1fd19d34ec588e7926644e4868c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8373a9cece2649709c48af2c27c170a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9135c46378eb450bbe09bb3ee372b41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37847b513fa42b0820ee142e385e70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b8bf57d9ab47f28163d6cedd25133a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d659d5cd094d4a0b9e763ab72f20d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e97516fd3c4f2cb7d51ddd84db19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470ae28d7e954ca69374bc82c3258cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d25238392fa416da3e824029fd09f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[WordTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[0]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[1]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[2]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[3]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(dataset=tuple(datasets[4]), tokenizer=WordTokenizer())\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-10 11:34:04,645 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-10 11:34:06,342 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9078f309d148fea1b98b053f601279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914319b30f674d5e87b1a817fd9d2ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-10 11:34:14,532 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-10 11:34:16,192 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d573dd5eb340b7afe39d5028f3a520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b778c77ca004ae4a54ee0ef01b37415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-10 11:50:41,788 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-10 11:50:43,525 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5413c15b504bcdaa76e0a222931c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5764ea5cb77847bda6bd9bc84f360e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-10 12:22:42,665 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-10 12:22:44,454 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaf068552f846fc9b0801f3d1f25de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a85b832759e435c978b2316045235b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-10 14:24:38,767 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2023-03-10 14:24:40,509 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62357e8a2ec64022bffac0aa8293f50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9176c88b9b8a42a28479141631ac27dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[FarasaMorphologicalTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[0]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[1]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[2]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[3]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[4]), tokenizer=FarasaMorphologicalTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4270bc9ff85c477581742ef4ce3a92a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce291eb253de4a07ad5b94e8e2791c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f099448b16f45e4ae09c61722ee4b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f81da1dd5c4c1f8673d937f0f74435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/637565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3c39c1a7604ddf90885d325cd4299c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b7cc3572b648429f28348c2176d14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7714858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101d544d8b2941e597e23e8bc2f616f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d56f2ba58594a4da34635dcf26096b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4636663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f2cf9b29e5444d863cec40d7c408b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e462451636a4c9a9376a162337aa553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2784041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_datasets[DisjointLetterTokenizer.__name__] = [\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[0]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[1]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[2]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[3]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    process_dataset(\n",
    "        dataset=tuple(\n",
    "            tokenize_dataset(\n",
    "                dataset=tuple(datasets[4]), tokenizer=DisjointLetterTokenizer()\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c51390dd5ad445b961a1f2be0856b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tokenizer in tqdm(\n",
    "    [\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ]\n",
    "):\n",
    "    aggregated_dataset = []\n",
    "    for _dataset in tokenized_datasets[tokenizer.__name__]:\n",
    "        aggregated_dataset.extend(_dataset)\n",
    "    tokenized_datasets[tokenizer.__name__].append(aggregated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0a94010a384aeb877f53b497bb98a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60dc9f6f593417e9dee4c6bc2d80b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7405c15bd14e0ea1612f49ca6e2ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a2e9214724195a0301fcf571f80e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tokenizer in tqdm(\n",
    "    [\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ]\n",
    "):\n",
    "    for dataset in tqdm(tokenized_datasets[tokenizer.__name__]):\n",
    "        random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_datasets_names = [\n",
    "    \"quran\",\n",
    "    \"sanadset hadeeth\",\n",
    "    \"poems\",\n",
    "    \"wikipedia\",\n",
    "    \"news\",\n",
    "    \"aggregated\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we may need to report the error on the regression line: MSE, MAE, etc.\n",
    "@lru_cache()\n",
    "def get_heaps_law_constants(dataset, get_points_list=False, clear_lru_cache=False):\n",
    "    X, Y = [], []\n",
    "    vocabulary = set()\n",
    "    number_of_tokens = 0\n",
    "    for index in tqdm(range(len(dataset))):\n",
    "        counter = words_counter(\n",
    "            tuple([dataset[index]]),\n",
    "            use_tqdm=False,\n",
    "        )\n",
    "\n",
    "        number_of_tokens += sum(counter.values())\n",
    "        X.append(\n",
    "            math.log(\n",
    "                number_of_tokens,\n",
    "                math.e,  # base\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        vocabulary.update(counter.keys())\n",
    "        Y.append(\n",
    "            math.log(\n",
    "                len(vocabulary),\n",
    "                math.e,  # base\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if clear_lru_cache:\n",
    "            words_counter.cache_clear()\n",
    "\n",
    "    slope, intercept, *_ = linregress(X, Y)\n",
    "\n",
    "    b = slope\n",
    "    k = math.e**intercept\n",
    "\n",
    "    if get_points_list:\n",
    "        return b, k, (X, Y)\n",
    "    return b, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafaab1928bb4a8fbf79ba5e7107ad29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.7778029956777596, 2.6510038690153572)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_heaps_law_constants(dataset=tuple(datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde5c488765430fbda8631627616a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de63c1665ea64b809fc9960b095741e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f78952d3ddb44fbba40a2c06326dddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4523eac001804cc983dc94071be1cbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "undotted_aggregated = {}\n",
    "for tokenizer in (\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    ):\n",
    "    _undotted_dataset = []\n",
    "    for _dataset in tqdm(tokenized_datasets[tokenizer.__name__][:-1]):\n",
    "        _undotted_dataset.extend(undot_dataset(dataset=tuple(_dataset)))\n",
    "    random.shuffle(_undotted_dataset)\n",
    "    undotted_aggregated[tokenizer.__name__] = _undotted_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heaps_multiple(figsize=(40, 40),fontsize=15):\n",
    "    unlog = lambda points: list(map(lambda point: math.e**point, points))\n",
    "\n",
    "    tokenizers = (\n",
    "        WordTokenizer,\n",
    "        FarasaMorphologicalTokenizer,\n",
    "        DisjointLetterTokenizer,\n",
    "    )\n",
    "\n",
    "    figure, axis = plt.subplots(\n",
    "        nrows=len(tokenizers), ncols=2, figsize=figsize, dpi=200, layout=\"constrained\"\n",
    "    )\n",
    "\n",
    "    plt.rc(\"font\", size=fontsize)\n",
    "    figure.supxlabel(\"Running Text (N)\", fontsize=fontsize+3)\n",
    "    figure.supylabel(\"Vocabulary (V)\", fontsize=fontsize+3)\n",
    "\n",
    "    for irow, tokenizer in enumerate(tokenizers):\n",
    "        print('working with tokenizer: ',tokenizer.__name__)\n",
    "        # The first plot is for Quran dataset\n",
    "        beta, k, (X, Y) = get_heaps_law_constants(\n",
    "            dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),\n",
    "            get_points_list=True,\n",
    "        )\n",
    "        dataset_name = all_datasets_names[0]\n",
    "\n",
    "        undotted_beta, undotted_k, (undotted_X, undotted_Y) = get_heaps_law_constants(\n",
    "            dataset=tuple(tokenized_datasets[tokenizer.__name__][0]),\n",
    "            get_points_list=True,\n",
    "        )\n",
    "\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(X), unlog(Y), label=f\"{dataset_name} dataset (k={k:.3f},β={beta:.3f})\"\n",
    "        )\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(X),\n",
    "            [k * x**beta for x in unlog(X)],\n",
    "            label=f\"dotted regression fit for {dataset_name}\",\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(undotted_X),\n",
    "            unlog(undotted_Y),\n",
    "            label=f\"undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})\",\n",
    "        )\n",
    "        axis[irow, 0].plot(\n",
    "            unlog(undotted_X),\n",
    "            [undotted_k * x**undotted_beta for x in unlog(undotted_X)],\n",
    "            label=f\"undotted regression fit for {dataset_name}\",\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "\n",
    "        for i, dataset_name in enumerate(all_datasets_names[1:],start=1):\n",
    "            beta, k, (X, Y) = get_heaps_law_constants(\n",
    "                dataset=tuple(tokenized_datasets[tokenizer.__name__][i]),\n",
    "                get_points_list=True,\n",
    "            )\n",
    "\n",
    "            if \"aggregated\" in dataset_name:\n",
    "                undotted_dataset = undotted_aggregated[tokenizer.__name__]\n",
    "            else:\n",
    "                undotted_dataset = undot_dataset(\n",
    "                    dataset=tuple(tokenized_datasets[tokenizer.__name__][i])\n",
    "                )\n",
    "\n",
    "            (\n",
    "                undotted_beta,\n",
    "                undotted_k,\n",
    "                (undotted_X, undotted_Y),\n",
    "            ) = get_heaps_law_constants(\n",
    "                dataset=tuple(undotted_dataset),\n",
    "                get_points_list=True,\n",
    "            )\n",
    "\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(X),\n",
    "                unlog(Y),\n",
    "                label=f\"{dataset_name} dataset (k={k:.3f},β={beta:.3f})\",\n",
    "            )\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(X),\n",
    "                [k * x**beta for x in unlog(X)],\n",
    "                label=f\"dotted regression fit for {dataset_name}\",\n",
    "                linestyle=\"dashed\",\n",
    "            )\n",
    "\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(undotted_X),\n",
    "                unlog(undotted_Y),\n",
    "                label=f\"undotted {dataset_name} dataset (k={undotted_k:.3f},β={undotted_beta:.3f})\",\n",
    "            )\n",
    "            axis[irow, 1].plot(\n",
    "                unlog(undotted_X),\n",
    "                [undotted_k * x**undotted_beta for x in unlog(undotted_X)],\n",
    "                label=f\"undotted regression fit for {dataset_name}\",\n",
    "                linestyle=\"dashed\",\n",
    "            )\n",
    "            axis[irow, 1].legend(loc='lower right',fontsize=fontsize)\n",
    "            # https://stackoverflow.com/a/11745291/4412324\n",
    "            axis[irow, 1].set_ylim(bottom=0,top=None,auto=True)\n",
    "            axis[irow, 1].set_xlim(left=0,right=None,auto=True)\n",
    "        print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with tokenizer:  WordTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "working with tokenizer:  FarasaMorphologicalTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "working with tokenizer:  DisjointLetterTokenizer\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "plot_heaps_multiple()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
